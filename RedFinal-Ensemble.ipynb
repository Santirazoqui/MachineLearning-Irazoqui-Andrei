{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auJ2yClw527_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats.mstats import winsorize\n",
        "from google.colab import drive, files\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar datos y dividir dataset"
      ],
      "metadata": {
        "id": "rBFpCmouf3Z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.random import set_seed\n",
        "set_seed(234730)"
      ],
      "metadata": {
        "id": "iRskxHDSvIB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/train.csv\")\n",
        "test = pd.read_csv('test_without_label.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKK78-1y1wF-",
        "outputId": "d93d9a8b-6a0c-47ef-c79a-7b680e76b41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['id'] = train['id'].astype(str)\n",
        "train['tweet'] = train['tweet'].astype(str)\n",
        "train['label'] = train['label'].astype(str)"
      ],
      "metadata": {
        "id": "8bBGRjnTxdsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.drop(['id'], axis=1)\n",
        "train.set_index(\"id\", inplace = True)"
      ],
      "metadata": {
        "id": "XwO2hMspxjLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "# parseo de category\n",
        "train[['label']] = train[['label']].apply(lambda col: label_encoder.fit_transform(col))"
      ],
      "metadata": {
        "id": "yI7GxIR9W6xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords, lemmatize, punctuacion\n",
        "Hay muchas palabras que estan repetidas como \"august\", \"august,\", \"august.\", \"august:\", etc. Para resolver esto vamos a sacar la punctuaci√≥n antes de hacer el lemmatizer. Por ahora vamos a dejar algunos s√≠mbolos como los \"#\", quizas los hastags pueden tener informaci√≥n importante.\n",
        "\n",
        "Como en el √≥rden de ejecuci√≥n del CountVectorizer primero se hace el lemmatizing,  no podemos ponerlo en las stop words. Por lo tanto lo vamos a hacer en el mismo m√©todo de lemmatizing."
      ],
      "metadata": {
        "id": "fv52mEwudEkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "Lemmatizer = WordNetLemmatizer()\n",
        "#analyzer = CountVectorizer().build_analyzer()\n",
        "\n",
        "from zipfile import ZipFile\n",
        "file_loc = '/root/nltk_data/corpora/wordnet.zip'\n",
        "with ZipFile(file_loc, 'r') as z:\n",
        "  z.extractall('/root/nltk_data/corpora/')"
      ],
      "metadata": {
        "id": "q_vBq9se2qMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970df71c-9e5f-4335-b633-039209fd8061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#All_punct = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "All_punct = '''!()-[]{};:'\"‚Äú‚Äù‚Äò‚Äô\\,<>./?%^&*_~'''\n",
        "#CountVectorizer con stopwords de Natural Language Toolkit\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def is_Not_Link(word):\n",
        "  return word[0:4] != \"http\"\n",
        "\n",
        "def remove_Punctuation(doc_split):\n",
        "  for i in range(len(doc_split)):\n",
        "    word = doc_split[i];\n",
        "    if is_Not_Link(word):\n",
        "      for elements in word:\n",
        "        if elements in All_punct:\n",
        "          doc_split[i] = word.replace(elements, \"\")\n",
        "  return doc_split\n",
        "\n",
        "def remove_Stopwords(doc_split):\n",
        "  doc_with_Stopwords = doc_split.copy();\n",
        "  for i in range(len(doc_split)):\n",
        "    word = doc_split[i];\n",
        "    if word in nltk.corpus.stopwords.words('english'):\n",
        "      doc_with_Stopwords.remove(word);\n",
        "\n",
        "  return doc_with_Stopwords\n",
        "\n",
        "def new_analyzer(doc):\n",
        "  doc_split = doc.split();\n",
        "  doc_split = remove_Punctuation(doc_split);\n",
        "  doc_split = remove_Stopwords(doc_split);\n",
        "  return (Lemmatizer.lemmatize(w.lower()) for w in doc_split);\n",
        "\n",
        "\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV02dxaTsbyP",
        "outputId": "44955c10-6c22-4fc3-c498-abb76d282176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingenieria de atributos"
      ],
      "metadata": {
        "id": "sReTyoQ6OFv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Largo del documento\n",
        "Uno de los atributos que nos interesa investigar es el largo de los atributos"
      ],
      "metadata": {
        "id": "4jN4WAX6PSh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "letters = \"abcdefghijklmn√±opqrstuvwxyz\"\n",
        "def contarLetras(row):\n",
        "  count = 0;\n",
        "  for letter in row['tweet']:\n",
        "    if letter in letters:\n",
        "      count = count +1;\n",
        "  return count\n",
        "\n",
        "def contarCovid(row):\n",
        "  count = 0;\n",
        "  if \"covid\" in row['tweet']:\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "def contarHashtags(row):\n",
        "  count = 0;\n",
        "  for letter in row['tweet']:\n",
        "    if letter == \"#\":\n",
        "      count = count +1;\n",
        "  return count * 10\n",
        "\n",
        "emojis = \"‚ÑπÔ∏è‚ÜóÔ∏è‚è∫Ô∏è‚ñ™Ô∏è‚óæ‚ñ∂Ô∏è‚òéÔ∏è‚òë‚ò∫‚öñÔ∏è‚õ™‚úÖ‚úà‚úâÔ∏è‚úîÔ∏è‚ùÑÔ∏è‚ùå‚ùå‚ùó‚ù§Ô∏è‚û°‚û°Ô∏èÔøΩüÜïüÜôüåçüåêüè†üè™üè•üè´üëáüëâüëâüèæüë®üë©‚öïÔ∏èüíâüíäüìÉüìàüìâüìçüìèüì¢üì®üì∫üî∞üî¥üîπÔ∏èüïåüïêüò∑üôèüö∂‚ôÄÔ∏èüõ°Ô∏èüü¢ü§îü§±üèøü•ºüß§üß™\"\n",
        "def contarEmojis(row):\n",
        "  count = 0;\n",
        "  for letter in row['tweet']:\n",
        "    if letter in emojis:\n",
        "      count = count + 1\n",
        "  return count"
      ],
      "metadata": {
        "id": "71xXnfVT0dKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['largoDocumento'] = train.tweet.str.len()\n",
        "train['CantidadPalabras'] = train.apply(lambda row : len(row['tweet'].split()), axis=1)\n",
        "train['cantidad de letras'] = train.apply(lambda row : contarLetras(row), axis=1)\n",
        "train['cantidad de covid'] = train.apply(lambda row : contarLetras(row), axis=1)\n",
        "train['cantidad de hashtag'] = train.apply(lambda row : contarHashtags(row), axis=1)\n",
        "train['cantidad de emojis'] = train.apply(lambda row : contarEmojis(row), axis=1)"
      ],
      "metadata": {
        "id": "3dGgC6kpOX52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# X e Y\n",
        " Dividir los datos en X e Y"
      ],
      "metadata": {
        "id": "lJcxI3B8Pv7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Y = train[\"label\"]\n",
        "X = train.drop('label', axis=1)\n",
        "\n",
        "\n",
        "#X_train, X_validation, Y_train, Y_validation = train_test_split(\n",
        " #  train.drop('label', axis=1),\n",
        "  #  train['label'],\n",
        "   # test_size=(1.0/3), random_state=42)"
      ],
      "metadata": {
        "id": "FXppDGXJxsy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit"
      ],
      "metadata": {
        "id": "1Sqi2438Ibd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TF_IDF_Vectorizer = TfidfVectorizer(analyzer=new_analyzer, max_features=15000)\n",
        "tfidf_vector = TF_IDF_Vectorizer.fit(X[\"tweet\"])"
      ],
      "metadata": {
        "id": "-aBmH9uJtACD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform"
      ],
      "metadata": {
        "id": "l7IRYu7XtBRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vector = TF_IDF_Vectorizer.transform(X[\"tweet\"])\n",
        "tfidf_vector = pd.DataFrame(tfidf_vector.toarray(), index = X.index, columns = TF_IDF_Vectorizer.get_feature_names())"
      ],
      "metadata": {
        "id": "laKfx_8Fx4Nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2851a2e-8b59-4b48-9d00-2eaaa647dc32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A√±adir atributos al nuevo dataframe de TFIDF"
      ],
      "metadata": {
        "id": "aP8B8t9KZaie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vector['largoDocumento'] = X['largoDocumento']\n",
        "tfidf_vector['CantidadPalabras'] = X['CantidadPalabras']\n",
        "tfidf_vector['cantidad de letras'] = X['cantidad de letras']\n",
        "tfidf_vector['cantidad de covid']  = X['cantidad de covid'] \n",
        "tfidf_vector['cantidad de hashtag'] = X['cantidad de hashtag']\n",
        "tfidf_vector['cantidad de emojis'] = X['cantidad de emojis']"
      ],
      "metadata": {
        "id": "BmRkiQpXZZ5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red Neuronal"
      ],
      "metadata": {
        "id": "ilGGPtUT0xFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import LeakyReLU"
      ],
      "metadata": {
        "id": "e8pMorDV1DI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam_optimizer='Adam'\n",
        "loss='binary_crossentropy'\n",
        "metric='accuracy'\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_shape=(tfidf_vector.shape[1],)))\n",
        "model.add(Activation(LeakyReLU(alpha=0.3)))\n",
        "model.add(Dense(30, use_bias=True))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1, use_bias=True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.compile(loss=loss, optimizer=adam_optimizer, metrics=[metric])"
      ],
      "metadata": {
        "id": "lW8SiL-t01M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "X_train_array = np.asarray(tfidf_vector)\n",
        "Y_train_array = np.asarray(Y)\n",
        "\n",
        "training = model.fit(X_train_array, Y_train_array, epochs=200, batch_size=100, validation_split=0.2)"
      ],
      "metadata": {
        "id": "gY0iDNOLDG88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras\n",
        "\n",
        "model.save(\"basic_model\")\n",
        "saved_model = keras.models.load_model(\"basic_model\")"
      ],
      "metadata": {
        "id": "rCRKAKT9oRpN",
        "outputId": "b9312ed4-f6ef-4467-835f-8536860d79a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "dQyZTFSjXjir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test['id'].astype('str')\n",
        "test['tweet'].astype('str')\n",
        "\n",
        "test_bag = TF_IDF_Vectorizer.transform(test[\"tweet\"])\n",
        "test_bag = pd.DataFrame(test_bag.toarray(), index = test.index, columns = TF_IDF_Vectorizer.get_feature_names())\n",
        "\n",
        "test['largoDocumento'] = test.tweet.str.len()\n",
        "test['CantidadPalabras'] = test.apply(lambda row : len(row['tweet'].split()), axis=1)\n",
        "test['cantidad de letras'] = test.apply(lambda row : contarLetras(row), axis=1)\n",
        "test['cantidad de covid'] = test.apply(lambda row : contarLetras(row), axis=1)\n",
        "test['cantidad de hashtag'] = test.apply(lambda row : contarHashtags(row), axis=1)\n",
        "test['cantidad de emojis'] = test.apply(lambda row : contarEmojis(row), axis=1)\n",
        "\n",
        "test_bag['largoDocumento'] = test['largoDocumento']\n",
        "test_bag['largoDocumento'] = test['largoDocumento']\n",
        "test_bag['CantidadPalabras'] = test['CantidadPalabras']\n",
        "test_bag['cantidad de letras'] = test['cantidad de letras']\n",
        "test_bag['cantidad de covid']  = test['cantidad de covid'] \n",
        "test_bag['cantidad de hashtag'] = test['cantidad de hashtag']\n",
        "test_bag['cantidad de emojis'] = test['cantidad de emojis']\n",
        "\n",
        "\n",
        "test_bag_array = np.asarray(test_bag)\n",
        "test_prediction = saved_model.predict(test_bag_array)\n",
        "\n",
        "copy = test_prediction.copy()\n",
        "predNumber = [];\n",
        "predLabel = [];\n",
        "for i in range(len(copy)):\n",
        "  if copy[i][0] > 0.5:\n",
        "    predNumber.append([1])\n",
        "    predLabel.append([\"real\"])\n",
        "  else:\n",
        "    predNumber.append([0])\n",
        "    predLabel.append([\"fake\"])\n",
        "\n",
        "prediction = pd.DataFrame(predLabel, columns=['label'])\n",
        "prediction.index += 1\n",
        "prediction = prediction.to_csv('prediction.csv')"
      ],
      "metadata": {
        "id": "hZMa_5ZcWMWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a9baae-af24-4145-ea92-98b949f5dd15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67/67 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('prediction.csv')"
      ],
      "metadata": {
        "id": "odAqKelbXH36",
        "outputId": "c6958c4d-5275-4776-d249-d202e0dc20c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cf009192-8e8d-48da-8e50-db2134b50b81\", \"prediction.csv\", 20300)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold_result_arr"
      ],
      "metadata": {
        "id": "FOadql0keOYu",
        "outputId": "52fb5c2c-b7c9-45db-fb2b-3ba14be568e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 1.0, 1.0, 1.0, 0.9349736379613357]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}