{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "auJ2yClw527_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats.mstats import winsorize\n",
        "from google.colab import drive, files\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar datos y dividir dataset"
      ],
      "metadata": {
        "id": "rBFpCmouf3Z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.random import set_seed\n",
        "set_seed(234730)"
      ],
      "metadata": {
        "id": "iRskxHDSvIB1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test_without_label.csv')"
      ],
      "metadata": {
        "id": "EDu6L8kKvMio"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['id'] = train['id'].astype(str)\n",
        "train['tweet'] = train['tweet'].astype(str)\n",
        "train['label'] = train['label'].astype(str)"
      ],
      "metadata": {
        "id": "8bBGRjnTxdsG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.drop(['id'], axis=1)\n",
        "train.set_index(\"id\", inplace = True)"
      ],
      "metadata": {
        "id": "XwO2hMspxjLi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "# parseo de category\n",
        "train[['label']] = train[['label']].apply(lambda col: label_encoder.fit_transform(col))"
      ],
      "metadata": {
        "id": "yI7GxIR9W6xB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords, lemmatize, punctuacion\n",
        "Hay muchas palabras que estan repetidas como \"august\", \"august,\", \"august.\", \"august:\", etc. Para resolver esto vamos a sacar la punctuación antes de hacer el lemmatizer. Por ahora vamos a dejar algunos símbolos como los \"#\", quizas los hastags pueden tener información importante.\n",
        "\n",
        "Como en el órden de ejecución del CountVectorizer primero se hace el lemmatizing,  no podemos ponerlo en las stop words. Por lo tanto lo vamos a hacer en el mismo método de lemmatizing."
      ],
      "metadata": {
        "id": "fv52mEwudEkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "Lemmatizer = WordNetLemmatizer()\n",
        "#analyzer = CountVectorizer().build_analyzer()\n",
        "\n",
        "from zipfile import ZipFile\n",
        "file_loc = '/root/nltk_data/corpora/wordnet.zip'\n",
        "with ZipFile(file_loc, 'r') as z:\n",
        "  z.extractall('/root/nltk_data/corpora/')"
      ],
      "metadata": {
        "id": "q_vBq9se2qMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a837393-70d6-448c-c636-ad14534dfe66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#All_punct = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "All_punct = '''!()-[]{};:'\"\\,<>./?%^&*_~'''\n",
        "#CountVectorizer con stopwords de Natural Language Toolkit\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def is_Not_Link(word):\n",
        "  return word[0:4] != \"http\"\n",
        "\n",
        "def remove_Punctuation(doc_split):\n",
        "  for i in range(len(doc_split)):\n",
        "    word = doc_split[i];\n",
        "    if is_Not_Link(word):\n",
        "      for elements in word:\n",
        "        if elements in All_punct:\n",
        "          doc_split[i] = word.replace(elements, \"\")\n",
        "  return doc_split\n",
        "\n",
        "def remove_Stopwords(doc_split):\n",
        "  doc_with_Stopwords = doc_split.copy();\n",
        "  for i in range(len(doc_split)):\n",
        "    word = doc_split[i];\n",
        "    if word in nltk.corpus.stopwords.words('english'):\n",
        "      doc_with_Stopwords.remove(word);\n",
        "\n",
        "  return doc_with_Stopwords\n",
        "\n",
        "def new_analyzer(doc):\n",
        "  doc_split = doc.split();\n",
        "  doc_split = remove_Punctuation(doc_split);\n",
        "  doc_split = remove_Stopwords(doc_split);\n",
        "  return (Lemmatizer.lemmatize(w.lower()) for w in doc_split);\n",
        "\n",
        "\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV02dxaTsbyP",
        "outputId": "b57dbf97-ba56-4c9f-d44f-c7ddfbd397f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingenieria de atributos"
      ],
      "metadata": {
        "id": "sReTyoQ6OFv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Largo del documento\n",
        "Uno de los atributos que nos interesa investigar es el largo de los atributos"
      ],
      "metadata": {
        "id": "4jN4WAX6PSh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['largoDocumento'] = train.tweet.str.len()\n",
        "train['CantidadPalabras'] = train.apply(lambda row : len(row['tweet'].split()), axis=1)"
      ],
      "metadata": {
        "id": "3dGgC6kpOX52"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train / Validation\n",
        "Al final de la ingeniería dividimoms en train y validation"
      ],
      "metadata": {
        "id": "lJcxI3B8Pv7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(\n",
        "    train.drop('label', axis=1),\n",
        "    train['label'],\n",
        "    test_size=(1.0/3), random_state=42)"
      ],
      "metadata": {
        "id": "FXppDGXJxsy2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit"
      ],
      "metadata": {
        "id": "1Sqi2438Ibd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TF_IDF_Vectorizer = TfidfVectorizer(analyzer=new_analyzer, max_features=5000)\n",
        "bag_of_words = TF_IDF_Vectorizer.fit(X_train[\"tweet\"])\n",
        "\n",
        "print(bag_of_words.get_feature_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aBmH9uJtACD",
        "outputId": "867abecb-7d9a-4fc8-d929-f25d898dbc1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '\"im', '\"its', '#', '#1', '#2020presidentialelection', '#21dayslockdown', '#acog2020', '#actnow', '#america', '#amitabhbachchan', '#asymptomatic', '#auspol', '#bacteria', '#beer', '#bihar', '#breakfast', '#cdnpoli', '#china', '#codecheck', '#colorado', '#connecticut', '#containmentzones', '#corona', '#coronabeer', '#coronacheck', '#coronaoutbreak', '#coronaupdate', '#coronaupdates', '#coronaupdatesindia', '#coronavirus', '#coronavirusfacts', '#coronavirusindia', '#coronavirusoutbreak', '#coronaviruspandemic', '#coronavirusupdate', '#coronavirusupdates', '#coronawarriors', '#coronawatch', '#coronil', '#covid', '#covid-19', '#covid19', '#covid19associated', '#covid19india', '#covid19like', '#covid19nigeria', '#covid19pakistan', '#covid19science', '#covid2019', '#covidindiaseva', '#covidupdates', '#covidview', '#covidー19', '#datoscoronavirus', '#death', '#delhi', '#disease', '#donaldtrump', '#dranthonyfauci', '#dyk', '#facemasks', '#factcheck', '#fakenews', '#faq', '#fatalityrate', '#flu', '#food', '#foxnews', '#fullyimmunizeeverychild', '#gujarat', '#haryana', '#hcp', '#hcps', '#hcq', '#health', '#healthcare', '#healthequity', '#healthforall', '#healthworkers', '#hiv', '#hollywood', '#homeisolation', '#homequarantine', '#hydroxychloroquine', '#icmrfightscovid19', '#immunity', '#immunizationforall', '#incontext', '#india', '#indiafightscorona', '#indiafightscoronavirus', '#indiafightscovid19', '#indiawillwin', '#institutionalquarantine', '#josephbiden', '#kayburley', '#lockdown', '#maharashtra', '#mainbhinewschecker', '#masks', '#michigan', '#mrna', '#n95', '#nashville', '#ncdcrrt', '#newschecker', '#newyork', '#nigeria', '#novel', '#nyc', '#onpoli', '#pandemic', '#pmqs', '#politicalcorrectness', '#politics', '#ppe', '#privacy', '#propaganda', '#ptfcovid19', '#qanda', '#quarantine', '#rapidantigentest', '#recoveryrate', '#remdesivir', '#reopeningsafely', '#rnc2020', '#rtpcr', '#sam2020', '#saturdaynightfever', '#seanhannity', '#selfisolation', '#sepsis', '#slowthespread', '#socialdistance', '#socialdistancing', '#sofi2020', '#sports', '#springst', '#stayathome', '#stayhome', '#stayhomestaysafe', '#staysafe', '#symptomatic', '#takeresponsibility', '#tamilnadu', '#telangana', '#un75', '#unemployment', '#unga', '#unlock4', '#usa', '#uttarpradesh', '#vaccine', '#vaccineswork', '#vermont', '#virus', '#wearamask', '#whatsapp', '#whitehouse', '#who', '#whoimpact', '#wildfire', '#worldmaskweek', '#wuhan', '$100', '$50', '&amp', '&gt&gt&gt', '(#pmsby', '(1.6%', '(1.7%', '(1.8%', '(1/2', '(2/4', '(21.9%', '(3/4', '(4025079', '(act', '(cfr', '(coronavirus', '(covid-19', '(covid-19)', '(ie', '(india', '(pmjjby', '(ppe)', '(tpm', '(who', '+', '+1', '0', '01', '02', '04', '05', '06', '0800', '0930', '1', '1%', '10', '100', '1000', '1000+', '10000', '100000', '100000+', '1000190000', '100k', '101', '102', '1040', '105', '1054', '107', '10k', '10pm', '10th', '11', '1100', '110000', '111', '1130', '114', '115', '115000', '1154', '1155pm', '116', '117', '1170', '118', '1184', '119', '11th', '12', '120', '120000', '1206', '1217', '1219', '123', '128', '12th', '13', '130', '130000', '134', '13th', '14', '140', '1455', '1481', '14th', '15', '150', '1500', '15000', '15001100000', '1504', '156', '158', '15th', '16', '160', '167', '16th', '17', '170', '176', '17th', '18', '18th', '19', '190', '19000', '195', '19th', '1m', '1ondo', '1plateau', '1st', '1…', '1⃣', '2', '20', '20%', '200', '2000', '20000', '200000', '2001', '2005', '2008', '2009', '200k', '2010', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2020👇', '2021', '2022', '204', '20k', '21', '21000', '214', '2159', '21k', '22', '2200', '22nd', '23', '232', '233', '23k', '24', '2404585', '242', '247', '24th', '25', '25000', '25th', '26', '2621', '26th', '27', '2700', '276', '27th', '28', '288', '28k', '28th', '29', '299', '29k', '29th', '2bauchi', '2gombe', '2m', '2nd', '2pm', '3', '30', '300', '3000', '30000', '300k', '30th', '31', '31k', '31st', '32', '323', '33', '34', '35', '350', '35k', '36', '37', '38', '385', '38k', '39', '3bauchi', '3borno', '3rd', '4', '40', '400', '4000', '400000', '41', '42', '42k', '43', '4322', '43k', '44', '4422', '44k', '45', '47', '47k', '48', '481', '49', '49000', '4th', '4x', '5', '50', '500', '5000', '50000', '500000', '501', '50k', '51', '52', '53', '54', '55', '55000', '56', '56k', '57', '57k', '58', '59', '5g', '5th', '5x', '6', '60', '600', '6000', '60000', '61', '62', '63', '64', '65', '65+', '65k', '66', '67', '675', '68', '69', '6th', '7', '7.5', '70', '70k', '7103', '72', '73', '74', '75', '76', '77', '78', '7day', '7th', '8', '80', '80%', '800', '8000', '81', '83', '8316', '85', '86', '87', '89', '8am', '8th', '9', '90', '900', '90000+', '91', '92', '93', '94', '95', '96', '97', '98', '99', '9th', '=', '??', '???covid19', '@aajtak', '@aamaadmiparty', '@africacdc', '@airnewsalerts', '@alexismadrigal', '@ani', '@arvindkejriwal', '@ashwinikchoubey', '@biharhealthdept', '@boomlivein', '@butchthorne', '@cdc_hivaids', '@cdcdirector', '@cdcemergency', '@cdcgov', '@cdcmmwr', '@cdctravel', '@chikwei', '@cmoguj', '@cmomaharashtra', '@cnn', '@conservvoice', '@couchmaria', '@covid19tracking', '@covidindiaseva', '@covidnewsbymib', '@csogok', '@ddnewslive', '@dgpgujarat', '@dreoehanire', '@drharshvardhan', '@drhvoffice', '@drjohnwhyte', '@drtedros', '@factchecknet', '@fmohnigeria', '@followlasg', '@gavi', '@geraintmeysydd', '@govrondesantis', '@harvardgh', '@hhsgov', '@hmoindia', '@icmrdelhi', '@imperialcollege', '@julie34479', '@matthancock', '@mbuhari', '@minhealthnz', '@mohfw_india', '@mohfwindia', '@mygovindia', '@narendramodi', '@nature', '@nih', '@niosh', '@nitiaayog', '@normanbrennan', '@nsitharaman', '@ntanewsnow', '@nytimes', '@peterwa97559477', '@pib_india', '@pibindia', '@pjpaton', '@pmoindia', '@prakashjavdekar', '@profbhargava', '@ptfcovid19', '@ptinews', '@realdonaldtrump', '@riccigeri', '@smileygirl19683', '@statedept', '@surgeon_general', '@theatlantic', '@thelancet', '@tony80554056', '@vmaledew', '@webmd', '@whatsapp', '@who', '@whoafro', '@whos', '@yayitsrob', 'a', 'a&ampe', 'aaj', 'aamir', 'abbott', 'abia', 'abia1', 'abia2', 'abia6', 'abia9', 'ability', 'able', 'abortion', 'about', 'absolute', 'abuja', 'acc', 'accelerate', 'accelerating', 'accelerator', 'accept', 'accepted', 'access', 'accidentally', 'according', 'account', 'accounting', 'accurate', 'accused', 'achieve', 'achievement', 'achieves', 'acquired', 'across', 'act', 'actaccelerator', 'acting', 'action', 'activated', 'active', 'actively', 'activist', 'activity', 'actor', 'actress', 'actual', 'actually', 'acute', 'ad', 'adamawa1', 'add', 'added', 'adding', 'addition', 'additional', 'address', 'adhere', 'adjust', 'adjustment', 'administration', 'administrator', 'admins', 'admission', 'admit', 'admits', 'admitted', 'adult', 'advance', 'advantage', 'advice', 'advise', 'advised', 'adviser', 'advises', 'advising', 'advisory', 'affair', 'affect', 'affected', 'affecting', 'africa', 'african', 'after', 'again', 'age', 'aged', 'agency', 'agent', 'aggressive', 'ago', 'agreed', 'agreement', 'agrees', 'ahead', 'aid', 'aiims', 'aim', 'aimed', 'air', 'airborne', 'airline', 'airport', 'airway', 'akwa', 'al', 'alabama', 'album', 'alcohol', 'alert', 'ali', 'alive', 'alkaline', 'all', 'allah', 'alleged', 'allegedly', 'allergy', 'allocation', 'allow', 'allowance', 'allowed', 'allows', 'alltime', 'almost', 'alone', 'along', 'alongside', 'already', 'also', 'alternative', 'although', 'always', 'am', 'amazing', 'amazon', 'ambulance', 'america', 'american', 'americold', 'amid', 'amidst', 'amit', 'amitabh', 'among', 'amongst', 'amount', 'an', 'analysis', 'anambra', 'anambra1', 'anambra2', 'and', 'andhra', 'andor', 'and…', 'angela', 'anger', 'angry', 'animal', 'announce', 'announced', 'announcement', 'announces', 'announcing', 'another', 'answer', 'anthony', 'antibiotic', 'antibody', 'antigen', 'antimalarial', 'antiviral', 'antonio', 'anxiety', 'any', 'anybody', 'anyone', 'anything', 'apart', 'api', 'apologise', 'app', 'appalling', 'apparently', 'appear', 'appeared', 'appears', 'apple', 'apply', 'appointed', 'approach', 'appropriate', 'approval', 'approved', 'approximately', 'april', 'ar', 'arabia', 'are', 'area', 'arent', 'aren’t', 'argar', 'argentina', 'arizona', 'arkansas', 'arm', 'army', 'aroha', 'around', 'arrange', 'arrest', 'arrested', 'arrival', 'arrived', 'arriving', 'arsenicum', 'article', 'asia', 'asian', 'aside', 'ask', 'asked', 'asking', 'asks', 'aspect', 'aspirin', 'ass', 'assam', 'assessment', 'assign', 'assigned', 'assist', 'assistance', 'associate', 'associated', 'association', 'assume', 'asymptomatic', 'at', 'athlete', 'attack', 'attacked', 'attempt', 'attempted', 'attend', 'attendance', 'attended', 'attending', 'attention', 'attributed', 'auckland', 'audio', 'aug', 'august', 'australia', 'australian', 'author', 'authority', 'authorization', 'autopsy', 'availability', 'available', 'average', 'avifavir', 'avoid', 'avoiding', 'awaiting', 'aware', 'awareness', 'away', 'ayurveda', 'ayurvedic', 'ayush', 'az', 'azim', 'azithromycin', 'b', 'baba', 'baby', 'back', 'backbone', 'backfill', 'backlog', 'backlogged', 'bacteria', 'bacterium', 'bad', 'bag', 'bakery', 'baking', 'balance', 'bame', 'ban', 'bangalore', 'bank', 'banned', 'banning', 'bar', 'barack', 'based', 'basic', 'basically', 'basis', 'bat', 'bath', 'battle', 'bauchi', 'bauchi1', 'bauchi2', 'bauchi3', 'bauchi8', 'bay', 'bayelsa1', 'bayelsa14', 'bbc', 'be', 'beach', 'beard', 'beat', 'beaten', 'beating', 'became', 'because', 'become', 'becomes', 'becoming', 'bed', 'beef', 'been', 'beer', 'before', 'began', 'begin', 'beginning', 'begun', 'behaviour', 'behind', 'being', 'belief', 'believe', 'belong', 'benchmark', 'benefit', 'bengal', 'benue', 'benue1', 'benue3', 'bereavement', 'berlin', 'besides', 'best', 'betacoronaviruses', 'betadine', 'better', 'beware', 'beyond', 'bharat', 'bicarbonate', 'biden', 'bidensoetoro', 'big', 'biggest', 'bihar', 'bill', 'billion', 'billionaire', 'bima', 'bioengineered', 'biological', 'biology', 'biotech', 'bioweapon', 'bird', 'birth', 'bit', 'bizarre', 'bjp', 'black', 'blame', 'blamed', 'bleach', 'bless', 'block', 'blog', 'blood', 'blowing', 'blue', 'board', 'body', 'boiled', 'boiling', 'bolivia', 'bollywood', 'bolsonaro', 'bolton', 'book', 'boost', 'border', 'boris', 'borno', 'borno1', 'borno12', 'borno2', 'borno6', 'borno8', 'both', 'bought', 'bounce', 'bound', 'bowl', 'box', 'boy', 'brain', 'branch', 'brazil', 'brazilian', 'break', 'breakdown', 'breaking', 'breastfeeding', 'breath', 'breathe', 'breathing', 'brexit', 'brian', 'briefing', 'bring', 'bringing', 'brings', 'britain', 'british', 'briton', 'broad', 'broader', 'broiler', 'broke', 'broken', 'brother', 'brought', 'budget', 'bug', 'build', 'building', 'built', 'burden', 'buried', 'burning', 'bury', 'bus', 'business', 'busy', 'but', 'buy', 'buying', 'by', 'c', 'c19', 'ca', 'cabbage', 'cabinet', 'calculate', 'california', 'california’s', 'call', 'called', 'calling', 'came', 'camel', 'camila', 'camp', 'campaign', 'can', 'canada', 'canadian', 'cancel', 'cancellation', 'cancelled', 'cancer', 'candidate', 'candle', 'cannot', 'cant', 'can’t', 'capacity', 'capital', 'caption', 'capture', 'captured', 'capturing', 'caput', 'car', 'carbon', 'card', 'cardiac', 'care', 'careful', 'carefully', 'carolina', 'carona', 'carried', 'carrier', 'carry', 'carrying', 'case', 'caseload', 'cases+deaths', 'casestill', 'cases”', 'cases\\u2063', 'casual', 'casualty', 'cat', 'catch', 'catching', 'category', 'cattle', 'caught', 'cause', 'caused', 'causing', 'caveat', 'ccp', 'cdc', 'cdc’s', 'celebrate', 'celebrating', 'celebratory', 'celebrity', 'cell', 'center', 'central', 'centre', 'centreled', 'century', 'ceo', 'certain', 'certainly', 'certificate', 'certified', 'cfr', 'cghs', 'chain', 'chair', 'chairman', 'challenge', 'challenging', 'chance', 'chancellor', 'chandigarh', 'change', 'changed', 'changeover', 'changer', 'changing', 'channel', 'chaos', 'characteristic', 'charge', 'charity', 'charles', 'chart', 'charter', 'chat', 'cheap', 'cheaper', 'check', 'checked', 'checker', 'checking', 'checkup', 'chemical', 'chemist', 'chemistry', 'chest', 'chhattisgarh', 'chicago', 'chicken', 'chicken.', 'chief', 'child', 'childcare', 'childhood', 'childrens', 'child’s', 'chile', 'chin', 'china', 'china.', 'chinatown', 'chinese', 'chip', 'chlorine', 'chloroquine', 'choice', 'choose', 'choosing', 'chose', 'chris', 'christchurch', 'chronic', 'chuck', 'chunk', 'church', 'chyna', 'cinema', 'circuit', 'circulate', 'circulated', 'circulates', 'circulating', 'circulation', 'circumstance', 'circus', 'cite', 'cited', 'citizen', 'city', 'civil', 'civilization', 'ciyal', 'claim', 'claimed', 'claiming', 'claire', 'clara', 'clarification', 'clarified', 'clarifies', 'clarify', 'class', 'clean', 'cleaner', 'cleaning', 'clear', 'clearance', 'cleared', 'clearly', 'clever', 'click', 'climate', 'climb', 'climbing', 'clinic', 'clinical', 'clinically', 'clinician', 'clinton', 'clip', 'clo2', 'clock', 'clorox', 'close', 'closed', 'closely', 'closer', 'closing', 'closure', 'clot', 'cloth', 'clothes', 'club', 'cluster', 'cluster\\u2063', 'clínicas', 'cm', 'cnn', 'cns', 'coach', 'coaching', 'cobra', 'coca', 'cocaine', 'coca’s', 'code', 'coded', 'coffee', 'coffin', 'cohort', 'coin', 'cold', 'colfax', 'collaboration', 'collaborative', 'collapse', 'collapsing', 'collates', 'colleague', 'collection', 'collective', 'college', 'colloidal', 'colombia', 'color', 'colorado', 'colour', 'columbia', 'column', 'combat', 'combating', 'combination', 'combined', 'combo', 'come', 'coming', 'commemorative', 'commercial', 'commission', 'commit', 'commitment', 'commits', 'committed', 'committee', 'commodity', 'common', 'commonly', 'communal', 'communicate', 'communicated', 'communicating', 'communication', 'communist', 'community', 'communitybased', 'comorbidities', 'company', 'company’s', 'comparable', 'compare', 'compared', 'comparing', 'comparison', 'compassionate', 'complaint', 'complete', 'completed', 'completely', 'completes', 'completing', 'completion', 'complex', 'complexity', 'compliance', 'complicated', 'complication', 'complicit', 'component', 'compound', 'comprehensive', 'comprehensively', 'comprising', 'compulsory', 'computer', 'concentrated', 'concentration', 'concern', 'concerned', 'concerning', 'concluded', 'conclusion', 'conclusive', 'condemned', 'condition', 'conduct', 'conducted', 'conducting', 'conference', 'confidence', 'confident', 'confirm', 'confirmation', 'confirmed', 'confirming', 'confirms', 'confuse', 'confused', 'confusing', 'congregation', 'congress', 'connect', 'connected', 'connecticut', 'connecting', 'connection', 'consecutive', 'consent', 'consequence', 'consider', 'considerable', 'consideration', 'considered', 'considering', 'consistent', 'consistently', 'conspiracy', 'constant', 'constantly', 'constituent', 'constrained', 'constructed', 'consult', 'consultation', 'consume', 'consuming', 'consumption', 'contact', 'contacted', 'contacted\\u2063', 'contacting', 'contagion', 'contain', 'containing', 'containment', 'contains', 'contaminated', 'content', 'context', 'continent', 'continue', 'continued', 'continues', 'continuing', 'continuous', 'continuously', 'contract', 'contracted', 'contracting', 'contrary', 'contribute', 'contributed', 'contributing', 'contribution', 'control', 'controlled', 'controller', 'controlling', 'controversial', 'controversy', 'convalescent', 'convened', 'convention', 'conversation', 'conversion', 'convert', 'converted', 'converting', 'cool', 'coordinate', 'coordinated', 'coordination', 'cop', 'cope', 'copied', 'coping', 'core', 'corona', 'coronacheck', 'coronavac', 'coronavirus', 'coronavirus.', 'coronavirus.�', 'coronaviruses', 'coronavirusthemed', 'coronavirus”', 'coronil', 'corporation', 'corps', 'corpse', 'correct', 'corrected', 'correction', 'correctly', 'correctness', 'correlation', 'corridor', 'corrientes', 'corticosteroid', 'cost', 'costco', 'cough', 'coughing', 'could', 'couldnt', 'couldn’t', 'council', 'count', 'counted', 'counter', 'counterpart', 'counting', 'countries@drtedros', 'country', 'country’s', 'county', 'couple', 'coupled', 'course', 'court', 'court’s', 'cov2', 'covax', 'covaxin', 'cover', 'coverage', 'covered', 'covering', 'covid', 'covid-19', 'covid-19.', 'covid-19.�', 'covid-19\\u2063', 'covid19', 'covidrelated', 'covidsafe', 'cow', 'cr', 'cream', 'create', 'created', 'creating', 'credit', 'crematorium', 'crew', 'crime', 'criminal', 'crisis', 'cristiano', 'criterion', 'critical', 'critically', 'crore', 'cross', 'crossed', 'crowd', 'crowded', 'crucial', 'cruise', 'cry', 'cuba', 'cummings', 'cumulative', 'cumulatively', 'cuomo', 'cup', 'cupboard', 'cure', 'cured', 'cureddischargedmigrated', 'cureddischargedmigrated+active', 'curedrecovered', 'curfew', 'current', 'currently', 'curve', 'customer', 'cut', 'cutting', 'cv', 'd', 'da', 'daily', 'dakota', 'damage', 'danger', 'dangerous', 'daniel', 'dark', 'dashboard', 'data', 'dataset', 'date', 'dating', 'daughter', 'david', 'day', 'dc', 'dcgi', 'de', 'dead', 'deadly', 'deal', 'dear', 'death', 'debt', 'debunked', 'debunking', 'decade', 'deceased', 'december', 'decide', 'decided', 'decision', 'declared', 'decline', 'declined', 'declining', 'decrease', 'decreased', 'decreasing', 'dedicated', 'deep', 'defeat', 'defense', 'definitely', 'definition', 'degree', 'dehradun', 'delaware', 'delay', 'delayed', 'deleted', 'delhi', 'delivered', 'delivering', 'delivery', 'delta', 'delta10', 'delta12', 'delta3', 'delta6', 'delta7', 'demand', 'demanding', 'democrat', 'democratic', 'demographic', 'demonstrated', 'denied', 'denies', 'departing', 'department', 'departure', 'depending', 'depends', 'deploy', 'deployed', 'depression', 'deputy', 'desantis', 'describes', 'describing', 'description', 'design', 'despite', 'destroy', 'destroyed', 'detail', 'detect', 'detected', 'detection', 'determine', 'develop', 'developed', 'developer', 'developing', 'development', 'dexamethasone', 'dg', 'diabetes', 'diagnose', 'diagnosed', 'diagnosis', 'diagnostic', 'diagnostics', 'did', 'didnt', 'didn’t', 'dido', 'die', 'died', 'difference', 'different', 'differently', 'difficult', 'difficulty', 'digital', 'dioxide', 'dip', 'direct', 'direction', 'directive', 'directly', 'director', 'directorgeneral', 'directs', 'dirty', 'disaster', 'discharge', 'discharged', 'discontinued', 'discover', 'discovered', 'discovers', 'discrepancy', 'discus', 'discussion', 'disease', 'disinfect', 'disinfectant', 'disinfection', 'disorder', 'disparity', 'disproportionately', 'disrupted', 'disruption', 'distance', 'distancing', 'distributed', 'distributing', 'distribution', 'district', 'diverse', 'dna', 'do', 'doc', 'doctor', 'doctored', 'document', 'documentary', 'doesnt', 'doesn’t', 'dog', 'doha', 'dollar', 'domestic', 'dominic', 'donald', 'donate', 'donated', 'donation', 'done', 'dont', 'don’t', 'door', 'dos', 'dose', 'double', 'doubled', 'doubling', 'doubt', 'doug', 'down', 'download', 'dozen', 'dr', 'drink', 'drinking', 'drive', 'driven', 'driver', 'driving', 'drop', 'droplet', 'dropped', 'drove', 'drug', 'dublin', 'duck', 'due', 'during', 'duterte', 'duty', 'dy', 'dying', 'dynamic', 'each', 'earlier', 'early', 'earn', 'earth', 'ease', 'easier', 'easily', 'easing', 'east', 'easy', 'eat', 'eating', 'ebola', 'ebonyi11', 'ebonyi3', 'ebonyi4', 'ebonyi9', 'economic', 'economy', 'ecuador', 'ed', 'edo', 'edo1', 'edo17', 'edo22', 'education', 'educational', 'edward', 'effect', 'effective', 'effectively', 'effectiveness', 'efficient', 'efficiently', 'effort', 'eight', 'either', 'ekiti', 'ekiti1', 'ekiti2', 'ekiti4', 'ekiti6', 'elbow', 'elderly', 'elected', 'election', 'elective', 'elevated', 'eligible', 'eliminate', 'eliminates', 'elizabeth', 'else', 'email', 'emerged', 'emergency', 'emerging', 'emirate', 'emission', 'emphasis', 'employee', 'employment', 'empty', 'enable', 'encourage', 'encouraged', 'encourages', 'encouraging', 'end', 'ended', 'ending', 'endorses', 'enema', 'enemy', 'enforce', 'enforced', 'enforcement', 'engage', 'engagement', 'engaging', 'engineered', 'engineering', 'england', 'english', 'enhanced', 'enjoy', 'enjoyed', 'enough', 'enrolled', 'enrollment', 'ensure', 'ensured', 'ensuring', 'enter', 'entered', 'entering', 'entire', 'entirely', 'entitled', 'entity', 'entrance', 'entry', 'enugu', 'enugu14', 'enugu15', 'enugu2', 'enugu25', 'enugu6', 'enugu7', 'enugu9', 'environmental', 'envoy', 'epidemic', 'epidemiological', 'epidemiologist', 'equipment', 'equitable', 'equity', 'equivalent', 'er', 'eradicate', 'erratic', 'erroneously', 'error', 'especially', 'essential', 'established', 'establishes', 'estimate', 'estimated', 'et', 'etc', 'ethiopian', 'ethnic', 'ethnicity', 'eu', 'eucalyptus', 'euro', 'europe', 'european', 'evaluate', 'evaluation', 'evangelical', 'evangelicals', 'even', 'evening', 'event', 'eventually', 'ever', 'every', 'everybody', 'everyday', 'everyone', 'everything', 'everywhere', 'eviction', 'evidence', 'evil', 'evolution', 'exact', 'exactly', 'exaggerated', 'exam', 'examination', 'examines', 'example', 'exceed', 'exceeded', 'exceeds', 'except', 'exception', 'exceptional', 'excerpt', 'excess', 'excessive', 'excited', 'exclusive', 'excuse', 'executive', 'exempt', 'exemption', 'exercise', 'exhausted', 'exist', 'existed', 'existing', 'exists', 'exosome', 'exotic', 'expand', 'expanded', 'expanding', 'expansion', 'expect', 'expected', 'expecting', 'expel', 'expelled', 'expensive', 'experience', 'experienced', 'experiencing', 'expert', 'explain', 'explained', 'explainer', 'explains', 'explanation', 'explosion', 'exponential', 'exponentially', 'export', 'exporting', 'exposed', 'exposure', 'expresident', 'expression', 'expressly', 'extend', 'extended', 'extending', 'extends', 'extension', 'extensive', 'extent', 'extra', 'extreme', 'extremely', 'extremist', 'eye', 'fabiflu', 'face', 'facebook', 'facemask', 'facilitate', 'facilitating', 'facilitation', 'facilities\\u200b\\u2063', 'facility', 'facing', 'fact', 'fact-check', 'factchecked', 'factchecking', 'factchecks', 'factor', 'factory', 'fad', 'fade', 'fail', 'failed', 'failing', 'fails', 'failure', 'fair', 'faith', 'fake', 'fall', 'fallen', 'falling', 'false', 'falsehood', 'family', 'famous', 'fan', 'faq', 'far', 'farmer', 'fascinating', 'fascist', 'fashion', 'fast', 'faster', 'fastest', 'fatal', 'fatality', 'father', 'fatigue', 'fauccis', 'fauci', 'faucian', 'favipiravir', 'favor', 'fbi', 'fct', 'fct138', 'fct14', 'fct25', 'fct26', 'fct29', 'fct34', 'fct35', 'fct38', 'fct52', 'fct60', 'fda', 'fear', 'feature', 'feb', 'february', 'federal', 'feedback', 'feel', 'feeling', 'fell', 'fellowship', 'felt', 'fema', 'female', 'fennel', 'ferguson', 'fernández', 'fever', 'fewer', 'fewest', 'fibrosis', 'fiction', 'field', 'fifth', 'fight', 'fighting', 'figure', 'figuring', 'file', 'filed', 'filipino', 'filled', 'film', 'filmed', 'filming', 'final', 'finally', 'finance', 'financial', 'financially', 'find', 'finding', 'fine', 'fined', 'fire', 'fired', 'firing', 'first', 'fiscal', 'fit', 'five', 'fixed', 'fl', 'flag', 'flattened', 'flawed', 'flight', 'floor', 'florida', 'flour', 'flouting', 'flu', 'flying', 'focus', 'focused', 'folk', 'follow', 'followed', 'following', 'follows', 'followup', 'food', 'foot', 'footage', 'for', 'force', 'forced', 'forecast', 'foreign', 'foreseeable', 'forever', 'forget', 'form', 'former', 'fortaleza', 'forward', 'fought', 'found', 'foundation', 'four', 'fourteen', 'fourth', 'fox', 'france', 'francis', 'fraud', 'free', 'freedom', 'freely', 'french', 'frequency', 'frequently', 'fresh', 'friday', 'friend', 'from', 'front', 'frontline', 'fruit', 'ft', 'fuck', 'full', 'fully', 'function', 'fund', 'funded', 'funding', 'funeral', 'fungal', 'funny', 'future', 'ga', 'gain', 'galicia', 'game', 'gandhi', 'ganga', 'gap', 'gargle', 'gargling', 'garlic', 'gate', 'gather', 'gathering', 'gave', 'general', 'generally', 'generated', 'generation', 'genetic', 'george', 'georgia', 'germ', 'german', 'germany', 'get', 'getting', 'ghana', 'ghislaine', 'gift', 'gilead', 'ginger', 'girl', 'give', 'given', 'giving', 'glad', 'global', 'globally', 'globe', 'glove', 'go', 'goal', 'god', 'goi', 'going', 'gold', 'gombe', 'gombe1', 'gombe2', 'gombe3', 'gombe4', 'gombe5', 'gone', 'good', 'goodbye', 'google', 'got', 'gotten', 'gov', 'government', 'government’s', 'governor', 'govt', 'gown', 'gp', 'gps', 'grade', 'graded', 'granted', 'graph', 'graphic', 'grateful', 'grave', 'graveyard', 'great', 'greater', 'greatest', 'green', 'gretchen', 'grim', 'grocery', 'ground', 'groundbreaking', 'group', 'grow', 'growing', 'growth', 'guess', 'guest', 'guidance', 'guide', 'guideline', 'guidelinesnotifications', 'gujarat', 'gun', 'guy', 'gym', 'h1n1', 'ha', 'habit', 'hair', 'half', 'hall', 'hamster', 'hancock', 'hand', 'handed', 'handing', 'handle', 'handling', 'hangover', 'hank', 'hantavirus', 'happen', 'happened', 'happening', 'happens', 'happy', 'hard', 'harding', 'hardship', 'harm', 'harmful', 'harris', 'harvard', 'hasnt', 'hasn’t', 'have', 'havent', 'haven’t', 'hcq', 'he', 'head', 'headache', 'headline', 'headroom', 'heal', 'health', 'healthcare', 'healthline', 'healthy', 'health\\u2063', 'hear', 'heard', 'hearing', 'heart', 'heat', 'heath', 'heavy', 'height', 'held', 'hell', 'hello', 'help', 'helped', 'helping', 'helpline', 'hence', 'her', 'herbal', 'herd', 'here', 'here’s', 'hero', 'he’s', 'hhs', 'hi', 'high', 'higher', 'highest', 'highlight', 'highlighting', 'highly', 'highrisk', 'hill', 'hindu', 'his', 'historic', 'historical', 'history', 'hit', 'hiv', 'hmh', 'hoax', 'hold', 'holding', 'holiday', 'home', 'homemade', 'homeopathic', 'honey', 'hong', 'honjo', 'honourable', 'hope', 'hoping', 'hopkins', 'horse', 'hosp', 'hospital', 'hospitality', 'hospitalization', 'hospitalized', 'hospitallevel', 'host', 'hot', 'hotel', 'hotline', 'hotspot', 'hour', 'house', 'household', 'housing', 'how', 'however', 'hr', 'https://t.co/1ato9qo0he', 'https://t.co/25nxirzawb.', 'https://t.co/2caogenfur', 'https://t.co/2l2qxh0m45', 'https://t.co/2rvg3dwbkt.', 'https://t.co/2xlf9mccft.', 'https://t.co/3lb8xsviva', 'https://t.co/4ku7nklzcq', 'https://t.co/4yblkisg6u', 'https://t.co/5l9jptglai', 'https://t.co/5rbbdr4r0u', 'https://t.co/6kydz6twwe', 'https://t.co/9zn2zytaud.', 'https://t.co/adsrezpsfh', 'https://t.co/cs7uxqrrmn', 'https://t.co/fegmdobt0q.', 'https://t.co/fqhg5m5xrn', 'https://t.co/ft6cgmampx', 'https://t.co/ft6cgmampx.', 'https://t.co/ftca9h75oa', 'https://t.co/g8duiqxiwr.', 'https://t.co/gdfjrxl032', 'https://t.co/gialss3xd7', 'https://t.co/gpqyw8gnx3', 'https://t.co/gudbpy0x9d.', 'https://t.co/i0ya5yftvz', 'https://t.co/igh6ua8oha.', 'https://t.co/ihbwco8lbl', 'https://t.co/ioqv5omdka.', 'https://t.co/ip0gahip7q.', 'https://t.co/isbyfxnv5a.', 'https://t.co/jkwwztfwss.', 'https://t.co/jkwwztxyhs.', 'https://t.co/jw9vsogjgo', 'https://t.co/jy6ojd1g7y', 'https://t.co/ktyazabu4m', 'https://t.co/kulxwpdc15.', 'https://t.co/kzncyfx70q', 'https://t.co/lcfch0fxct', 'https://t.co/lmiaablmji', 'https://t.co/lmr9xro4wi', 'https://t.co/lqlfumwmvu', 'https://t.co/lqlfunenns', 'https://t.co/lvzejloxmh.', 'https://t.co/lxwme4nubd.', 'https://t.co/mcp09udspe', 'https://t.co/mlj56qcw37.', 'https://t.co/mp3spdaktl', 'https://t.co/mpuji2yq0v', 'https://t.co/n4zdlimwkk', 'https://t.co/n6iwcmznbk', 'https://t.co/n8oor2o8ep', 'https://t.co/nyzlpjyfxm', 'https://t.co/opc8z4tnib', 'https://t.co/oz9kcdajij', 'https://t.co/pagzfxjnbi', 'https://t.co/pgxjssssu9', 'https://t.co/pqd1i8xgam', 'https://t.co/pzrmh3tkeq', 'https://t.co/pzrmh4bl5y', 'https://t.co/q1dtcxukin.', 'https://t.co/sldrvxxfcz', 'https://t.co/sldrvxxfcz.', 'https://t.co/swcqc3xlyj', 'https://t.co/tt49zoec8n.', 'https://t.co/tt49zon1hf.', 'https://t.co/uargztrh5l.', 'https://t.co/uwscnvzq9m', 'https://t.co/vih89d6gzj', 'https://t.co/wiufbkr3uh', 'https://t.co/wlbfspafzw', 'https://t.co/wlp3gx6f5t', 'https://t.co/wwdyvm3nnl', 'https://t.co/xdfm4f80bx', 'https://t.co/xrey2ncs7n', 'https://t.co/y3qswuj23i', 'https://t.co/yapqkvdncj', 'https://t.co/z9k0shxj1g.', 'https://t.co/zc39azvrge', 'https://t.co/zfgtnxmadb', 'https://t.co/zp4vylo0pb', 'https://t.co/zp4vylo0pb.', 'https://t.co/zqrpneofet', 'https://t.co/zxferiys6i', 'https:…', 'huawei', 'huddled', 'huge', 'hugging', 'human', 'humanitarian', 'humanity', 'humanmade', 'hundred', 'hunger', 'hungry', 'hurricane', 'hurt', 'husband', 'hussain', 'hussein', 'hut', 'hv', 'hyderabad', 'hydroxichloroquine', 'hydroxy', 'hydroxychloroquine', 'hygiene', 'hypermarket', 'hypochlorite', 'hypoxia', 'h…', 'i', 'ibadan', 'ibom', 'ibom11', 'ibom13', 'ibrahim', 'ibuprofen', 'icai', 'ice', 'icmr', 'icu', 'icu\\u2063', 'icymi', 'id', 'idaho', 'idea', 'identification', 'identified', 'identify', 'identifying', 'idiocy', 'ie', 'if', 'ignorance', 'ignore', 'ignoring', 'ii', 'iii', 'iit', 'ill', 'illegal', 'illegally', 'illinois', 'illness', 'im', 'image', 'imam', 'imf', 'immediate', 'immediately', 'immigrant', 'immigration', 'immune', 'immunisation', 'immunity', 'immunization', 'immunoresponse', 'immunosuppression', 'imo', 'imo1', 'imo12', 'imo2', 'imo3', 'imo5', 'imo9', 'impact', 'impacted', 'impeachment', 'imperial', 'implant', 'implement', 'implementation', 'implemented', 'implication', 'implying', 'importance', 'important', 'importantly', 'imported', 'impose', 'imposed', 'impossible', 'impressive', 'improve', 'improved', 'improvement', 'improves', 'improving', 'imran', 'in', 'inching', 'incidence', 'incident', 'incl', 'include', 'included', 'includes', 'including', 'inclusion', 'income', 'incompetence', 'incomplete', 'increase', 'increased', 'increasing', 'incubation', 'incurred', 'independent', 'index', 'india', 'india)', 'india.', 'indian', 'indiana', 'india’s', 'indicate', 'indicated', 'indicates', 'indicating', 'indication', 'indicator', 'indigenous', 'indigenously', 'individual', 'indonesia', 'indonesian', 'indoors', 'indore', 'industrial', 'industrialist', 'industry', 'ineffective', 'inequity', 'inevitable', 'infant', 'infect', 'infected', 'infecting', 'infection', 'infectious', 'infects', 'inference', 'inflammation', 'inflammatory', 'inflating', 'influence', 'influenza', 'info', 'infodemic', 'inform', 'information', 'informed', 'inhalation', 'inhale', 'inhaling', 'inhibitor', 'initial', 'initiate', 'initiated', 'initiation', 'initiative', 'injected', 'injecting', 'injection', 'injury', 'inmate', 'inperson', 'inquiry', 'inserted', 'inside', 'insight', 'insisting', 'insists', 'instagram', 'instant', 'instead', 'institute', 'institution', 'institutional', 'instructed', 'instruction', 'instrument', 'insufficient', 'insurance', 'intelligence', 'intelligent', 'intended', 'intense', 'intensify', 'intensive', 'interconnectedness', 'interest', 'interested', 'interferon', 'interim', 'intermediate', 'international', 'internet', 'interpret', 'interpretation', 'interruption', 'interstate', 'intervention', 'interview', 'into', 'intravenous', 'introduce', 'introduced', 'introduces', 'introduction', 'invalid', 'invention', 'inventor', 'invest', 'investigate', 'investigated', 'investigating', 'investigation', 'investigator', 'investment', 'invisible', 'invite', 'invoking', 'involve', 'involved', 'involving', 'in…', 'iodine', 'ipc', 'iran', 'ireland', 'is', 'islam', 'island', 'isnt', 'isn’t', 'isolate', 'isolated', 'isolating', 'isolation', 'israel', 'issue', 'issued', 'it', 'italian', 'italy', 'item', 'it’s', 'ive', 'ivermectin', 'i…', 'jacksonville', 'jail', 'jair', 'jamaat', 'jan', 'janeiro', 'january', 'japan', 'japanese', 'jeevan', 'jersey', 'jesus', 'jet', 'ji', 'jigawa2', 'jihadi', 'jinping', 'job', 'joe', 'john', 'johnson', 'join', 'joined', 'journal', 'journalist', 'joão', 'juice', 'july', 'jump', 'jumped', 'jumping', 'june', 'jurisdiction', 'just', 'justin', 'jyoti', 'k', 'ka', 'kaduna', 'kaduna10', 'kaduna17', 'kaduna19', 'kaduna23', 'kaduna6', 'kaduna8', 'kaduna9', 'kalonji', 'kano', 'kano1', 'kano2', 'kano3', 'kano4', 'kano5', 'kano6', 'kano73', 'kansa', 'karnataka', 'katsina', 'katsina1', 'katsina21', 'katsina6', 'katsina7', 'kebbi2', 'keep', 'keeping', 'keir', 'kejriwal', 'kentucky', 'kenya', 'kept', 'kerala', 'key', 'khan', 'kia', 'kick', 'kid', 'kidney', 'kill', 'killed', 'killing', 'kind', 'kindly', 'king', 'kingdom', 'kit', 'kiwi', 'knew', 'know', 'knowledge', 'known', 'kong', 'korea', 'kowheori19', 'kumar', 'kwara', 'kwara10', 'kwara11', 'kwara4', 'kwara5', 'ky', 'la', 'lab', 'labconfirmed', 'label', 'labeled', 'laboratory', 'labour', 'lack', 'lady', 'lag', 'lagos', 'lagos33', 'laid', 'lakh', 'landmark', 'language', 'lanka', 'lankan', 'large', 'largely', 'larger', 'largest', 'last', 'late', 'later', 'latest', 'launch', 'launched', 'laureate', 'law', 'lawsuit', 'laying', 'le', 'lead', 'leader', 'leadership', 'leading', 'leaf', 'leaked', 'leapfrogged', 'learn', 'learned', 'learning', 'least', 'leave', 'leaving', 'led', 'left', 'legal', 'legionnaire', 'lemon', 'length', 'let', 'letter', 'let’s', 'level', 'lgas', 'li', 'liaison', 'licking', 'lieber', 'life', 'lifesaving', 'lifethreatening', 'lift', 'lifted', 'light', 'like', 'likely', 'limit', 'limited', 'limiting', 'line', 'link', 'linked', 'linking', 'lion', 'liquid', 'list', 'listed', 'listen', 'little', 'live', 'liver', 'living', 'load', 'local', 'locally', 'located', 'location', 'lock', 'lockdown', 'lockdown.', 'locked', 'log', 'logo', 'london', 'long', 'longer', 'longterm', 'look', 'looked', 'looking', 'lorry', 'los', 'lose', 'loses', 'losing', 'loss', 'lost', 'lot', 'louisiana', 'love', 'loved', 'low', 'lower', 'lowerincome', 'lowest', 'ltc', 'ltd', 'lucky', 'luke', 'lumped', 'lung', 'lying', 'lysol', 'm', 'ma', 'macedonia', 'macedonian', 'machine', 'madagascan', 'madagascar', 'maddow', 'made', 'madhya', 'madrid', 'maduro', 'magazine', 'magic', 'magne', 'maharashtra', 'mail', 'main', 'maine', 'maintain', 'maintained', 'maintaining', 'maintains', 'major', 'majority', 'make', 'maker', 'making', 'malaria', 'malaysia', 'male', 'mall', 'man', 'manage', 'managed', 'management', 'manager', 'managing', 'manatū', 'mandate', 'mandated', 'mandatory', 'manipulated', 'manipulation', 'manipur', 'manmade', 'mannequin', 'mantri', 'manual', 'manufacture', 'manufactured', 'manufacturer', 'manufacturing', 'many', 'man’s', 'map', 'marapr', 'march', 'margaret’s', 'marine', 'mark', 'market', 'marriage', 'marseille', 'martin', 'maryland', 'masjid', 'mask', 'mask-wearing', 'masked', 'masking', 'maskwearing', 'mass', 'massachusetts', 'massive', 'mast', 'match', 'material', 'maternity', 'matt', 'matter', 'maulana', 'maximum', 'maxwell', 'may', 'maybe', 'mayor', 'mcconnell', 'mean', 'meaning', 'meant', 'meantime', 'meanwhile', 'measles', 'measure', 'meat', 'meatpacking', 'mechanical', 'mechanism', 'median', 'medic', 'medical', 'medicare', 'medication', 'medicine', 'medium', 'medscape', 'meet', 'meeting', 'megha', 'melbourne', 'melinda', 'member', 'membrane', 'meme', 'memo', 'men', 'mental', 'mentioned', 'merit', 'merkel', 'mers', 'message', 'messaging', 'messed', 'messonnier', 'messy', 'met', 'metformin', 'method', 'methodological', 'methylxanthine', 'metre', 'metric', 'metro', 'mexico', 'mi', 'mic', 'michael', 'michigan', 'michigan’s', 'micro', 'microchip', 'microphase', 'microsoft', 'mid-april', 'mid-may', 'mid-october', 'middle', 'middleincome', 'middlemore', 'middleton', 'midnight', 'midst', 'midwest', 'might', 'migrant', 'mike', 'milan', 'mild', 'milestone', 'military', 'milk', 'million', 'min', 'mind', 'mine', 'mineral', 'minister', 'ministry', 'minority', 'minute', 'miq', 'miracle', 'misinformation', 'misleading', 'missed', 'missing', 'mission', 'mississippi', 'missouri', 'mistake', 'mitch', 'mitigate', 'mix', 'mla', 'mm', 'mobile', 'mobility', 'mode', 'model', 'modelling', 'moderate', 'modi', 'molecular', 'moment', 'monday', 'money', 'monitor', 'monitoring', 'monkey', 'montanari', 'month', 'more', 'morning', 'mortality', 'mortgage', 'mosque', 'mosquito', 'most', 'mostly', 'mother', 'mountain', 'mouth', 'mouthwash', 'move', 'moved', 'movement', 'moving', 'mp', 'mr', 'mri', 'mrna1273', 'mt', 'much', 'multiple', 'multisystem', 'mumbai', 'murder', 'music', 'muslim', 'must', 'mustard', 'my', 'myanmar', 'mystery', 'n', 'n95', 'nadu', 'nagpur', 'namaz', 'name', 'named', 'nanavati', 'nancy', 'narendra', 'nasal', 'nasarawa1', 'nasarawa2', 'nasarawa3', 'nasarawa8', 'nashville', 'nation', 'national', 'nationally', 'nationwide', 'nation’s', 'natural', 'nature', 'nba', 'ncdc', 'ne', 'near', 'nearly', 'nears', 'nebraska', 'necessarily', 'necessary', 'need', 'needed', 'negative', 'negligence', 'neighbor', 'neighbour', 'neil', 'neither', 'net', 'netherlands', 'network', 'nevada', 'never', 'new', 'newborn', 'newcase', 'newest', 'newly', 'news', 'newsletter', 'newspaper', 'next', 'nh', 'nicola', 'niger', 'niger1', 'niger2', 'nigeria', 'nigerian', 'nigeria’s', 'night', 'nih', 'nine', 'nipah', 'nj', 'no', 'nobel', 'nobody', 'none', 'nonessential', 'nonhispanic', 'noninvasive', 'norm', 'normal', 'normally', 'norris', 'north', 'northeast', 'northeastern', 'northern', 'nose', 'nostradamus', 'not', 'note', 'noted', 'nothing', 'notice', 'noticed', 'noting', 'novel', 'november', 'novotel', 'now', 'now)', 'now”', 'nude', 'number', 'nurse', 'nursing', 'nutrition', 'nv', 'ny', 'nyc', 'nyt', 'nz', 'obama', 'obamacare', 'obesity', 'obey', 'object', 'observation', 'observed', 'observing', 'obtain', 'obvious', 'obviously', 'occur', 'occurred', 'occurring', 'occurs', 'oct', 'october', 'odd', 'odds', 'odisha', 'of', 'off', 'offence', 'offending', 'offense', 'offer', 'offered', 'offering', 'office', 'officer', 'official', 'officially', 'often', 'of…', 'ogun', 'ogun1', 'ogun12', 'ogun13', 'ogun14', 'ogun19', 'ogun2', 'ogun29', 'ogun3', 'ogun35', 'ogun4', 'ogun6', 'ogun7', 'ogun9', 'oh', 'ohio', 'oil', 'ok', 'oklahoma', 'old', 'older', 'olive', 'olympics', 'on', 'once', 'ondo', 'ondo1', 'ondo11', 'ondo16', 'ondo18', 'ondo2', 'ondo3', 'ondo4', 'ondo46', 'ondo5', 'ondo6', 'ondo8', 'one', 'oneday', 'onefifth', 'oneself', 'one’s', 'ongoing', 'onion', 'online', 'onlinedistance', 'only', 'onset', 'onto', 'open', 'opened', 'opening', 'operate', 'operation', 'opportunity', 'opposite', 'opposition', 'opt', 'optimisation', 'optimistic', 'optimization', 'option', 'or', 'order', 'ordered', 'oregon', 'organisation', 'organization', 'organization\\u200b', 'organization\\u2063', 'origin', 'original', 'originated', 'os', 'osun', 'osun1', 'osun2', 'osun20', 'osun3', 'other', 'others', 'our', 'out', 'outbreak', 'outcome', 'outlet', 'outside', 'outwards', 'over', 'overall', 'overcome', 'overloaded', 'overnight', 'overseas', 'overwhelmed', 'overwhelming', 'owner', 'oxford', 'oxygen', 'oyo', 'oyo17', 'oyo18', 'oyo20', 'oyo8', 'pacific', 'pack', 'package', 'packed', 'packet', 'page', 'paid', 'pain', 'pakistan', 'pakistani', 'panama', 'pandemic', 'pandemic.', 'panel', 'panic', 'pant', 'paper', 'parameter', 'parent', 'paris', 'park', 'parliament', 'part', 'partial', 'partially', 'participant', 'participate', 'participating', 'particle', 'particular', 'particularly', 'partner', 'partnership', 'party', 'party’s', 'pas', 'pass', 'passed', 'passenger', 'past', 'patanjali', 'patel', 'patent', 'patient', 'patients’', 'patient’s', 'patrick', 'pattern', 'paul', 'paulo', 'pause', 'pay', 'paying', 'payment', 'pcr', 'peak', 'peaked', 'pediatric', 'pegged', 'pelosi', 'pending', 'penny', 'people', 'people”', 'pepper', 'per', 'percapita', 'percent', 'percentage', 'performance', 'performed', 'performing', 'period', 'permanently', 'permission', 'permitted', 'person', 'personal', 'personnel', 'perspective', 'pet', 'ph', 'pharma', 'pharmaceutical', 'phase', 'philippine', 'phone', 'photo', 'photograph', 'physical', 'physically', 'physician', 'pib', 'pick', 'picked', 'picking', 'picture', 'pie', 'piece', 'pile', 'pin', 'place', 'placed', 'plague', 'plan', 'plandemic', 'planned', 'planning', 'plasma', 'plastic', 'plateau', 'plateau1', 'plateau11', 'plateau18', 'plateaued', 'platform', 'play', 'player', 'please', 'pleased', 'pledge', 'plenty', 'plot', 'pls', 'plus', 'plz', 'pm', 'pneumonia', 'podcast', 'poem', 'point', 'pointed', 'pointing', 'poison', 'pole', 'police', 'policeman', 'policy', 'polio', 'political', 'politician', 'politifact', 'poll', 'polling', 'pollutant', 'pollution', 'pondicherry', 'pool', 'pooled', 'poor', 'pope', 'popular', 'population', 'port', 'portal', 'portfolio', 'portion', 'portuguese', 'pose', 'posed', 'position', 'positive', 'positives.', 'positivity', 'possibility', 'possible', 'post', 'posted', 'poster', 'posting', 'postpone', 'postponed', 'postregistration', 'potential', 'potentially', 'poverty', 'powder', 'power', 'powerful', 'ppe', 'ppes', 'ppi', 'ppl', 'practice', 'practicing', 'practitioner', 'pradesh', 'pradhan', 'praise', 'praised', 'prattle', 'pray', 'prayed', 'prayer', 'praying', 'precaution', 'precautionary', 'preceding', 'precipitously', 'predict', 'predicted', 'prediction', 'predictive', 'predicts', 'preexisting', 'preferred', 'pregnancy', 'pregnant', 'premier', 'premji', 'prepare', 'prepared', 'preparedness', 'prepares', 'preparing', 'preregister', 'pres', 'prescribe', 'prescribed', 'prescribing', 'prescription', 'present', 'presentation', 'presented', 'presenting', 'presently', 'president', 'presidential', 'president’s', 'press', 'pressure', 'presumably', 'presumptive', 'presymptomatic', 'pretend', 'pretty', 'prevalence', 'prevent', 'prevented', 'preventing', 'prevention', 'preventive', 'prevents', 'previous', 'previously', 'price', 'priest', 'primarily', 'primary', 'primate', 'prime', 'prince', 'princess', 'principal', 'prior', 'prioritised', 'priority', 'prison', 'prisoner', 'priti', 'privacy', 'private', 'priyanka', 'prize', 'prizewinning', 'probability', 'probable', 'probably', 'problem', 'problem”', 'procedure', 'proceeding', 'process', 'processed', 'proclaim', 'procurement', 'produce', 'produced', 'product', 'production', 'prof', 'professional', 'professor', 'profile', 'profit', 'program', 'programme', 'progress', 'progression', 'progressive', 'progressively', 'prohibited', 'prohibits', 'project', 'projected', 'projecting', 'projection', 'promise', 'promising', 'promoting', 'prompt', 'pronounced', 'proof', 'propaganda', 'proper', 'properly', 'property', 'prophylactic', 'proportion', 'proposed', 'prospect', 'protect', 'protected', 'protecting', 'protection', 'protective', 'protects', 'protein', 'protest', 'protesting', 'protestors', 'protocol', 'proud', 'prove', 'proved', 'proven', 'provide', 'provided', 'provider', 'provides', 'providing', 'province', 'proving', 'pub', 'public', 'publication', 'publish', 'published', 'pulled', 'pune', 'punishable', 'pupil', 'purchase', 'purchasing', 'purported', 'purportedly', 'purpose', 'push', 'pushed', 'pushing', 'put', 'putin', 'putting', 'pvt', 'p…', 'q', 'qanon', 'qr', 'quality', 'quarantine', 'quarantined', 'quarantining', 'quarter', 'queen', 'question', 'queue', 'quick', 'quickly', 'quietly', 'quinine', 'quite', 'quote', 'quoted', 'quran', 'r', 'r&ampd', 'ra', 'raab', 'race', 'racial', 'racist', 'radiation', 'radio', 'rahul', 'rai', 'railway', 'raise', 'rajasthan', 'rally', 'ram', 'ramdev', 'ramped', 'ramu', 'range', 'rapid', 'rapidly', 'rash', 'ratan', 'rate', 'rated', 'rather', 'ratio', 'raw', 'ray', 'rayner', 're-opening', 'reach', 'reached', 'reaching', 'reacts', 'read', 'readily', 'reading', 'ready', 'real', 'reality', 'really', 'realtime', 'reason', 'reassured', 'receive', 'received', 'receiving', 'recent', 'recently', 'recognize', 'recommend', 'recommendation', 'recommended', 'recommends', 'record', 'recorded', 'recording', 'recover', 'recovered', 'recovering', 'recovery', 'red', 'redfield', 'reduce', 'reduced', 'reduces', 'reducing', 'reduction', 'refer', 'reference', 'referred', 'reflect', 'reflects', 'refuse', 'refused', 'refusing', 'regarding', 'regardless', 'regime', 'region', 'regional', 'register', 'registered', 'registration', 'registry', 'regular', 'regularly', 'regulation', 'reject', 'related', 'relationship', 'relative', 'relatively', 'relaxed', 'release', 'released', 'releasing', 'reliable', 'reliably', 'relief', 'religious', 'relying', 'remain', 'remained', 'remaining', 'remains', 'remdesivir', 'remedy', 'remember', 'reminder', 'removed', 'rent', 'reopen', 'reopened', 'reopening', 'reopenings', 'reopens', 'repeat', 'repeatedly', 'report', 'reported', 'reportedly', 'reporter', 'reporting', 'report\\u2063', 'represent', 'representative', 'represented', 'representing', 'republican', 'request', 'requested', 'requesting', 'require', 'required', 'requirement', 'requires', 'requiring', 'rescue', 'research', 'researcher', 'residence', 'resident', 'resigned', 'resolution', 'resorting', 'resource', 'respect', 'respectively', 'respirator', 'respiratory', 'respond', 'responder', 'responding', 'response', 'responsibility', 'responsible', 'rest', 'restart', 'restaurant', 'restriction', 'restrictive', 'result', 'resulted', 'resume', 'resurgence', 'retail', 'retested', 'return', 'returned', 'returnees', 'returning', 'reveal', 'revealed', 'revealing', 'reveals', 'review', 'reviewed', 'revised', 'richard', 'rid', 'right', 'rio', 'rise', 'risen', 'rising', 'risk', 'risking', 'risky', 'river', 'river3', 'rivers1', 'rivers12', 'rivers2', 'rivers3', 'rna', 'road', 'rob', 'robert', 'roche', 'rodrigo', 'role', 'rolled', 'rolling', 'rollout', 'ron', 'ronaldo', 'room', 'rose', 'roskill', 'rotorua', 'roughly', 'round', 'routine', 'row', 'rt', 'rtpcr', 'rule', 'ruled', 'ruling', 'rumor', 'rumour', 'run', 'running', 'rupee', 'rural', 'rushed', 'russia', 'russian', 'sacrifice', 'sadiq', 'sadly', 'safe', 'safely', 'safety', 'sage', 'said', 'saint', 'salad', 'salary', 'sale', 'saliva', 'salt', 'same', 'sample', 'san', 'sander', 'sanitizer', 'sanitizers', 'sarah', 'sars', 'sars-cov-2', 'sarscov2', 'sat', 'saturday', 'saudi', 'save', 'saved', 'saving', 'saw', 'say', 'saying', 'sc', 'scale', 'scaled', 'scaling', 'scan', 'scenario', 'schedule', 'scheme', 'school', 'science', 'scientific', 'scientifically', 'scientist', 'score', 'scotland', 'screen', 'screening', 'screenshot', 'sea', 'sealed', 'search', 'season', 'seasonal', 'seat', 'seated', 'second', 'secondary', 'secondhighest', 'secret', 'secretary', 'secretly', 'section', 'sector', 'security', 'see', 'seed', 'seeing', 'seek', 'seeking', 'seem', 'seems', 'seen', 'segment', 'self', 'self-isolate', 'self-isolation', 'selfchecker', 'selfisolate', 'selfisolating', 'selfisolation', 'selfquarantine', 'selling', 'senate', 'senator', 'send', 'sending', 'senegal', 'senior', 'sense', 'sent', 'sep', 'separate', 'separated', 'sept', 'september', 'series', 'serious', 'seriously', 'serum', 'serve', 'server', 'service', 'set', 'setting', 'seven', 'sevenday', 'several', 'severe', 'severely', 'severity', 'sex', 'shadow', 'shah', 'shall', 'shape', 'share', 'shared', 'sharepic', 'sharing', 'sharp', 'shave', 'shaving', 'she', 'sheet', 'shenzhen', 'shield', 'shift', 'ship', 'shit', 'shiva', 'shooting', 'shop', 'shopper', 'shopping', 'shore', 'short', 'shortage', 'shortly', 'shortness', 'shot', 'should', 'shouldnt', 'show', 'showed', 'showing', 'shown', 'shrinking', 'shut', 'shutdown', 'shutting', 'sick', 'side', 'sign', 'signal', 'signed', 'significant', 'significantly', 'signing', 'silver', 'similar', 'simple', 'simply', 'since', 'singapore', 'singh', 'singing', 'single', 'singleday', 'sir', 'sister', 'site', 'sitting', 'situation', 'six', 'sixth', 'skill', 'skilled', 'skin', 'sky', 'slack', 'slaughter', 'sleep', 'slightly', 'slow', 'slowed', 'slowing', 'slowly', 'small', 'smaller', 'smoke', 'smoking', 'sneeze', 'sneezing', 'so', 'soap', 'social', 'society', 'soda', 'sodium', 'sokoto', 'sokoto1', 'sokoto2', 'sold', 'solid', 'solidarity', 'solution', 'solve', 'some', 'somehow', 'someone', 'something', 'sometimes', 'son', 'soon', 'sooner', 'sop', 'sore', 'soros', 'sort', 'sound', 'source', 'south', 'southeast', 'southern', 'space', 'spain', 'spanish', 'speak', 'speaker', 'speaking', 'special', 'specialist', 'specific', 'specimen', 'speed', 'speedy', 'spend', 'spent', 'spike', 'spit', 'spitting', 'spoke', 'spoken', 'sport', 'spot', 'spray', 'spread', 'spreading', 'spring', 'sri', 'st', 'stable', 'stacked', 'stadium', 'staff', 'stage', 'stand', 'standard', 'standing', 'stanford', 'star', 'starmer', 'start', 'started', 'starting', 'state', 'stated', 'statement', 'states/uts', 'statesuts', 'statewise', 'state’s', 'stating', 'station', 'statistic', 'stats', 'status', 'stay', 'stayathome', 'staying', 'steadily', 'steady', 'steam', 'steep', 'step', 'steroid', 'stick', 'stigma', 'still', 'stimulus', 'stock', 'stomach', 'stone', 'stop', 'stopped', 'stopping', 'store', 'storm', 'story', 'straight', 'strain', 'stranded', 'strategic', 'strategy', 'street', 'strengthen', 'strengthening', 'stress', 'stressful', 'strict', 'strong', 'stronger', 'struggle', 'struggling', 'student', 'study', 'stuff', 'stupid', 'sturgeon', 'subject', 'submarine', 'substance', 'substantial', 'substantially', 'success', 'successful', 'successfully', 'successive', 'such', 'sudden', 'suffer', 'suffering', 'sufficient', 'suggest', 'suggested', 'suggesting', 'suggestion', 'suggests', 'suicide', 'sum', 'summary', 'summer', 'sun', 'sunday', 'sunlight', 'super', 'supermarket', 'supplement', 'supply', 'support', 'supported', 'supporter', 'supporting', 'supposed', 'supposedly', 'suppress', 'suppressed', 'supreme', 'suraksha', 'surat', 'sure', 'surface', 'surge', 'surgery', 'surge”', 'surgical', 'surpass', 'surpassed', 'surprised', 'surveillance', 'survey', 'survival', 'survive', 'survived', 'suspect', 'suspected', 'suspended', 'sustained', 'swab', 'sweden', 'sweet', 'swimming', 'swine', 'switzerland', 'symptom', 'symptomatic', 'syndrome', 'system', 'systematic', 'são', 'table', 'tablet', 'tablighi', 'tackle', 'tag', 'taiwan', 'tak', 'take', 'taken', 'taking', 'talk', 'talked', 'talking', 'tally', 'tamil', 'tank', 'taraba3', 'target', 'task', 'taste', 'tasuku', 'tata', 'tax', 'tb', 'tea', 'teacher', 'teaching', 'team', 'tech', 'technology', 'teen', 'telangana', 'telemedicine', 'tell', 'telling', 'temperature', 'temple', 'temporary', 'ten', 'tennessee', 'term', 'territory', 'test', 'tested', 'testing', 'texas', 'text', 'thailand', 'than', 'thane', 'thank', 'thanks', 'that', 'thats', 'that’s', 'that”', 'the', 'their', 'then', 'theory', 'therapeutic', 'therapy', 'there', 'therefore', 'there’s', 'thermal', 'these', 'they', 'theyre', 'they’re', 'the…', 'thing', 'think', 'thinking', 'third', 'this', 'those', 'though', 'thought', 'thousand', 'thread', 'threat', 'threatens', 'three', 'throat', 'thrombosis', 'through', 'throughout', 'throwing', 'thrown', 'thursday', 'thus', 'tick', 'ticked', 'tiger', 'tighter', 'till', 'time', 'timeline', 'timely', 'time”', 'tip', 'tipping', 'to', 'today', 'today\\u200b', 'today’s', 'together', 'toilet', 'tokoroa', 'told', 'toll', 'tom', 'tomorrow', 'tonight', 'took', 'tool', 'toolkit', 'top', 'topic', 'total', 'totally', 'totaltestresults', 'tota…', 'touch', 'touched', 'touching', 'tough', 'touted', 'towards', 'tower', 'town', 'trace', 'traced', 'tracer', 'tracing', 'track', 'tracked', 'tracker', 'tracking', 'trade', 'traditional', 'train', 'trained', 'training', 'transferred', 'transmission', 'transmit', 'transmitted', 'transmitting', 'transparency', 'transport', 'transportation', 'travel', 'traveler', 'travelled', 'traveller', 'travelling', 'treat', 'treated', 'treating', 'treatment', 'trend', 'trending', 'trial', 'tribal', 'trick', 'tried', 'trillion', 'trip', 'trouble', 'truck', 'trudeaus', 'true', 'truenat', 'truly', 'trump', 'trump’s', 'trust', 'truth', 'try', 'trying', 'tshirt', 'tuesday', 'tune', 'tunisian', 'turkey', 'turn', 'turned', 'turning', 'tv', 'tweet', 'tweeted', 'twenty', 'twice', 'twitter', 'two', 'tx', 'type', 't…', 'u', 'uganda', 'uk', 'ultimately', 'umbrella', 'un', 'unable', 'uncertainty', 'unchanged', 'unclear', 'uncomfortable', 'under', 'undergo', 'underlying', 'understand', 'understanding', 'understands', 'underway', 'unemployed', 'unemployment', 'unfortunately', 'unicef', 'union', 'unique', 'unit', 'united', 'universal', 'university', 'unknown', 'unless', 'unlike', 'unlikely', 'unprecedented', 'unwell', 'up', 'update', 'updated', 'update\\u200b', 'updating', 'upgraded', 'upon', 'upper', 'upsurge', 'upto', 'urge', 'urged', 'urgent', 'urgently', 'urging', 'urine', 'us', 'usa', 'usage', 'use', 'used', 'useful', 'useless', 'user', 'using', 'usual', 'usually', 'ut', 'utah', 'uttar', 'uv', 'v', 'vaccinated', 'vaccination', 'vaccine', 'validated', 'vallance', 'value', 'van', 'varies', 'various', 'vast', 'vati', 'vatican', 'vegetable', 'vegetarian', 'vehicle', 'ventilation', 'ventilator', 'verified', 'vermont', 'verse', 'version', 'via', 'vice', 'victim', 'victoria', 'victorian', 'video', 'view', 'viewed', 'vinegar', 'violated', 'violating', 'violation', 'viral', 'virginia', 'virologist', 'virology', 'virtual', 'virus', 'virus.', 'virus.�', 'virus”', 'visa', 'visit', 'visited', 'visiting', 'vital', 'vitamin', 'vk', 'vladimir', 'voice', 'volleyball', 'volume', 'voluntary', 'volunteer', 'vote', 'voting', 'vp', 'vte', 'vulnerable', 'w', 'wa', 'waikato', 'wait', 'waitakere', 'waiting', 'wake', 'wale', 'walking', 'wall', 'want', 'wanted', 'war', 'ward', 'warm', 'warn', 'warned', 'warning', 'warns', 'warrior', 'wash', 'washing', 'washington', 'wasnt', 'watch', 'watching', 'water', 'wave', 'way', 'we', 'weak', 'weapon', 'wear', 'wearing', 'weather', 'webinar', 'website', 'wed', 'wedding', 'wednesday', 'weed', 'week', 'weekend', 'weekly', 'well', 'wellbeing', 'wellington', 'wenliang', 'went', 'were', 'west', 'western', 'wet', 'weve', 'we‘ve', 'we’ll', 'we’re', 'we’ve', 'what', 'whatever', 'whats', 'whatsapp', 'what’s', 'when', 'where', 'whereas', 'whether', 'which', 'while', 'whistleblower', 'white', 'whitty', 'who', 'whole', 'whose', 'who’s', 'why', 'wide', 'widely', 'widespread', 'wife', 'will', 'william', 'window', 'winning', 'winter', 'wipe', 'wiped', 'wisconsin', 'wise', 'with', 'within', 'without', 'witness', 'woman', 'wonder', 'wonderful', 'wondering', 'wont', 'won’t', 'word', 'wore', 'work', 'worked', 'worker', 'working', 'workplace', 'world', 'worldwide', 'world’s', 'world”', 'worse', 'worship', 'worst', 'worth', 'would', 'wouldnt', 'writing', 'written', 'wrong', 'wrongly', 'wrote', 'wuhan', 'wwn', 'xi', 'yan', 'yeah', 'year', 'yemen', 'yes', 'yesterday', 'yet', 'yobe1', 'yobe2', 'yobe3', 'yoga', 'yojana', 'york', 'york’s', 'you', 'youll', 'young', 'younger', 'your', 'youre', 'youth', 'youtube', 'you’re', 'you’ve', 'zealand', 'zealander', 'zealand’s', 'zero', 'zika', 'zinc', 'zone', '|', '£10000', '£500', '\\u200b', '\\u200b\\u2063', '\\u200b\\u2063\\u2063', '–', '—', '“a', '“all', '“as', '“big', '“do’s”', '“every', '“i', '“if', '“implanting', '“in', '“it', '“it’s', '“ive', '“miracle', '“new', '“only”', '“seamless”', '“the', '“this', '“totally', '“we', '“we’re', '•', '…', '\\u2063', '\\u2063\\u2063', '\\u2063\\u2063\\u2063\\u2063', '▪️', '▶️1.60', '◾', '✅', '✅avoid', '✅indias', '✅mandatory', '✅more', '✅observe', '✅over', '✅recovery', '✅wash', '✅wear', '❌', '❗', '❤️', '➡', '➡️#covid19', '➡️active', '➡️confirmed', '➡️deaths', '➡️interventions', '➡️recovered', '➡️states', '➡️total', '⠀', '🌍', '🏫', '👇', '👉', '👉https:t.cor5ssfz6cdl', '👉india', '👉maharashtra', '👉more', '👉the', '👍', '👦', '📍', '📍#covid19', '📍increasing', '📍statewise', '📍steady', '📍total', '📢#coronavirusupdates', '📱', '📺', '🔰full', '🔰read', '🔰report', '🔴', '😷', '🙌', '🙏🙏🙏', '🚶🏽', '🧪']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform"
      ],
      "metadata": {
        "id": "l7IRYu7XtBRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words = TF_IDF_Vectorizer.transform(X_train[\"tweet\"])\n",
        "bag_of_words = pd.DataFrame(bag_of_words.toarray(), index = X_train.index, columns = TF_IDF_Vectorizer.get_feature_names())"
      ],
      "metadata": {
        "id": "laKfx_8Fx4Nb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Añadir atributos al nuevo dataframe de TFIDF"
      ],
      "metadata": {
        "id": "aP8B8t9KZaie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words['largoDocumento'] = X_train['largoDocumento']\n",
        "bag_of_words['CantidadPalabras'] = X_train['CantidadPalabras']"
      ],
      "metadata": {
        "id": "BmRkiQpXZZ5w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red Neuronal"
      ],
      "metadata": {
        "id": "ilGGPtUT0xFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import LeakyReLU"
      ],
      "metadata": {
        "id": "e8pMorDV1DI9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam_optimizer='Adam'\n",
        "loss='binary_crossentropy'\n",
        "metric='accuracy'\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_shape=(bag_of_words.shape[1],)))\n",
        "model.add(Activation(LeakyReLU(alpha=0.3)))\n",
        "model.add(Dense(30, use_bias=True))\n",
        "model.add(Activation(LeakyReLU(alpha=0.3)))\n",
        "model.add(Dense(1, use_bias=True))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.compile(loss=loss, optimizer=adam_optimizer, metrics=[metric])"
      ],
      "metadata": {
        "id": "lW8SiL-t01M0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_array = np.asarray(bag_of_words)\n",
        "Y_train_array = np.asarray(Y_train)\n",
        "\n",
        "training = model.fit(X_train_array, Y_train_array, epochs=200, batch_size=100, validation_split=0.2)"
      ],
      "metadata": {
        "id": "gY0iDNOLDG88",
        "outputId": "a100cf66-d566-4e3b-cef0-03ae7a0cfd03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.7032 - accuracy: 0.5294 - val_loss: 0.6426 - val_accuracy: 0.5788\n",
            "Epoch 2/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.6375 - accuracy: 0.6803 - val_loss: 0.6120 - val_accuracy: 0.6778\n",
            "Epoch 3/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.5966 - accuracy: 0.7822 - val_loss: 0.5714 - val_accuracy: 0.8179\n",
            "Epoch 4/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.5485 - accuracy: 0.7969 - val_loss: 0.5220 - val_accuracy: 0.7715\n",
            "Epoch 5/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.4440 - accuracy: 0.8692 - val_loss: 0.3930 - val_accuracy: 0.8511\n",
            "Epoch 6/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.3303 - accuracy: 0.8994 - val_loss: 0.3115 - val_accuracy: 0.8905\n",
            "Epoch 7/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.2691 - accuracy: 0.9139 - val_loss: 0.3058 - val_accuracy: 0.8704\n",
            "Epoch 8/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.2040 - accuracy: 0.9415 - val_loss: 0.2654 - val_accuracy: 0.8888\n",
            "Epoch 9/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.1670 - accuracy: 0.9531 - val_loss: 0.2310 - val_accuracy: 0.9081\n",
            "Epoch 10/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.1720 - accuracy: 0.9470 - val_loss: 0.2243 - val_accuracy: 0.9124\n",
            "Epoch 11/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.1542 - accuracy: 0.9494 - val_loss: 0.2118 - val_accuracy: 0.9063\n",
            "Epoch 12/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.1115 - accuracy: 0.9730 - val_loss: 0.2072 - val_accuracy: 0.9098\n",
            "Epoch 13/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0918 - accuracy: 0.9779 - val_loss: 0.2079 - val_accuracy: 0.9002\n",
            "Epoch 14/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0839 - accuracy: 0.9809 - val_loss: 0.1982 - val_accuracy: 0.9142\n",
            "Epoch 15/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0837 - accuracy: 0.9785 - val_loss: 0.9625 - val_accuracy: 0.6637\n",
            "Epoch 16/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.2203 - accuracy: 0.9211 - val_loss: 0.1994 - val_accuracy: 0.9142\n",
            "Epoch 17/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0622 - accuracy: 0.9858 - val_loss: 0.2006 - val_accuracy: 0.9177\n",
            "Epoch 18/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0568 - accuracy: 0.9877 - val_loss: 0.2065 - val_accuracy: 0.9046\n",
            "Epoch 19/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0521 - accuracy: 0.9897 - val_loss: 0.2027 - val_accuracy: 0.9081\n",
            "Epoch 20/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0480 - accuracy: 0.9919 - val_loss: 0.2027 - val_accuracy: 0.9168\n",
            "Epoch 21/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0430 - accuracy: 0.9915 - val_loss: 0.2031 - val_accuracy: 0.9177\n",
            "Epoch 22/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0450 - accuracy: 0.9915 - val_loss: 0.8431 - val_accuracy: 0.7198\n",
            "Epoch 23/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0758 - accuracy: 0.9748 - val_loss: 0.2043 - val_accuracy: 0.9124\n",
            "Epoch 24/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0333 - accuracy: 0.9947 - val_loss: 0.2066 - val_accuracy: 0.9107\n",
            "Epoch 25/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0307 - accuracy: 0.9954 - val_loss: 0.2075 - val_accuracy: 0.9107\n",
            "Epoch 26/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0283 - accuracy: 0.9958 - val_loss: 0.2092 - val_accuracy: 0.9116\n",
            "Epoch 27/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0264 - accuracy: 0.9952 - val_loss: 0.2111 - val_accuracy: 0.9116\n",
            "Epoch 28/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0248 - accuracy: 0.9965 - val_loss: 0.2134 - val_accuracy: 0.9116\n",
            "Epoch 29/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0218 - accuracy: 0.9976 - val_loss: 0.2207 - val_accuracy: 0.9168\n",
            "Epoch 30/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0200 - accuracy: 0.9976 - val_loss: 0.2189 - val_accuracy: 0.9186\n",
            "Epoch 31/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0185 - accuracy: 0.9976 - val_loss: 0.2223 - val_accuracy: 0.9194\n",
            "Epoch 32/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0234 - accuracy: 0.9956 - val_loss: 1.4133 - val_accuracy: 0.6515\n",
            "Epoch 33/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.1423 - accuracy: 0.9479 - val_loss: 0.2235 - val_accuracy: 0.9133\n",
            "Epoch 34/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0182 - accuracy: 0.9974 - val_loss: 0.2258 - val_accuracy: 0.9151\n",
            "Epoch 35/200\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0148 - accuracy: 0.9982 - val_loss: 0.2292 - val_accuracy: 0.9168\n",
            "Epoch 36/200\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0138 - accuracy: 0.9985 - val_loss: 0.2318 - val_accuracy: 0.9186\n",
            "Epoch 37/200\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0128 - accuracy: 0.9985 - val_loss: 0.2323 - val_accuracy: 0.9133\n",
            "Epoch 38/200\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0119 - accuracy: 0.9987 - val_loss: 0.2348 - val_accuracy: 0.9142\n",
            "Epoch 39/200\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0110 - accuracy: 0.9993 - val_loss: 0.2377 - val_accuracy: 0.9168\n",
            "Epoch 40/200\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.2387 - val_accuracy: 0.9151\n",
            "Epoch 41/200\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9177\n",
            "Epoch 42/200\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.2457 - val_accuracy: 0.9177\n",
            "Epoch 43/200\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.2459 - val_accuracy: 0.9159\n",
            "Epoch 44/200\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2481 - val_accuracy: 0.9133\n",
            "Epoch 45/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2503 - val_accuracy: 0.9133\n",
            "Epoch 46/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.2548 - val_accuracy: 0.9177\n",
            "Epoch 47/200\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.2567 - val_accuracy: 0.9168\n",
            "Epoch 48/200\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.2575 - val_accuracy: 0.9124\n",
            "Epoch 49/200\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.2609 - val_accuracy: 0.9151\n",
            "Epoch 50/200\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.2627 - val_accuracy: 0.9159\n",
            "Epoch 51/200\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.2658 - val_accuracy: 0.9151\n",
            "Epoch 52/200\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.2675 - val_accuracy: 0.9159\n",
            "Epoch 53/200\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.2701 - val_accuracy: 0.9159\n",
            "Epoch 54/200\n",
            "46/46 [==============================] - 1s 23ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.2720 - val_accuracy: 0.9133\n",
            "Epoch 55/200\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9124\n",
            "Epoch 56/200\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.2764 - val_accuracy: 0.9133\n",
            "Epoch 57/200\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2786 - val_accuracy: 0.9133\n",
            "Epoch 58/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.2816 - val_accuracy: 0.9159\n",
            "Epoch 59/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2839 - val_accuracy: 0.9151\n",
            "Epoch 60/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2862 - val_accuracy: 0.9142\n",
            "Epoch 61/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.2903 - val_accuracy: 0.9151\n",
            "Epoch 62/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.2903 - val_accuracy: 0.9133\n",
            "Epoch 63/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2948 - val_accuracy: 0.9159\n",
            "Epoch 64/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2958 - val_accuracy: 0.9159\n",
            "Epoch 65/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2981 - val_accuracy: 0.9159\n",
            "Epoch 66/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3005 - val_accuracy: 0.9168\n",
            "Epoch 67/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3017 - val_accuracy: 0.9142\n",
            "Epoch 68/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3039 - val_accuracy: 0.9151\n",
            "Epoch 69/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9177\n",
            "Epoch 70/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3104 - val_accuracy: 0.9168\n",
            "Epoch 71/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3117 - val_accuracy: 0.9168\n",
            "Epoch 72/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3141 - val_accuracy: 0.9177\n",
            "Epoch 73/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3154 - val_accuracy: 0.9159\n",
            "Epoch 74/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3179 - val_accuracy: 0.9168\n",
            "Epoch 75/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3192 - val_accuracy: 0.9133\n",
            "Epoch 76/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3259 - val_accuracy: 0.9116\n",
            "Epoch 77/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3240 - val_accuracy: 0.9151\n",
            "Epoch 78/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3256 - val_accuracy: 0.9133\n",
            "Epoch 79/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3286 - val_accuracy: 0.9159\n",
            "Epoch 80/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3297 - val_accuracy: 0.9133\n",
            "Epoch 81/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3339 - val_accuracy: 0.9168\n",
            "Epoch 82/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 9.7748e-04 - accuracy: 1.0000 - val_loss: 0.3349 - val_accuracy: 0.9159\n",
            "Epoch 83/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 9.4667e-04 - accuracy: 1.0000 - val_loss: 0.3359 - val_accuracy: 0.9151\n",
            "Epoch 84/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 9.1190e-04 - accuracy: 1.0000 - val_loss: 0.3387 - val_accuracy: 0.9133\n",
            "Epoch 85/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 8.5076e-04 - accuracy: 1.0000 - val_loss: 0.3406 - val_accuracy: 0.9133\n",
            "Epoch 86/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 8.1901e-04 - accuracy: 1.0000 - val_loss: 0.3427 - val_accuracy: 0.9133\n",
            "Epoch 87/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 7.8285e-04 - accuracy: 1.0000 - val_loss: 0.3446 - val_accuracy: 0.9133\n",
            "Epoch 88/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 7.4701e-04 - accuracy: 1.0000 - val_loss: 0.3469 - val_accuracy: 0.9133\n",
            "Epoch 89/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 7.1850e-04 - accuracy: 1.0000 - val_loss: 0.3508 - val_accuracy: 0.9133\n",
            "Epoch 90/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 6.8939e-04 - accuracy: 1.0000 - val_loss: 0.3516 - val_accuracy: 0.9142\n",
            "Epoch 91/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 6.6069e-04 - accuracy: 1.0000 - val_loss: 0.3524 - val_accuracy: 0.9133\n",
            "Epoch 92/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 6.3778e-04 - accuracy: 1.0000 - val_loss: 0.3544 - val_accuracy: 0.9142\n",
            "Epoch 93/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 6.1094e-04 - accuracy: 1.0000 - val_loss: 0.3570 - val_accuracy: 0.9116\n",
            "Epoch 94/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.7955e-04 - accuracy: 1.0000 - val_loss: 0.3582 - val_accuracy: 0.9142\n",
            "Epoch 95/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.5744e-04 - accuracy: 1.0000 - val_loss: 0.3598 - val_accuracy: 0.9168\n",
            "Epoch 96/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.3676e-04 - accuracy: 1.0000 - val_loss: 0.3631 - val_accuracy: 0.9124\n",
            "Epoch 97/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.1901e-04 - accuracy: 1.0000 - val_loss: 0.3640 - val_accuracy: 0.9142\n",
            "Epoch 98/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 5.1096e-04 - accuracy: 1.0000 - val_loss: 0.3658 - val_accuracy: 0.9168\n",
            "Epoch 99/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.7755e-04 - accuracy: 1.0000 - val_loss: 0.3680 - val_accuracy: 0.9133\n",
            "Epoch 100/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 4.5709e-04 - accuracy: 1.0000 - val_loss: 0.3701 - val_accuracy: 0.9133\n",
            "Epoch 101/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 4.3586e-04 - accuracy: 1.0000 - val_loss: 0.3728 - val_accuracy: 0.9124\n",
            "Epoch 102/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.3224 - accuracy: 0.9597 - val_loss: 0.8087 - val_accuracy: 0.8205\n",
            "Epoch 103/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.1437 - accuracy: 0.9592 - val_loss: 0.2691 - val_accuracy: 0.9107\n",
            "Epoch 104/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0621 - accuracy: 0.9812 - val_loss: 0.4532 - val_accuracy: 0.8687\n",
            "Epoch 105/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9993 - val_loss: 0.3139 - val_accuracy: 0.9107\n",
            "Epoch 106/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3184 - val_accuracy: 0.9107\n",
            "Epoch 107/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3217 - val_accuracy: 0.9116\n",
            "Epoch 108/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3251 - val_accuracy: 0.9116\n",
            "Epoch 109/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3280 - val_accuracy: 0.9116\n",
            "Epoch 110/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3310 - val_accuracy: 0.9107\n",
            "Epoch 111/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3338 - val_accuracy: 0.9107\n",
            "Epoch 112/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3367 - val_accuracy: 0.9124\n",
            "Epoch 113/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3394 - val_accuracy: 0.9116\n",
            "Epoch 114/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3422 - val_accuracy: 0.9116\n",
            "Epoch 115/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 9.7698e-04 - accuracy: 1.0000 - val_loss: 0.3456 - val_accuracy: 0.9116\n",
            "Epoch 116/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 9.1249e-04 - accuracy: 1.0000 - val_loss: 0.3485 - val_accuracy: 0.9107\n",
            "Epoch 117/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 8.5845e-04 - accuracy: 1.0000 - val_loss: 0.3510 - val_accuracy: 0.9116\n",
            "Epoch 118/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 8.0494e-04 - accuracy: 1.0000 - val_loss: 0.3543 - val_accuracy: 0.9124\n",
            "Epoch 119/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 7.5649e-04 - accuracy: 1.0000 - val_loss: 0.3570 - val_accuracy: 0.9116\n",
            "Epoch 120/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 7.1346e-04 - accuracy: 1.0000 - val_loss: 0.3597 - val_accuracy: 0.9116\n",
            "Epoch 121/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 6.7383e-04 - accuracy: 1.0000 - val_loss: 0.3624 - val_accuracy: 0.9116\n",
            "Epoch 122/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 6.3951e-04 - accuracy: 1.0000 - val_loss: 0.3655 - val_accuracy: 0.9116\n",
            "Epoch 123/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 6.0285e-04 - accuracy: 1.0000 - val_loss: 0.3678 - val_accuracy: 0.9124\n",
            "Epoch 124/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.7059e-04 - accuracy: 1.0000 - val_loss: 0.3701 - val_accuracy: 0.9124\n",
            "Epoch 125/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.4182e-04 - accuracy: 1.0000 - val_loss: 0.3728 - val_accuracy: 0.9124\n",
            "Epoch 126/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 5.1178e-04 - accuracy: 1.0000 - val_loss: 0.3747 - val_accuracy: 0.9116\n",
            "Epoch 127/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.8794e-04 - accuracy: 1.0000 - val_loss: 0.3770 - val_accuracy: 0.9124\n",
            "Epoch 128/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.6484e-04 - accuracy: 1.0000 - val_loss: 0.3794 - val_accuracy: 0.9124\n",
            "Epoch 129/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 4.4056e-04 - accuracy: 1.0000 - val_loss: 0.3813 - val_accuracy: 0.9124\n",
            "Epoch 130/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.1961e-04 - accuracy: 1.0000 - val_loss: 0.3835 - val_accuracy: 0.9124\n",
            "Epoch 131/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.0086e-04 - accuracy: 1.0000 - val_loss: 0.3857 - val_accuracy: 0.9124\n",
            "Epoch 132/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.8260e-04 - accuracy: 1.0000 - val_loss: 0.3878 - val_accuracy: 0.9124\n",
            "Epoch 133/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 3.6604e-04 - accuracy: 1.0000 - val_loss: 0.3902 - val_accuracy: 0.9124\n",
            "Epoch 134/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.4985e-04 - accuracy: 1.0000 - val_loss: 0.3924 - val_accuracy: 0.9124\n",
            "Epoch 135/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.3309e-04 - accuracy: 1.0000 - val_loss: 0.3947 - val_accuracy: 0.9124\n",
            "Epoch 136/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 3.1782e-04 - accuracy: 1.0000 - val_loss: 0.3968 - val_accuracy: 0.9124\n",
            "Epoch 137/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 3.0276e-04 - accuracy: 1.0000 - val_loss: 0.3995 - val_accuracy: 0.9124\n",
            "Epoch 138/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 2.8773e-04 - accuracy: 1.0000 - val_loss: 0.4019 - val_accuracy: 0.9124\n",
            "Epoch 139/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 2.7298e-04 - accuracy: 1.0000 - val_loss: 0.4049 - val_accuracy: 0.9124\n",
            "Epoch 140/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 2.5918e-04 - accuracy: 1.0000 - val_loss: 0.4081 - val_accuracy: 0.9124\n",
            "Epoch 141/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 2.4578e-04 - accuracy: 1.0000 - val_loss: 0.4109 - val_accuracy: 0.9133\n",
            "Epoch 142/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 2.3476e-04 - accuracy: 1.0000 - val_loss: 0.4135 - val_accuracy: 0.9133\n",
            "Epoch 143/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 2.2455e-04 - accuracy: 1.0000 - val_loss: 0.4160 - val_accuracy: 0.9142\n",
            "Epoch 144/200\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 2.1550e-04 - accuracy: 1.0000 - val_loss: 0.4182 - val_accuracy: 0.9142\n",
            "Epoch 145/200\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 2.0747e-04 - accuracy: 1.0000 - val_loss: 0.4205 - val_accuracy: 0.9142\n",
            "Epoch 146/200\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 2.0015e-04 - accuracy: 1.0000 - val_loss: 0.4224 - val_accuracy: 0.9142\n",
            "Epoch 147/200\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 1.9298e-04 - accuracy: 1.0000 - val_loss: 0.4242 - val_accuracy: 0.9142\n",
            "Epoch 148/200\n",
            "46/46 [==============================] - 1s 11ms/step - loss: 1.8694e-04 - accuracy: 1.0000 - val_loss: 0.4260 - val_accuracy: 0.9142\n",
            "Epoch 149/200\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 1.8104e-04 - accuracy: 1.0000 - val_loss: 0.4277 - val_accuracy: 0.9142\n",
            "Epoch 150/200\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 1.7572e-04 - accuracy: 1.0000 - val_loss: 0.4293 - val_accuracy: 0.9142\n",
            "Epoch 151/200\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 1.7077e-04 - accuracy: 1.0000 - val_loss: 0.4305 - val_accuracy: 0.9133\n",
            "Epoch 152/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.6572e-04 - accuracy: 1.0000 - val_loss: 0.4323 - val_accuracy: 0.9142\n",
            "Epoch 153/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 1.6124e-04 - accuracy: 1.0000 - val_loss: 0.4335 - val_accuracy: 0.9142\n",
            "Epoch 154/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 1.5662e-04 - accuracy: 1.0000 - val_loss: 0.4350 - val_accuracy: 0.9142\n",
            "Epoch 155/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.5229e-04 - accuracy: 1.0000 - val_loss: 0.4363 - val_accuracy: 0.9142\n",
            "Epoch 156/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.4833e-04 - accuracy: 1.0000 - val_loss: 0.4377 - val_accuracy: 0.9142\n",
            "Epoch 157/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.4426e-04 - accuracy: 1.0000 - val_loss: 0.4392 - val_accuracy: 0.9142\n",
            "Epoch 158/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.4030e-04 - accuracy: 1.0000 - val_loss: 0.4404 - val_accuracy: 0.9142\n",
            "Epoch 159/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.3656e-04 - accuracy: 1.0000 - val_loss: 0.4418 - val_accuracy: 0.9133\n",
            "Epoch 160/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 1.3301e-04 - accuracy: 1.0000 - val_loss: 0.4430 - val_accuracy: 0.9142\n",
            "Epoch 161/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 1.2973e-04 - accuracy: 1.0000 - val_loss: 0.4446 - val_accuracy: 0.9133\n",
            "Epoch 162/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 1.2623e-04 - accuracy: 1.0000 - val_loss: 0.4456 - val_accuracy: 0.9133\n",
            "Epoch 163/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 1.2301e-04 - accuracy: 1.0000 - val_loss: 0.4469 - val_accuracy: 0.9133\n",
            "Epoch 164/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 1.1987e-04 - accuracy: 1.0000 - val_loss: 0.4483 - val_accuracy: 0.9133\n",
            "Epoch 165/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.1688e-04 - accuracy: 1.0000 - val_loss: 0.4494 - val_accuracy: 0.9133\n",
            "Epoch 166/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.1382e-04 - accuracy: 1.0000 - val_loss: 0.4507 - val_accuracy: 0.9124\n",
            "Epoch 167/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 1.1108e-04 - accuracy: 1.0000 - val_loss: 0.4520 - val_accuracy: 0.9124\n",
            "Epoch 168/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 1.0800e-04 - accuracy: 1.0000 - val_loss: 0.4531 - val_accuracy: 0.9133\n",
            "Epoch 169/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 1.0515e-04 - accuracy: 1.0000 - val_loss: 0.4542 - val_accuracy: 0.9133\n",
            "Epoch 170/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 1.0267e-04 - accuracy: 1.0000 - val_loss: 0.4560 - val_accuracy: 0.9133\n",
            "Epoch 171/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 9.9955e-05 - accuracy: 1.0000 - val_loss: 0.4567 - val_accuracy: 0.9133\n",
            "Epoch 172/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 9.7178e-05 - accuracy: 1.0000 - val_loss: 0.4582 - val_accuracy: 0.9133\n",
            "Epoch 173/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 9.4763e-05 - accuracy: 1.0000 - val_loss: 0.4596 - val_accuracy: 0.9133\n",
            "Epoch 174/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 9.2442e-05 - accuracy: 1.0000 - val_loss: 0.4608 - val_accuracy: 0.9133\n",
            "Epoch 175/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 9.0034e-05 - accuracy: 1.0000 - val_loss: 0.4621 - val_accuracy: 0.9133\n",
            "Epoch 176/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 8.7825e-05 - accuracy: 1.0000 - val_loss: 0.4634 - val_accuracy: 0.9133\n",
            "Epoch 177/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 8.5708e-05 - accuracy: 1.0000 - val_loss: 0.4647 - val_accuracy: 0.9133\n",
            "Epoch 178/200\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 8.3583e-05 - accuracy: 1.0000 - val_loss: 0.4663 - val_accuracy: 0.9124\n",
            "Epoch 179/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 8.1496e-05 - accuracy: 1.0000 - val_loss: 0.4678 - val_accuracy: 0.9124\n",
            "Epoch 180/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 7.9393e-05 - accuracy: 1.0000 - val_loss: 0.4690 - val_accuracy: 0.9124\n",
            "Epoch 181/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 7.7534e-05 - accuracy: 1.0000 - val_loss: 0.4705 - val_accuracy: 0.9124\n",
            "Epoch 182/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 7.5456e-05 - accuracy: 1.0000 - val_loss: 0.4717 - val_accuracy: 0.9124\n",
            "Epoch 183/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 7.3631e-05 - accuracy: 1.0000 - val_loss: 0.4730 - val_accuracy: 0.9124\n",
            "Epoch 184/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 7.1900e-05 - accuracy: 1.0000 - val_loss: 0.4741 - val_accuracy: 0.9124\n",
            "Epoch 185/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 7.0254e-05 - accuracy: 1.0000 - val_loss: 0.4753 - val_accuracy: 0.9116\n",
            "Epoch 186/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 6.8358e-05 - accuracy: 1.0000 - val_loss: 0.4771 - val_accuracy: 0.9124\n",
            "Epoch 187/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 6.6754e-05 - accuracy: 1.0000 - val_loss: 0.4781 - val_accuracy: 0.9124\n",
            "Epoch 188/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 6.5105e-05 - accuracy: 1.0000 - val_loss: 0.4793 - val_accuracy: 0.9124\n",
            "Epoch 189/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 6.3570e-05 - accuracy: 1.0000 - val_loss: 0.4803 - val_accuracy: 0.9116\n",
            "Epoch 190/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 6.2201e-05 - accuracy: 1.0000 - val_loss: 0.4816 - val_accuracy: 0.9124\n",
            "Epoch 191/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 6.0661e-05 - accuracy: 1.0000 - val_loss: 0.4831 - val_accuracy: 0.9124\n",
            "Epoch 192/200\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.9237e-05 - accuracy: 1.0000 - val_loss: 0.4839 - val_accuracy: 0.9124\n",
            "Epoch 193/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 5.7965e-05 - accuracy: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.9116\n",
            "Epoch 194/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 5.6715e-05 - accuracy: 1.0000 - val_loss: 0.4862 - val_accuracy: 0.9124\n",
            "Epoch 195/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 5.5210e-05 - accuracy: 1.0000 - val_loss: 0.4880 - val_accuracy: 0.9124\n",
            "Epoch 196/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 5.4118e-05 - accuracy: 1.0000 - val_loss: 0.4885 - val_accuracy: 0.9124\n",
            "Epoch 197/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 5.2834e-05 - accuracy: 1.0000 - val_loss: 0.4903 - val_accuracy: 0.9124\n",
            "Epoch 198/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 5.1665e-05 - accuracy: 1.0000 - val_loss: 0.4912 - val_accuracy: 0.9124\n",
            "Epoch 199/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 5.0406e-05 - accuracy: 1.0000 - val_loss: 0.4922 - val_accuracy: 0.9124\n",
            "Epoch 200/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.9320e-05 - accuracy: 1.0000 - val_loss: 0.4940 - val_accuracy: 0.9124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras\n",
        "\n",
        "model.save(\"basic_model\")\n",
        "saved_model = keras.models.load_model(\"basic_model\")"
      ],
      "metadata": {
        "id": "rCRKAKT9oRpN",
        "outputId": "7b4f40e7-0eea-46ee-f0cd-3d03707c83af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as leaky_re_lu_layer_call_fn, leaky_re_lu_layer_call_and_return_conditional_losses, leaky_re_lu_1_layer_call_fn, leaky_re_lu_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_Validation_Bag = TF_IDF_Vectorizer.transform(X_validation[\"tweet\"])\n",
        "X_Validation_Bag_df = pd.DataFrame(X_Validation_Bag.toarray(), index = X_validation.index,columns = TF_IDF_Vectorizer.get_feature_names())"
      ],
      "metadata": {
        "id": "40thu1nco6Nu",
        "outputId": "c70208af-bde4-4f70-a7e3-bea47dbb1803",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_Validation_Bag_df"
      ],
      "metadata": {
        "id": "YBZ8D7WDhRyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Validation_Bag_df['largoDocumento'] = X_validation['largoDocumento']\n",
        "X_Validation_Bag_df['CantidadPalabras'] = X_validation['CantidadPalabras']"
      ],
      "metadata": {
        "id": "J91gPkLchKKR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Validation_Bag_df_array = np.asarray(X_Validation_Bag_df)\n",
        "Y_Validation_Predict = saved_model.predict(X_Validation_Bag_df_array)"
      ],
      "metadata": {
        "id": "0Fy5rxxbnw3U",
        "outputId": "e2f1fa46-fcca-447e-9662-d6949ec61f61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90/90 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_Validation_Predict"
      ],
      "metadata": {
        "id": "YpPOc0IL5Wyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformPrediction(df, predNumber, predLabel):\n",
        "  copy = df.copy()\n",
        "  for i in range(len(copy)):\n",
        "    if copy[i][0] > 0.5:\n",
        "      predNumber.append([1])\n",
        "      predLabel.append([\"real\"])\n",
        "    else:\n",
        "      predNumber.append([0])\n",
        "      predLabel.append([\"fake\"])\n"
      ],
      "metadata": {
        "id": "EWu5_-xv-smt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predValidationNumbers = []\n",
        "predValidationLabels = []\n",
        "transformPrediction(Y_Validation_Predict, predValidationNumbers, predValidationLabels)"
      ],
      "metadata": {
        "id": "i1hDHEULlcDX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validationNumbers = []\n",
        "validationLabels = []\n",
        "copy = np.asarray(Y_validation).copy()\n",
        "for i in range(len(copy)):\n",
        "  if copy[i] > 0.5:\n",
        "    validationNumbers.append([1])\n",
        "    validationLabels.append([\"real\"])\n",
        "  else:\n",
        "    validationNumbers.append([0])\n",
        "    validationLabels.append([\"fake\"])"
      ],
      "metadata": {
        "id": "ODiTBndqlzWX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.asarray(Y_validation)"
      ],
      "metadata": {
        "id": "hnmR9lfkk3J1",
        "outputId": "2181f223-2a8b-4ddc-adb1-cd0ed4af3494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(predValidationLabels,validationLabels))"
      ],
      "metadata": {
        "id": "AhJZ1LxVaC0D",
        "outputId": "ede0c277-9668-4fa4-a37a-05437a9427d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.920812894183602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K fold"
      ],
      "metadata": {
        "id": "lsp7hcI0Xlx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "WBbrhQ1qayX_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "k = 5\n",
        "\n",
        "def k_folds(X, y, k):\n",
        "  assert(len(X) == len(y))\n",
        "  folds_X = []\n",
        "  folds_y = []\n",
        "  initial_pos = 0\n",
        "\n",
        "  for i in range(k):\n",
        "    to_pos = min(math.ceil(len(X)/k)*(i+1), len(X))\n",
        "    \n",
        "    x_fold = X[initial_pos:to_pos]\n",
        "    y_fold = y[initial_pos:to_pos]\n",
        "\n",
        "    folds_X.append(x_fold)\n",
        "    folds_y.append(y_fold)\n",
        "    initial_pos = to_pos\n",
        "\n",
        "  return folds_X, folds_y"
      ],
      "metadata": {
        "id": "Mf9ISG6OXlV_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, X_to_eval, Y_to_eval):\n",
        "    Y_Prediction = model.predict(np.asarray(X_to_eval))\n",
        "    numbers = []\n",
        "    prediction_labels = []\n",
        "    transformPrediction(Y_Prediction, numbers, prediction_labels)\n",
        "\n",
        "    validationLabels = []\n",
        "    copy = np.asarray(Y_to_eval).copy()\n",
        "    for i in range(len(copy)):\n",
        "      if copy[i] > 0.5:\n",
        "        validationLabels.append([\"real\"])\n",
        "      else:\n",
        "        validationLabels.append([\"fake\"])\n",
        "\n",
        "    return accuracy_score(validationLabels, prediction_labels)"
      ],
      "metadata": {
        "id": "DAADQdFEYKtZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold_cross_val(folds_X, folds_y, model):\n",
        "  assert(len(folds_X) == len(folds_y))\n",
        "  X_to_train = []\n",
        "  y_to_train = []\n",
        "  X_to_eval = 0\n",
        "  y_to_eval = 0\n",
        "  scores = []\n",
        "  for i in range(len(folds_X)):\n",
        "    X_to_train = []\n",
        "    y_to_train = []\n",
        "    for j in range(len(folds_X)):\n",
        "      if  i == j:\n",
        "        X_to_eval = folds_X[i]\n",
        "        y_to_eval = folds_y[i]\n",
        "      else:\n",
        "        X_to_train.append(folds_X[j])\n",
        "        y_to_train.append(folds_y[j])\n",
        "    \n",
        "    X_train = np.concatenate(X_to_train)\n",
        "    y_train = np.concatenate(y_to_train)\n",
        "    model.fit(X_train,y_train, epochs=200, batch_size=100, validation_split=0.2)\n",
        "    score = evaluate(model, X_to_eval, y_to_eval)\n",
        "    scores.append(score)\n",
        "  return np.mean(np.array(scores)), scores"
      ],
      "metadata": {
        "id": "iWHnyEvRY_5_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folds_X, folds_y = k_folds(np.asarray(bag_of_words), np.asarray(Y_train), 5)\n"
      ],
      "metadata": {
        "id": "iYJyO491ZBgK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold_result_mean, k_fold_result_arr = k_fold_cross_val(folds_X, folds_y, model)"
      ],
      "metadata": {
        "id": "1lH1Uz4HbMln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36765172-8ae9-4241-af73-071a13978b64"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "37/37 [==============================] - 1s 18ms/step - loss: 1.1106e-08 - accuracy: 1.0000 - val_loss: 1.0137 - val_accuracy: 0.9157\n",
            "Epoch 2/200\n",
            "37/37 [==============================] - 1s 15ms/step - loss: 7.4413e-09 - accuracy: 1.0000 - val_loss: 1.0199 - val_accuracy: 0.9168\n",
            "Epoch 3/200\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 0.7171 - accuracy: 0.9485 - val_loss: 1.0587 - val_accuracy: 0.9102\n",
            "Epoch 4/200\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 1.0103 - val_accuracy: 0.9058\n",
            "Epoch 5/200\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 3.1217e-06 - accuracy: 1.0000 - val_loss: 1.0379 - val_accuracy: 0.9047\n",
            "Epoch 6/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.4258e-06 - accuracy: 1.0000 - val_loss: 1.0365 - val_accuracy: 0.9047\n",
            "Epoch 7/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.0421e-06 - accuracy: 1.0000 - val_loss: 1.0350 - val_accuracy: 0.9036\n",
            "Epoch 8/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.8277e-06 - accuracy: 1.0000 - val_loss: 1.0338 - val_accuracy: 0.9047\n",
            "Epoch 9/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1.6823e-06 - accuracy: 1.0000 - val_loss: 1.0327 - val_accuracy: 0.9047\n",
            "Epoch 10/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1.5672e-06 - accuracy: 1.0000 - val_loss: 1.0315 - val_accuracy: 0.9047\n",
            "Epoch 11/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1.4673e-06 - accuracy: 1.0000 - val_loss: 1.0304 - val_accuracy: 0.9047\n",
            "Epoch 12/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.3918e-06 - accuracy: 1.0000 - val_loss: 1.0294 - val_accuracy: 0.9047\n",
            "Epoch 13/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.3168e-06 - accuracy: 1.0000 - val_loss: 1.0284 - val_accuracy: 0.9036\n",
            "Epoch 14/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.2588e-06 - accuracy: 1.0000 - val_loss: 1.0275 - val_accuracy: 0.9036\n",
            "Epoch 15/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.2015e-06 - accuracy: 1.0000 - val_loss: 1.0268 - val_accuracy: 0.9036\n",
            "Epoch 16/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1.1553e-06 - accuracy: 1.0000 - val_loss: 1.0260 - val_accuracy: 0.9036\n",
            "Epoch 17/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.1086e-06 - accuracy: 1.0000 - val_loss: 1.0252 - val_accuracy: 0.9036\n",
            "Epoch 18/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.0676e-06 - accuracy: 1.0000 - val_loss: 1.0244 - val_accuracy: 0.9036\n",
            "Epoch 19/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1.0284e-06 - accuracy: 1.0000 - val_loss: 1.0236 - val_accuracy: 0.9025\n",
            "Epoch 20/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 9.9280e-07 - accuracy: 1.0000 - val_loss: 1.0229 - val_accuracy: 0.9025\n",
            "Epoch 21/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.6191e-07 - accuracy: 1.0000 - val_loss: 1.0221 - val_accuracy: 0.9036\n",
            "Epoch 22/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 9.2816e-07 - accuracy: 1.0000 - val_loss: 1.0215 - val_accuracy: 0.9047\n",
            "Epoch 23/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 8.9853e-07 - accuracy: 1.0000 - val_loss: 1.0208 - val_accuracy: 0.9047\n",
            "Epoch 24/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 8.7058e-07 - accuracy: 1.0000 - val_loss: 1.0202 - val_accuracy: 0.9047\n",
            "Epoch 25/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.4706e-07 - accuracy: 1.0000 - val_loss: 1.0195 - val_accuracy: 0.9047\n",
            "Epoch 26/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 8.1895e-07 - accuracy: 1.0000 - val_loss: 1.0190 - val_accuracy: 0.9047\n",
            "Epoch 27/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 7.9625e-07 - accuracy: 1.0000 - val_loss: 1.0184 - val_accuracy: 0.9047\n",
            "Epoch 28/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 7.7414e-07 - accuracy: 1.0000 - val_loss: 1.0178 - val_accuracy: 0.9047\n",
            "Epoch 29/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 7.5201e-07 - accuracy: 1.0000 - val_loss: 1.0173 - val_accuracy: 0.9047\n",
            "Epoch 30/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 7.3268e-07 - accuracy: 1.0000 - val_loss: 1.0166 - val_accuracy: 0.9047\n",
            "Epoch 31/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 7.1257e-07 - accuracy: 1.0000 - val_loss: 1.0162 - val_accuracy: 0.9047\n",
            "Epoch 32/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 6.9559e-07 - accuracy: 1.0000 - val_loss: 1.0154 - val_accuracy: 0.9047\n",
            "Epoch 33/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 6.7542e-07 - accuracy: 1.0000 - val_loss: 1.0150 - val_accuracy: 0.9047\n",
            "Epoch 34/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 6.5871e-07 - accuracy: 1.0000 - val_loss: 1.0143 - val_accuracy: 0.9047\n",
            "Epoch 35/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 6.4217e-07 - accuracy: 1.0000 - val_loss: 1.0138 - val_accuracy: 0.9047\n",
            "Epoch 36/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 6.2600e-07 - accuracy: 1.0000 - val_loss: 1.0133 - val_accuracy: 0.9047\n",
            "Epoch 37/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 6.1084e-07 - accuracy: 1.0000 - val_loss: 1.0128 - val_accuracy: 0.9047\n",
            "Epoch 38/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 5.9559e-07 - accuracy: 1.0000 - val_loss: 1.0123 - val_accuracy: 0.9047\n",
            "Epoch 39/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 5.8237e-07 - accuracy: 1.0000 - val_loss: 1.0117 - val_accuracy: 0.9047\n",
            "Epoch 40/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 5.6719e-07 - accuracy: 1.0000 - val_loss: 1.0113 - val_accuracy: 0.9047\n",
            "Epoch 41/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 5.5437e-07 - accuracy: 1.0000 - val_loss: 1.0108 - val_accuracy: 0.9047\n",
            "Epoch 42/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.4242e-07 - accuracy: 1.0000 - val_loss: 1.0102 - val_accuracy: 0.9047\n",
            "Epoch 43/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 5.2992e-07 - accuracy: 1.0000 - val_loss: 1.0097 - val_accuracy: 0.9047\n",
            "Epoch 44/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 5.1739e-07 - accuracy: 1.0000 - val_loss: 1.0092 - val_accuracy: 0.9047\n",
            "Epoch 45/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.0622e-07 - accuracy: 1.0000 - val_loss: 1.0088 - val_accuracy: 0.9047\n",
            "Epoch 46/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 4.9403e-07 - accuracy: 1.0000 - val_loss: 1.0084 - val_accuracy: 0.9058\n",
            "Epoch 47/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.8299e-07 - accuracy: 1.0000 - val_loss: 1.0076 - val_accuracy: 0.9058\n",
            "Epoch 48/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.6885e-07 - accuracy: 1.0000 - val_loss: 1.0071 - val_accuracy: 0.9058\n",
            "Epoch 49/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 4.5932e-07 - accuracy: 1.0000 - val_loss: 1.0067 - val_accuracy: 0.9058\n",
            "Epoch 50/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 4.4911e-07 - accuracy: 1.0000 - val_loss: 1.0063 - val_accuracy: 0.9058\n",
            "Epoch 51/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 4.4060e-07 - accuracy: 1.0000 - val_loss: 1.0058 - val_accuracy: 0.9058\n",
            "Epoch 52/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.3044e-07 - accuracy: 1.0000 - val_loss: 1.0054 - val_accuracy: 0.9058\n",
            "Epoch 53/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.2192e-07 - accuracy: 1.0000 - val_loss: 1.0049 - val_accuracy: 0.9058\n",
            "Epoch 54/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.1359e-07 - accuracy: 1.0000 - val_loss: 1.0045 - val_accuracy: 0.9058\n",
            "Epoch 55/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 4.0492e-07 - accuracy: 1.0000 - val_loss: 1.0041 - val_accuracy: 0.9058\n",
            "Epoch 56/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.9677e-07 - accuracy: 1.0000 - val_loss: 1.0038 - val_accuracy: 0.9069\n",
            "Epoch 57/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.8925e-07 - accuracy: 1.0000 - val_loss: 1.0032 - val_accuracy: 0.9069\n",
            "Epoch 58/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.8075e-07 - accuracy: 1.0000 - val_loss: 1.0028 - val_accuracy: 0.9069\n",
            "Epoch 59/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.7345e-07 - accuracy: 1.0000 - val_loss: 1.0024 - val_accuracy: 0.9069\n",
            "Epoch 60/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.6592e-07 - accuracy: 1.0000 - val_loss: 1.0020 - val_accuracy: 0.9069\n",
            "Epoch 61/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.5881e-07 - accuracy: 1.0000 - val_loss: 1.0016 - val_accuracy: 0.9069\n",
            "Epoch 62/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.5175e-07 - accuracy: 1.0000 - val_loss: 1.0013 - val_accuracy: 0.9069\n",
            "Epoch 63/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 3.4383e-07 - accuracy: 1.0000 - val_loss: 1.0007 - val_accuracy: 0.9069\n",
            "Epoch 64/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 3.3647e-07 - accuracy: 1.0000 - val_loss: 1.0003 - val_accuracy: 0.9069\n",
            "Epoch 65/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 3.3030e-07 - accuracy: 1.0000 - val_loss: 1.0000 - val_accuracy: 0.9069\n",
            "Epoch 66/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 3.2378e-07 - accuracy: 1.0000 - val_loss: 0.9996 - val_accuracy: 0.9069\n",
            "Epoch 67/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.1759e-07 - accuracy: 1.0000 - val_loss: 0.9992 - val_accuracy: 0.9069\n",
            "Epoch 68/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 3.1194e-07 - accuracy: 1.0000 - val_loss: 0.9988 - val_accuracy: 0.9069\n",
            "Epoch 69/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 3.0593e-07 - accuracy: 1.0000 - val_loss: 0.9984 - val_accuracy: 0.9069\n",
            "Epoch 70/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.0009e-07 - accuracy: 1.0000 - val_loss: 0.9981 - val_accuracy: 0.9069\n",
            "Epoch 71/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.9418e-07 - accuracy: 1.0000 - val_loss: 0.9975 - val_accuracy: 0.9069\n",
            "Epoch 72/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2.8771e-07 - accuracy: 1.0000 - val_loss: 0.9972 - val_accuracy: 0.9080\n",
            "Epoch 73/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.8289e-07 - accuracy: 1.0000 - val_loss: 0.9968 - val_accuracy: 0.9080\n",
            "Epoch 74/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2.7801e-07 - accuracy: 1.0000 - val_loss: 0.9964 - val_accuracy: 0.9091\n",
            "Epoch 75/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.7243e-07 - accuracy: 1.0000 - val_loss: 0.9961 - val_accuracy: 0.9091\n",
            "Epoch 76/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2.6784e-07 - accuracy: 1.0000 - val_loss: 0.9958 - val_accuracy: 0.9091\n",
            "Epoch 77/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.6335e-07 - accuracy: 1.0000 - val_loss: 0.9954 - val_accuracy: 0.9091\n",
            "Epoch 78/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.5852e-07 - accuracy: 1.0000 - val_loss: 0.9950 - val_accuracy: 0.9091\n",
            "Epoch 79/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2.5374e-07 - accuracy: 1.0000 - val_loss: 0.9947 - val_accuracy: 0.9091\n",
            "Epoch 80/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2.4932e-07 - accuracy: 1.0000 - val_loss: 0.9944 - val_accuracy: 0.9091\n",
            "Epoch 81/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2.4512e-07 - accuracy: 1.0000 - val_loss: 0.9940 - val_accuracy: 0.9091\n",
            "Epoch 82/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2.4062e-07 - accuracy: 1.0000 - val_loss: 0.9937 - val_accuracy: 0.9091\n",
            "Epoch 83/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.3662e-07 - accuracy: 1.0000 - val_loss: 0.9933 - val_accuracy: 0.9091\n",
            "Epoch 84/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.3233e-07 - accuracy: 1.0000 - val_loss: 0.9930 - val_accuracy: 0.9102\n",
            "Epoch 85/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.2843e-07 - accuracy: 1.0000 - val_loss: 0.9927 - val_accuracy: 0.9102\n",
            "Epoch 86/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2.2443e-07 - accuracy: 1.0000 - val_loss: 0.9923 - val_accuracy: 0.9102\n",
            "Epoch 87/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.2026e-07 - accuracy: 1.0000 - val_loss: 0.9919 - val_accuracy: 0.9102\n",
            "Epoch 88/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.1609e-07 - accuracy: 1.0000 - val_loss: 0.9915 - val_accuracy: 0.9113\n",
            "Epoch 89/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2.1233e-07 - accuracy: 1.0000 - val_loss: 0.9912 - val_accuracy: 0.9113\n",
            "Epoch 90/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2.0865e-07 - accuracy: 1.0000 - val_loss: 0.9910 - val_accuracy: 0.9113\n",
            "Epoch 91/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.0580e-07 - accuracy: 1.0000 - val_loss: 0.9906 - val_accuracy: 0.9124\n",
            "Epoch 92/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2.0179e-07 - accuracy: 1.0000 - val_loss: 0.9903 - val_accuracy: 0.9124\n",
            "Epoch 93/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.9843e-07 - accuracy: 1.0000 - val_loss: 0.9900 - val_accuracy: 0.9124\n",
            "Epoch 94/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.9528e-07 - accuracy: 1.0000 - val_loss: 0.9897 - val_accuracy: 0.9124\n",
            "Epoch 95/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.9211e-07 - accuracy: 1.0000 - val_loss: 0.9893 - val_accuracy: 0.9124\n",
            "Epoch 96/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.8914e-07 - accuracy: 1.0000 - val_loss: 0.9890 - val_accuracy: 0.9124\n",
            "Epoch 97/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.8576e-07 - accuracy: 1.0000 - val_loss: 0.9887 - val_accuracy: 0.9124\n",
            "Epoch 98/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.8265e-07 - accuracy: 1.0000 - val_loss: 0.9884 - val_accuracy: 0.9124\n",
            "Epoch 99/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.7966e-07 - accuracy: 1.0000 - val_loss: 0.9882 - val_accuracy: 0.9124\n",
            "Epoch 100/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.7642e-07 - accuracy: 1.0000 - val_loss: 0.9877 - val_accuracy: 0.9124\n",
            "Epoch 101/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.7275e-07 - accuracy: 1.0000 - val_loss: 0.9874 - val_accuracy: 0.9124\n",
            "Epoch 102/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.6986e-07 - accuracy: 1.0000 - val_loss: 0.9871 - val_accuracy: 0.9124\n",
            "Epoch 103/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1.6761e-07 - accuracy: 1.0000 - val_loss: 0.9867 - val_accuracy: 0.9124\n",
            "Epoch 104/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.6441e-07 - accuracy: 1.0000 - val_loss: 0.9865 - val_accuracy: 0.9124\n",
            "Epoch 105/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.6187e-07 - accuracy: 1.0000 - val_loss: 0.9862 - val_accuracy: 0.9124\n",
            "Epoch 106/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.5933e-07 - accuracy: 1.0000 - val_loss: 0.9859 - val_accuracy: 0.9124\n",
            "Epoch 107/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1.5685e-07 - accuracy: 1.0000 - val_loss: 0.9856 - val_accuracy: 0.9124\n",
            "Epoch 108/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.5455e-07 - accuracy: 1.0000 - val_loss: 0.9853 - val_accuracy: 0.9124\n",
            "Epoch 109/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.5201e-07 - accuracy: 1.0000 - val_loss: 0.9850 - val_accuracy: 0.9124\n",
            "Epoch 110/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.4959e-07 - accuracy: 1.0000 - val_loss: 0.9848 - val_accuracy: 0.9124\n",
            "Epoch 111/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.4751e-07 - accuracy: 1.0000 - val_loss: 0.9845 - val_accuracy: 0.9124\n",
            "Epoch 112/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.4514e-07 - accuracy: 1.0000 - val_loss: 0.9842 - val_accuracy: 0.9124\n",
            "Epoch 113/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.4274e-07 - accuracy: 1.0000 - val_loss: 0.9840 - val_accuracy: 0.9124\n",
            "Epoch 114/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.4077e-07 - accuracy: 1.0000 - val_loss: 0.9836 - val_accuracy: 0.9124\n",
            "Epoch 115/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.3847e-07 - accuracy: 1.0000 - val_loss: 0.9834 - val_accuracy: 0.9124\n",
            "Epoch 116/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1.3620e-07 - accuracy: 1.0000 - val_loss: 0.9830 - val_accuracy: 0.9124\n",
            "Epoch 117/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.3386e-07 - accuracy: 1.0000 - val_loss: 0.9827 - val_accuracy: 0.9124\n",
            "Epoch 118/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.3184e-07 - accuracy: 1.0000 - val_loss: 0.9824 - val_accuracy: 0.9124\n",
            "Epoch 119/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.2992e-07 - accuracy: 1.0000 - val_loss: 0.9822 - val_accuracy: 0.9124\n",
            "Epoch 120/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.2794e-07 - accuracy: 1.0000 - val_loss: 0.9820 - val_accuracy: 0.9124\n",
            "Epoch 121/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.2606e-07 - accuracy: 1.0000 - val_loss: 0.9817 - val_accuracy: 0.9124\n",
            "Epoch 122/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.2418e-07 - accuracy: 1.0000 - val_loss: 0.9814 - val_accuracy: 0.9124\n",
            "Epoch 123/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.2242e-07 - accuracy: 1.0000 - val_loss: 0.9811 - val_accuracy: 0.9124\n",
            "Epoch 124/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.2057e-07 - accuracy: 1.0000 - val_loss: 0.9808 - val_accuracy: 0.9135\n",
            "Epoch 125/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.1876e-07 - accuracy: 1.0000 - val_loss: 0.9806 - val_accuracy: 0.9135\n",
            "Epoch 126/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1.1715e-07 - accuracy: 1.0000 - val_loss: 0.9803 - val_accuracy: 0.9135\n",
            "Epoch 127/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.1535e-07 - accuracy: 1.0000 - val_loss: 0.9800 - val_accuracy: 0.9135\n",
            "Epoch 128/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1.1360e-07 - accuracy: 1.0000 - val_loss: 0.9798 - val_accuracy: 0.9168\n",
            "Epoch 129/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.1201e-07 - accuracy: 1.0000 - val_loss: 0.9796 - val_accuracy: 0.9168\n",
            "Epoch 130/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.1033e-07 - accuracy: 1.0000 - val_loss: 0.9794 - val_accuracy: 0.9168\n",
            "Epoch 131/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1.0883e-07 - accuracy: 1.0000 - val_loss: 0.9791 - val_accuracy: 0.9168\n",
            "Epoch 132/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1.0728e-07 - accuracy: 1.0000 - val_loss: 0.9788 - val_accuracy: 0.9168\n",
            "Epoch 133/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.0576e-07 - accuracy: 1.0000 - val_loss: 0.9785 - val_accuracy: 0.9168\n",
            "Epoch 134/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.0424e-07 - accuracy: 1.0000 - val_loss: 0.9783 - val_accuracy: 0.9168\n",
            "Epoch 135/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.0272e-07 - accuracy: 1.0000 - val_loss: 0.9781 - val_accuracy: 0.9168\n",
            "Epoch 136/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1.0137e-07 - accuracy: 1.0000 - val_loss: 0.9778 - val_accuracy: 0.9168\n",
            "Epoch 137/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.9855e-08 - accuracy: 1.0000 - val_loss: 0.9775 - val_accuracy: 0.9168\n",
            "Epoch 138/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.8450e-08 - accuracy: 1.0000 - val_loss: 0.9773 - val_accuracy: 0.9168\n",
            "Epoch 139/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.7105e-08 - accuracy: 1.0000 - val_loss: 0.9770 - val_accuracy: 0.9168\n",
            "Epoch 140/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.5784e-08 - accuracy: 1.0000 - val_loss: 0.9768 - val_accuracy: 0.9168\n",
            "Epoch 141/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.4343e-08 - accuracy: 1.0000 - val_loss: 0.9766 - val_accuracy: 0.9168\n",
            "Epoch 142/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 9.3126e-08 - accuracy: 1.0000 - val_loss: 0.9763 - val_accuracy: 0.9168\n",
            "Epoch 143/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 9.1956e-08 - accuracy: 1.0000 - val_loss: 0.9761 - val_accuracy: 0.9168\n",
            "Epoch 144/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.0563e-08 - accuracy: 1.0000 - val_loss: 0.9758 - val_accuracy: 0.9168\n",
            "Epoch 145/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 8.9303e-08 - accuracy: 1.0000 - val_loss: 0.9756 - val_accuracy: 0.9168\n",
            "Epoch 146/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 8.8256e-08 - accuracy: 1.0000 - val_loss: 0.9753 - val_accuracy: 0.9168\n",
            "Epoch 147/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.6908e-08 - accuracy: 1.0000 - val_loss: 0.9751 - val_accuracy: 0.9168\n",
            "Epoch 148/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.5788e-08 - accuracy: 1.0000 - val_loss: 0.9749 - val_accuracy: 0.9168\n",
            "Epoch 149/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.4606e-08 - accuracy: 1.0000 - val_loss: 0.9747 - val_accuracy: 0.9168\n",
            "Epoch 150/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.3563e-08 - accuracy: 1.0000 - val_loss: 0.9744 - val_accuracy: 0.9168\n",
            "Epoch 151/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 8.2483e-08 - accuracy: 1.0000 - val_loss: 0.9742 - val_accuracy: 0.9168\n",
            "Epoch 152/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 8.1276e-08 - accuracy: 1.0000 - val_loss: 0.9740 - val_accuracy: 0.9168\n",
            "Epoch 153/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.0230e-08 - accuracy: 1.0000 - val_loss: 0.9738 - val_accuracy: 0.9168\n",
            "Epoch 154/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.9208e-08 - accuracy: 1.0000 - val_loss: 0.9736 - val_accuracy: 0.9168\n",
            "Epoch 155/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 7.8230e-08 - accuracy: 1.0000 - val_loss: 0.9733 - val_accuracy: 0.9179\n",
            "Epoch 156/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 7.7134e-08 - accuracy: 1.0000 - val_loss: 0.9731 - val_accuracy: 0.9179\n",
            "Epoch 157/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.6195e-08 - accuracy: 1.0000 - val_loss: 0.9728 - val_accuracy: 0.9189\n",
            "Epoch 158/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.5201e-08 - accuracy: 1.0000 - val_loss: 0.9726 - val_accuracy: 0.9179\n",
            "Epoch 159/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.4220e-08 - accuracy: 1.0000 - val_loss: 0.9724 - val_accuracy: 0.9179\n",
            "Epoch 160/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.3315e-08 - accuracy: 1.0000 - val_loss: 0.9722 - val_accuracy: 0.9179\n",
            "Epoch 161/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.2372e-08 - accuracy: 1.0000 - val_loss: 0.9719 - val_accuracy: 0.9179\n",
            "Epoch 162/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.1412e-08 - accuracy: 1.0000 - val_loss: 0.9718 - val_accuracy: 0.9179\n",
            "Epoch 163/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.0543e-08 - accuracy: 1.0000 - val_loss: 0.9715 - val_accuracy: 0.9179\n",
            "Epoch 164/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 6.9749e-08 - accuracy: 1.0000 - val_loss: 0.9713 - val_accuracy: 0.9179\n",
            "Epoch 165/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.8817e-08 - accuracy: 1.0000 - val_loss: 0.9711 - val_accuracy: 0.9179\n",
            "Epoch 166/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.7988e-08 - accuracy: 1.0000 - val_loss: 0.9709 - val_accuracy: 0.9179\n",
            "Epoch 167/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.7129e-08 - accuracy: 1.0000 - val_loss: 0.9707 - val_accuracy: 0.9179\n",
            "Epoch 168/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.6337e-08 - accuracy: 1.0000 - val_loss: 0.9705 - val_accuracy: 0.9179\n",
            "Epoch 169/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 6.5543e-08 - accuracy: 1.0000 - val_loss: 0.9703 - val_accuracy: 0.9179\n",
            "Epoch 170/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 6.4704e-08 - accuracy: 1.0000 - val_loss: 0.9701 - val_accuracy: 0.9179\n",
            "Epoch 171/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.4022e-08 - accuracy: 1.0000 - val_loss: 0.9698 - val_accuracy: 0.9179\n",
            "Epoch 172/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 6.3171e-08 - accuracy: 1.0000 - val_loss: 0.9697 - val_accuracy: 0.9179\n",
            "Epoch 173/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 6.2457e-08 - accuracy: 1.0000 - val_loss: 0.9695 - val_accuracy: 0.9179\n",
            "Epoch 174/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.1725e-08 - accuracy: 1.0000 - val_loss: 0.9693 - val_accuracy: 0.9179\n",
            "Epoch 175/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 6.1008e-08 - accuracy: 1.0000 - val_loss: 0.9690 - val_accuracy: 0.9179\n",
            "Epoch 176/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 6.0316e-08 - accuracy: 1.0000 - val_loss: 0.9688 - val_accuracy: 0.9179\n",
            "Epoch 177/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.9568e-08 - accuracy: 1.0000 - val_loss: 0.9686 - val_accuracy: 0.9179\n",
            "Epoch 178/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 5.8880e-08 - accuracy: 1.0000 - val_loss: 0.9684 - val_accuracy: 0.9179\n",
            "Epoch 179/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.8200e-08 - accuracy: 1.0000 - val_loss: 0.9682 - val_accuracy: 0.9179\n",
            "Epoch 180/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.7536e-08 - accuracy: 1.0000 - val_loss: 0.9680 - val_accuracy: 0.9179\n",
            "Epoch 181/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.6910e-08 - accuracy: 1.0000 - val_loss: 0.9679 - val_accuracy: 0.9179\n",
            "Epoch 182/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.6276e-08 - accuracy: 1.0000 - val_loss: 0.9677 - val_accuracy: 0.9179\n",
            "Epoch 183/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.5650e-08 - accuracy: 1.0000 - val_loss: 0.9674 - val_accuracy: 0.9179\n",
            "Epoch 184/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 5.5015e-08 - accuracy: 1.0000 - val_loss: 0.9673 - val_accuracy: 0.9179\n",
            "Epoch 185/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 5.4447e-08 - accuracy: 1.0000 - val_loss: 0.9671 - val_accuracy: 0.9168\n",
            "Epoch 186/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.3818e-08 - accuracy: 1.0000 - val_loss: 0.9669 - val_accuracy: 0.9168\n",
            "Epoch 187/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.3286e-08 - accuracy: 1.0000 - val_loss: 0.9667 - val_accuracy: 0.9168\n",
            "Epoch 188/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.2669e-08 - accuracy: 1.0000 - val_loss: 0.9665 - val_accuracy: 0.9168\n",
            "Epoch 189/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.2075e-08 - accuracy: 1.0000 - val_loss: 0.9663 - val_accuracy: 0.9168\n",
            "Epoch 190/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.1537e-08 - accuracy: 1.0000 - val_loss: 0.9662 - val_accuracy: 0.9168\n",
            "Epoch 191/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 5.0999e-08 - accuracy: 1.0000 - val_loss: 0.9660 - val_accuracy: 0.9168\n",
            "Epoch 192/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 5.0453e-08 - accuracy: 1.0000 - val_loss: 0.9658 - val_accuracy: 0.9168\n",
            "Epoch 193/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.0006e-08 - accuracy: 1.0000 - val_loss: 0.9656 - val_accuracy: 0.9168\n",
            "Epoch 194/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.9398e-08 - accuracy: 1.0000 - val_loss: 0.9655 - val_accuracy: 0.9168\n",
            "Epoch 195/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.8904e-08 - accuracy: 1.0000 - val_loss: 0.9653 - val_accuracy: 0.9168\n",
            "Epoch 196/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.8440e-08 - accuracy: 1.0000 - val_loss: 0.9651 - val_accuracy: 0.9168\n",
            "Epoch 197/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 4.7904e-08 - accuracy: 1.0000 - val_loss: 0.9649 - val_accuracy: 0.9168\n",
            "Epoch 198/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 4.7447e-08 - accuracy: 1.0000 - val_loss: 0.9648 - val_accuracy: 0.9168\n",
            "Epoch 199/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.7003e-08 - accuracy: 1.0000 - val_loss: 0.9646 - val_accuracy: 0.9168\n",
            "Epoch 200/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 4.6503e-08 - accuracy: 1.0000 - val_loss: 0.9644 - val_accuracy: 0.9168\n",
            "36/36 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.2299e-08 - accuracy: 1.0000 - val_loss: 0.9642 - val_accuracy: 0.9168\n",
            "Epoch 2/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.0897e-08 - accuracy: 1.0000 - val_loss: 0.9640 - val_accuracy: 0.9168\n",
            "Epoch 3/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 5.9611e-08 - accuracy: 1.0000 - val_loss: 0.9637 - val_accuracy: 0.9168\n",
            "Epoch 4/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 5.8412e-08 - accuracy: 1.0000 - val_loss: 0.9635 - val_accuracy: 0.9168\n",
            "Epoch 5/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.7355e-08 - accuracy: 1.0000 - val_loss: 0.9633 - val_accuracy: 0.9168\n",
            "Epoch 6/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 5.6294e-08 - accuracy: 1.0000 - val_loss: 0.9632 - val_accuracy: 0.9168\n",
            "Epoch 7/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 5.5364e-08 - accuracy: 1.0000 - val_loss: 0.9629 - val_accuracy: 0.9168\n",
            "Epoch 8/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 5.4407e-08 - accuracy: 1.0000 - val_loss: 0.9627 - val_accuracy: 0.9168\n",
            "Epoch 9/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 5.3512e-08 - accuracy: 1.0000 - val_loss: 0.9626 - val_accuracy: 0.9168\n",
            "Epoch 10/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.2720e-08 - accuracy: 1.0000 - val_loss: 0.9624 - val_accuracy: 0.9168\n",
            "Epoch 11/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.1997e-08 - accuracy: 1.0000 - val_loss: 0.9621 - val_accuracy: 0.9168\n",
            "Epoch 12/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 5.1141e-08 - accuracy: 1.0000 - val_loss: 0.9620 - val_accuracy: 0.9168\n",
            "Epoch 13/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 5.0501e-08 - accuracy: 1.0000 - val_loss: 0.9618 - val_accuracy: 0.9168\n",
            "Epoch 14/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.9720e-08 - accuracy: 1.0000 - val_loss: 0.9616 - val_accuracy: 0.9168\n",
            "Epoch 15/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.9061e-08 - accuracy: 1.0000 - val_loss: 0.9615 - val_accuracy: 0.9168\n",
            "Epoch 16/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.8470e-08 - accuracy: 1.0000 - val_loss: 0.9613 - val_accuracy: 0.9168\n",
            "Epoch 17/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.7802e-08 - accuracy: 1.0000 - val_loss: 0.9611 - val_accuracy: 0.9168\n",
            "Epoch 18/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 4.7212e-08 - accuracy: 1.0000 - val_loss: 0.9610 - val_accuracy: 0.9168\n",
            "Epoch 19/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 4.6536e-08 - accuracy: 1.0000 - val_loss: 0.9607 - val_accuracy: 0.9168\n",
            "Epoch 20/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.5959e-08 - accuracy: 1.0000 - val_loss: 0.9606 - val_accuracy: 0.9157\n",
            "Epoch 21/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 4.5427e-08 - accuracy: 1.0000 - val_loss: 0.9604 - val_accuracy: 0.9157\n",
            "Epoch 22/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 4.4895e-08 - accuracy: 1.0000 - val_loss: 0.9603 - val_accuracy: 0.9157\n",
            "Epoch 23/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.4381e-08 - accuracy: 1.0000 - val_loss: 0.9601 - val_accuracy: 0.9157\n",
            "Epoch 24/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.3919e-08 - accuracy: 1.0000 - val_loss: 0.9600 - val_accuracy: 0.9157\n",
            "Epoch 25/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 4.3448e-08 - accuracy: 1.0000 - val_loss: 0.9598 - val_accuracy: 0.9157\n",
            "Epoch 26/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.2968e-08 - accuracy: 1.0000 - val_loss: 0.9597 - val_accuracy: 0.9157\n",
            "Epoch 27/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 4.2534e-08 - accuracy: 1.0000 - val_loss: 0.9596 - val_accuracy: 0.9157\n",
            "Epoch 28/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 4.2126e-08 - accuracy: 1.0000 - val_loss: 0.9594 - val_accuracy: 0.9146\n",
            "Epoch 29/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 4.1669e-08 - accuracy: 1.0000 - val_loss: 0.9593 - val_accuracy: 0.9146\n",
            "Epoch 30/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.1307e-08 - accuracy: 1.0000 - val_loss: 0.9592 - val_accuracy: 0.9146\n",
            "Epoch 31/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 4.0871e-08 - accuracy: 1.0000 - val_loss: 0.9591 - val_accuracy: 0.9146\n",
            "Epoch 32/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 4.0560e-08 - accuracy: 1.0000 - val_loss: 0.9589 - val_accuracy: 0.9146\n",
            "Epoch 33/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.0118e-08 - accuracy: 1.0000 - val_loss: 0.9588 - val_accuracy: 0.9146\n",
            "Epoch 34/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.9789e-08 - accuracy: 1.0000 - val_loss: 0.9587 - val_accuracy: 0.9146\n",
            "Epoch 35/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.9424e-08 - accuracy: 1.0000 - val_loss: 0.9585 - val_accuracy: 0.9146\n",
            "Epoch 36/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.9062e-08 - accuracy: 1.0000 - val_loss: 0.9585 - val_accuracy: 0.9146\n",
            "Epoch 37/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.8743e-08 - accuracy: 1.0000 - val_loss: 0.9583 - val_accuracy: 0.9146\n",
            "Epoch 38/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.8421e-08 - accuracy: 1.0000 - val_loss: 0.9583 - val_accuracy: 0.9146\n",
            "Epoch 39/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.8124e-08 - accuracy: 1.0000 - val_loss: 0.9581 - val_accuracy: 0.9146\n",
            "Epoch 40/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.7804e-08 - accuracy: 1.0000 - val_loss: 0.9580 - val_accuracy: 0.9146\n",
            "Epoch 41/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.7517e-08 - accuracy: 1.0000 - val_loss: 0.9579 - val_accuracy: 0.9146\n",
            "Epoch 42/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.7217e-08 - accuracy: 1.0000 - val_loss: 0.9578 - val_accuracy: 0.9146\n",
            "Epoch 43/200\n",
            "37/37 [==============================] - 1s 19ms/step - loss: 3.6953e-08 - accuracy: 1.0000 - val_loss: 0.9577 - val_accuracy: 0.9146\n",
            "Epoch 44/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 3.6673e-08 - accuracy: 1.0000 - val_loss: 0.9576 - val_accuracy: 0.9146\n",
            "Epoch 45/200\n",
            "37/37 [==============================] - 1s 18ms/step - loss: 3.6409e-08 - accuracy: 1.0000 - val_loss: 0.9575 - val_accuracy: 0.9146\n",
            "Epoch 46/200\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 3.6174e-08 - accuracy: 1.0000 - val_loss: 0.9575 - val_accuracy: 0.9135\n",
            "Epoch 47/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.5889e-08 - accuracy: 1.0000 - val_loss: 0.9573 - val_accuracy: 0.9135\n",
            "Epoch 48/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.5572e-08 - accuracy: 1.0000 - val_loss: 0.9572 - val_accuracy: 0.9135\n",
            "Epoch 49/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.5342e-08 - accuracy: 1.0000 - val_loss: 0.9572 - val_accuracy: 0.9135\n",
            "Epoch 50/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 3.5118e-08 - accuracy: 1.0000 - val_loss: 0.9571 - val_accuracy: 0.9135\n",
            "Epoch 51/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.4891e-08 - accuracy: 1.0000 - val_loss: 0.9570 - val_accuracy: 0.9146\n",
            "Epoch 52/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.4669e-08 - accuracy: 1.0000 - val_loss: 0.9569 - val_accuracy: 0.9146\n",
            "Epoch 53/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.4464e-08 - accuracy: 1.0000 - val_loss: 0.9568 - val_accuracy: 0.9146\n",
            "Epoch 54/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.4222e-08 - accuracy: 1.0000 - val_loss: 0.9567 - val_accuracy: 0.9146\n",
            "Epoch 55/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.4022e-08 - accuracy: 1.0000 - val_loss: 0.9567 - val_accuracy: 0.9146\n",
            "Epoch 56/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.3830e-08 - accuracy: 1.0000 - val_loss: 0.9566 - val_accuracy: 0.9146\n",
            "Epoch 57/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.3641e-08 - accuracy: 1.0000 - val_loss: 0.9566 - val_accuracy: 0.9146\n",
            "Epoch 58/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.3457e-08 - accuracy: 1.0000 - val_loss: 0.9565 - val_accuracy: 0.9146\n",
            "Epoch 59/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.3264e-08 - accuracy: 1.0000 - val_loss: 0.9564 - val_accuracy: 0.9135\n",
            "Epoch 60/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.3091e-08 - accuracy: 1.0000 - val_loss: 0.9563 - val_accuracy: 0.9135\n",
            "Epoch 61/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.2910e-08 - accuracy: 1.0000 - val_loss: 0.9563 - val_accuracy: 0.9135\n",
            "Epoch 62/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.2746e-08 - accuracy: 1.0000 - val_loss: 0.9562 - val_accuracy: 0.9135\n",
            "Epoch 63/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.2592e-08 - accuracy: 1.0000 - val_loss: 0.9562 - val_accuracy: 0.9135\n",
            "Epoch 64/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.2422e-08 - accuracy: 1.0000 - val_loss: 0.9561 - val_accuracy: 0.9135\n",
            "Epoch 65/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.2271e-08 - accuracy: 1.0000 - val_loss: 0.9561 - val_accuracy: 0.9135\n",
            "Epoch 66/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.2110e-08 - accuracy: 1.0000 - val_loss: 0.9560 - val_accuracy: 0.9135\n",
            "Epoch 67/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.1936e-08 - accuracy: 1.0000 - val_loss: 0.9560 - val_accuracy: 0.9135\n",
            "Epoch 68/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.1794e-08 - accuracy: 1.0000 - val_loss: 0.9559 - val_accuracy: 0.9135\n",
            "Epoch 69/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.1653e-08 - accuracy: 1.0000 - val_loss: 0.9558 - val_accuracy: 0.9135\n",
            "Epoch 70/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.1512e-08 - accuracy: 1.0000 - val_loss: 0.9558 - val_accuracy: 0.9135\n",
            "Epoch 71/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.1391e-08 - accuracy: 1.0000 - val_loss: 0.9558 - val_accuracy: 0.9135\n",
            "Epoch 72/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.1254e-08 - accuracy: 1.0000 - val_loss: 0.9557 - val_accuracy: 0.9135\n",
            "Epoch 73/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.1132e-08 - accuracy: 1.0000 - val_loss: 0.9557 - val_accuracy: 0.9135\n",
            "Epoch 74/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.1017e-08 - accuracy: 1.0000 - val_loss: 0.9556 - val_accuracy: 0.9135\n",
            "Epoch 75/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.0891e-08 - accuracy: 1.0000 - val_loss: 0.9556 - val_accuracy: 0.9135\n",
            "Epoch 76/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.0776e-08 - accuracy: 1.0000 - val_loss: 0.9556 - val_accuracy: 0.9135\n",
            "Epoch 77/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.0659e-08 - accuracy: 1.0000 - val_loss: 0.9555 - val_accuracy: 0.9135\n",
            "Epoch 78/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.0526e-08 - accuracy: 1.0000 - val_loss: 0.9555 - val_accuracy: 0.9135\n",
            "Epoch 79/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 3.0398e-08 - accuracy: 1.0000 - val_loss: 0.9555 - val_accuracy: 0.9135\n",
            "Epoch 80/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.0298e-08 - accuracy: 1.0000 - val_loss: 0.9555 - val_accuracy: 0.9135\n",
            "Epoch 81/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.0193e-08 - accuracy: 1.0000 - val_loss: 0.9554 - val_accuracy: 0.9135\n",
            "Epoch 82/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 3.0095e-08 - accuracy: 1.0000 - val_loss: 0.9554 - val_accuracy: 0.9135\n",
            "Epoch 83/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 3.0000e-08 - accuracy: 1.0000 - val_loss: 0.9554 - val_accuracy: 0.9135\n",
            "Epoch 84/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.9905e-08 - accuracy: 1.0000 - val_loss: 0.9554 - val_accuracy: 0.9135\n",
            "Epoch 85/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.9814e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 86/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.9732e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 87/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.9638e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 88/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.9551e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 89/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.9470e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 90/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.9383e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 91/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.9303e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 92/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.9233e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 93/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.9148e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 94/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.9073e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 95/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.9004e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 96/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.8921e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 97/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.8853e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 98/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.8782e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 99/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.8724e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 100/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.8651e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 101/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.8547e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 102/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.8494e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 103/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.8430e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 104/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.8361e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 105/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.8300e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 106/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.8229e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 107/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.8180e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 108/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.8125e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 109/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.8045e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 110/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.7992e-08 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.9146\n",
            "Epoch 111/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.7937e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 112/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.7880e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 113/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.7820e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 114/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.7767e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 115/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.7702e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 116/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.7643e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 117/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.7585e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 118/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.7525e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 119/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.7476e-08 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.9146\n",
            "Epoch 120/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.7416e-08 - accuracy: 1.0000 - val_loss: 0.9554 - val_accuracy: 0.9146\n",
            "Epoch 121/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.7366e-08 - accuracy: 1.0000 - val_loss: 0.9554 - val_accuracy: 0.9146\n",
            "Epoch 122/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.7309e-08 - accuracy: 1.0000 - val_loss: 0.9554 - val_accuracy: 0.9146\n",
            "Epoch 123/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.7253e-08 - accuracy: 1.0000 - val_loss: 0.9554 - val_accuracy: 0.9146\n",
            "Epoch 124/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.7188e-08 - accuracy: 1.0000 - val_loss: 0.9554 - val_accuracy: 0.9146\n",
            "Epoch 125/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2.7149e-08 - accuracy: 1.0000 - val_loss: 0.9555 - val_accuracy: 0.9146\n",
            "Epoch 126/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.7082e-08 - accuracy: 1.0000 - val_loss: 0.9555 - val_accuracy: 0.9146\n",
            "Epoch 127/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.7036e-08 - accuracy: 1.0000 - val_loss: 0.9555 - val_accuracy: 0.9146\n",
            "Epoch 128/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2.6972e-08 - accuracy: 1.0000 - val_loss: 0.9555 - val_accuracy: 0.9146\n",
            "Epoch 129/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6932e-08 - accuracy: 1.0000 - val_loss: 0.9555 - val_accuracy: 0.9146\n",
            "Epoch 130/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6863e-08 - accuracy: 1.0000 - val_loss: 0.9555 - val_accuracy: 0.9146\n",
            "Epoch 131/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6812e-08 - accuracy: 1.0000 - val_loss: 0.9556 - val_accuracy: 0.9146\n",
            "Epoch 132/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6748e-08 - accuracy: 1.0000 - val_loss: 0.9556 - val_accuracy: 0.9146\n",
            "Epoch 133/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6704e-08 - accuracy: 1.0000 - val_loss: 0.9556 - val_accuracy: 0.9146\n",
            "Epoch 134/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6656e-08 - accuracy: 1.0000 - val_loss: 0.9556 - val_accuracy: 0.9146\n",
            "Epoch 135/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6593e-08 - accuracy: 1.0000 - val_loss: 0.9557 - val_accuracy: 0.9146\n",
            "Epoch 136/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.6538e-08 - accuracy: 1.0000 - val_loss: 0.9557 - val_accuracy: 0.9146\n",
            "Epoch 137/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6477e-08 - accuracy: 1.0000 - val_loss: 0.9557 - val_accuracy: 0.9146\n",
            "Epoch 138/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6438e-08 - accuracy: 1.0000 - val_loss: 0.9557 - val_accuracy: 0.9146\n",
            "Epoch 139/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.6378e-08 - accuracy: 1.0000 - val_loss: 0.9558 - val_accuracy: 0.9146\n",
            "Epoch 140/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6321e-08 - accuracy: 1.0000 - val_loss: 0.9558 - val_accuracy: 0.9146\n",
            "Epoch 141/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.6258e-08 - accuracy: 1.0000 - val_loss: 0.9558 - val_accuracy: 0.9146\n",
            "Epoch 142/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.6217e-08 - accuracy: 1.0000 - val_loss: 0.9558 - val_accuracy: 0.9146\n",
            "Epoch 143/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6156e-08 - accuracy: 1.0000 - val_loss: 0.9559 - val_accuracy: 0.9146\n",
            "Epoch 144/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.6097e-08 - accuracy: 1.0000 - val_loss: 0.9559 - val_accuracy: 0.9146\n",
            "Epoch 145/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.6071e-08 - accuracy: 1.0000 - val_loss: 0.9559 - val_accuracy: 0.9146\n",
            "Epoch 146/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.5998e-08 - accuracy: 1.0000 - val_loss: 0.9560 - val_accuracy: 0.9146\n",
            "Epoch 147/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.5936e-08 - accuracy: 1.0000 - val_loss: 0.9560 - val_accuracy: 0.9146\n",
            "Epoch 148/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.5869e-08 - accuracy: 1.0000 - val_loss: 0.9560 - val_accuracy: 0.9146\n",
            "Epoch 149/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.5854e-08 - accuracy: 1.0000 - val_loss: 0.9561 - val_accuracy: 0.9146\n",
            "Epoch 150/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.5767e-08 - accuracy: 1.0000 - val_loss: 0.9561 - val_accuracy: 0.9146\n",
            "Epoch 151/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.5708e-08 - accuracy: 1.0000 - val_loss: 0.9561 - val_accuracy: 0.9146\n",
            "Epoch 152/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.5652e-08 - accuracy: 1.0000 - val_loss: 0.9562 - val_accuracy: 0.9146\n",
            "Epoch 153/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.5609e-08 - accuracy: 1.0000 - val_loss: 0.9562 - val_accuracy: 0.9146\n",
            "Epoch 154/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.5554e-08 - accuracy: 1.0000 - val_loss: 0.9562 - val_accuracy: 0.9146\n",
            "Epoch 155/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.5487e-08 - accuracy: 1.0000 - val_loss: 0.9563 - val_accuracy: 0.9146\n",
            "Epoch 156/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.5430e-08 - accuracy: 1.0000 - val_loss: 0.9563 - val_accuracy: 0.9146\n",
            "Epoch 157/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.5374e-08 - accuracy: 1.0000 - val_loss: 0.9563 - val_accuracy: 0.9146\n",
            "Epoch 158/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.5329e-08 - accuracy: 1.0000 - val_loss: 0.9564 - val_accuracy: 0.9146\n",
            "Epoch 159/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.5265e-08 - accuracy: 1.0000 - val_loss: 0.9564 - val_accuracy: 0.9146\n",
            "Epoch 160/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.5204e-08 - accuracy: 1.0000 - val_loss: 0.9564 - val_accuracy: 0.9146\n",
            "Epoch 161/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.5157e-08 - accuracy: 1.0000 - val_loss: 0.9565 - val_accuracy: 0.9146\n",
            "Epoch 162/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.5095e-08 - accuracy: 1.0000 - val_loss: 0.9565 - val_accuracy: 0.9146\n",
            "Epoch 163/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.5038e-08 - accuracy: 1.0000 - val_loss: 0.9566 - val_accuracy: 0.9146\n",
            "Epoch 164/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.4986e-08 - accuracy: 1.0000 - val_loss: 0.9566 - val_accuracy: 0.9146\n",
            "Epoch 165/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.4929e-08 - accuracy: 1.0000 - val_loss: 0.9566 - val_accuracy: 0.9146\n",
            "Epoch 166/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.4868e-08 - accuracy: 1.0000 - val_loss: 0.9567 - val_accuracy: 0.9146\n",
            "Epoch 167/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.4809e-08 - accuracy: 1.0000 - val_loss: 0.9567 - val_accuracy: 0.9146\n",
            "Epoch 168/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.4768e-08 - accuracy: 1.0000 - val_loss: 0.9568 - val_accuracy: 0.9146\n",
            "Epoch 169/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.4701e-08 - accuracy: 1.0000 - val_loss: 0.9568 - val_accuracy: 0.9146\n",
            "Epoch 170/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.4647e-08 - accuracy: 1.0000 - val_loss: 0.9569 - val_accuracy: 0.9146\n",
            "Epoch 171/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.4575e-08 - accuracy: 1.0000 - val_loss: 0.9569 - val_accuracy: 0.9146\n",
            "Epoch 172/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.4517e-08 - accuracy: 1.0000 - val_loss: 0.9570 - val_accuracy: 0.9146\n",
            "Epoch 173/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.4484e-08 - accuracy: 1.0000 - val_loss: 0.9571 - val_accuracy: 0.9146\n",
            "Epoch 174/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.4420e-08 - accuracy: 1.0000 - val_loss: 0.9571 - val_accuracy: 0.9146\n",
            "Epoch 175/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.4345e-08 - accuracy: 1.0000 - val_loss: 0.9572 - val_accuracy: 0.9146\n",
            "Epoch 176/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.4292e-08 - accuracy: 1.0000 - val_loss: 0.9572 - val_accuracy: 0.9146\n",
            "Epoch 177/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.4229e-08 - accuracy: 1.0000 - val_loss: 0.9572 - val_accuracy: 0.9146\n",
            "Epoch 178/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.4166e-08 - accuracy: 1.0000 - val_loss: 0.9573 - val_accuracy: 0.9146\n",
            "Epoch 179/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.4105e-08 - accuracy: 1.0000 - val_loss: 0.9573 - val_accuracy: 0.9146\n",
            "Epoch 180/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.4049e-08 - accuracy: 1.0000 - val_loss: 0.9574 - val_accuracy: 0.9146\n",
            "Epoch 181/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.4009e-08 - accuracy: 1.0000 - val_loss: 0.9575 - val_accuracy: 0.9146\n",
            "Epoch 182/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.3928e-08 - accuracy: 1.0000 - val_loss: 0.9575 - val_accuracy: 0.9146\n",
            "Epoch 183/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.3861e-08 - accuracy: 1.0000 - val_loss: 0.9576 - val_accuracy: 0.9146\n",
            "Epoch 184/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.3818e-08 - accuracy: 1.0000 - val_loss: 0.9576 - val_accuracy: 0.9146\n",
            "Epoch 185/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.3768e-08 - accuracy: 1.0000 - val_loss: 0.9576 - val_accuracy: 0.9146\n",
            "Epoch 186/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.3684e-08 - accuracy: 1.0000 - val_loss: 0.9578 - val_accuracy: 0.9146\n",
            "Epoch 187/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.3618e-08 - accuracy: 1.0000 - val_loss: 0.9579 - val_accuracy: 0.9146\n",
            "Epoch 188/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.3555e-08 - accuracy: 1.0000 - val_loss: 0.9579 - val_accuracy: 0.9146\n",
            "Epoch 189/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.3521e-08 - accuracy: 1.0000 - val_loss: 0.9580 - val_accuracy: 0.9146\n",
            "Epoch 190/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.3421e-08 - accuracy: 1.0000 - val_loss: 0.9580 - val_accuracy: 0.9146\n",
            "Epoch 191/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.3365e-08 - accuracy: 1.0000 - val_loss: 0.9581 - val_accuracy: 0.9146\n",
            "Epoch 192/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.3307e-08 - accuracy: 1.0000 - val_loss: 0.9582 - val_accuracy: 0.9146\n",
            "Epoch 193/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.3234e-08 - accuracy: 1.0000 - val_loss: 0.9582 - val_accuracy: 0.9146\n",
            "Epoch 194/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.3196e-08 - accuracy: 1.0000 - val_loss: 0.9583 - val_accuracy: 0.9146\n",
            "Epoch 195/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.3113e-08 - accuracy: 1.0000 - val_loss: 0.9584 - val_accuracy: 0.9146\n",
            "Epoch 196/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.3069e-08 - accuracy: 1.0000 - val_loss: 0.9584 - val_accuracy: 0.9146\n",
            "Epoch 197/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.2987e-08 - accuracy: 1.0000 - val_loss: 0.9585 - val_accuracy: 0.9146\n",
            "Epoch 198/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.2925e-08 - accuracy: 1.0000 - val_loss: 0.9586 - val_accuracy: 0.9146\n",
            "Epoch 199/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.2863e-08 - accuracy: 1.0000 - val_loss: 0.9586 - val_accuracy: 0.9146\n",
            "Epoch 200/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.2799e-08 - accuracy: 1.0000 - val_loss: 0.9587 - val_accuracy: 0.9146\n",
            "36/36 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.3157e-08 - accuracy: 1.0000 - val_loss: 0.9588 - val_accuracy: 0.9146\n",
            "Epoch 2/200\n",
            "37/37 [==============================] - 1s 19ms/step - loss: 2.3068e-08 - accuracy: 1.0000 - val_loss: 0.9588 - val_accuracy: 0.9146\n",
            "Epoch 3/200\n",
            "37/37 [==============================] - 1s 22ms/step - loss: 2.2981e-08 - accuracy: 1.0000 - val_loss: 0.9589 - val_accuracy: 0.9146\n",
            "Epoch 4/200\n",
            "37/37 [==============================] - 1s 15ms/step - loss: 2.2920e-08 - accuracy: 1.0000 - val_loss: 0.9589 - val_accuracy: 0.9146\n",
            "Epoch 5/200\n",
            "37/37 [==============================] - 1s 15ms/step - loss: 2.2820e-08 - accuracy: 1.0000 - val_loss: 0.9590 - val_accuracy: 0.9146\n",
            "Epoch 6/200\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 2.2748e-08 - accuracy: 1.0000 - val_loss: 0.9591 - val_accuracy: 0.9146\n",
            "Epoch 7/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.2650e-08 - accuracy: 1.0000 - val_loss: 0.9592 - val_accuracy: 0.9146\n",
            "Epoch 8/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.2581e-08 - accuracy: 1.0000 - val_loss: 0.9592 - val_accuracy: 0.9146\n",
            "Epoch 9/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.2481e-08 - accuracy: 1.0000 - val_loss: 0.9593 - val_accuracy: 0.9146\n",
            "Epoch 10/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.2404e-08 - accuracy: 1.0000 - val_loss: 0.9594 - val_accuracy: 0.9146\n",
            "Epoch 11/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.2330e-08 - accuracy: 1.0000 - val_loss: 0.9594 - val_accuracy: 0.9146\n",
            "Epoch 12/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.2261e-08 - accuracy: 1.0000 - val_loss: 0.9596 - val_accuracy: 0.9146\n",
            "Epoch 13/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.2186e-08 - accuracy: 1.0000 - val_loss: 0.9596 - val_accuracy: 0.9146\n",
            "Epoch 14/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.2079e-08 - accuracy: 1.0000 - val_loss: 0.9597 - val_accuracy: 0.9146\n",
            "Epoch 15/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.2006e-08 - accuracy: 1.0000 - val_loss: 0.9598 - val_accuracy: 0.9146\n",
            "Epoch 16/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.1945e-08 - accuracy: 1.0000 - val_loss: 0.9599 - val_accuracy: 0.9146\n",
            "Epoch 17/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.1865e-08 - accuracy: 1.0000 - val_loss: 0.9600 - val_accuracy: 0.9146\n",
            "Epoch 18/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.1777e-08 - accuracy: 1.0000 - val_loss: 0.9601 - val_accuracy: 0.9146\n",
            "Epoch 19/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.1689e-08 - accuracy: 1.0000 - val_loss: 0.9602 - val_accuracy: 0.9146\n",
            "Epoch 20/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.1642e-08 - accuracy: 1.0000 - val_loss: 0.9603 - val_accuracy: 0.9146\n",
            "Epoch 21/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.1532e-08 - accuracy: 1.0000 - val_loss: 0.9604 - val_accuracy: 0.9146\n",
            "Epoch 22/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.1451e-08 - accuracy: 1.0000 - val_loss: 0.9605 - val_accuracy: 0.9146\n",
            "Epoch 23/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1391e-08 - accuracy: 1.0000 - val_loss: 0.9606 - val_accuracy: 0.9146\n",
            "Epoch 24/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.1308e-08 - accuracy: 1.0000 - val_loss: 0.9607 - val_accuracy: 0.9146\n",
            "Epoch 25/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.1231e-08 - accuracy: 1.0000 - val_loss: 0.9608 - val_accuracy: 0.9146\n",
            "Epoch 26/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.1135e-08 - accuracy: 1.0000 - val_loss: 0.9608 - val_accuracy: 0.9146\n",
            "Epoch 27/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.1076e-08 - accuracy: 1.0000 - val_loss: 0.9610 - val_accuracy: 0.9146\n",
            "Epoch 28/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.0983e-08 - accuracy: 1.0000 - val_loss: 0.9611 - val_accuracy: 0.9146\n",
            "Epoch 29/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.0911e-08 - accuracy: 1.0000 - val_loss: 0.9612 - val_accuracy: 0.9146\n",
            "Epoch 30/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.0818e-08 - accuracy: 1.0000 - val_loss: 0.9613 - val_accuracy: 0.9146\n",
            "Epoch 31/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.0736e-08 - accuracy: 1.0000 - val_loss: 0.9614 - val_accuracy: 0.9146\n",
            "Epoch 32/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.0678e-08 - accuracy: 1.0000 - val_loss: 0.9616 - val_accuracy: 0.9146\n",
            "Epoch 33/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.0578e-08 - accuracy: 1.0000 - val_loss: 0.9616 - val_accuracy: 0.9146\n",
            "Epoch 34/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.0507e-08 - accuracy: 1.0000 - val_loss: 0.9618 - val_accuracy: 0.9146\n",
            "Epoch 35/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.0426e-08 - accuracy: 1.0000 - val_loss: 0.9619 - val_accuracy: 0.9146\n",
            "Epoch 36/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.0361e-08 - accuracy: 1.0000 - val_loss: 0.9620 - val_accuracy: 0.9146\n",
            "Epoch 37/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.0227e-08 - accuracy: 1.0000 - val_loss: 0.9622 - val_accuracy: 0.9146\n",
            "Epoch 38/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.0155e-08 - accuracy: 1.0000 - val_loss: 0.9624 - val_accuracy: 0.9146\n",
            "Epoch 39/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.0082e-08 - accuracy: 1.0000 - val_loss: 0.9625 - val_accuracy: 0.9146\n",
            "Epoch 40/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.9988e-08 - accuracy: 1.0000 - val_loss: 0.9626 - val_accuracy: 0.9146\n",
            "Epoch 41/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.9895e-08 - accuracy: 1.0000 - val_loss: 0.9627 - val_accuracy: 0.9146\n",
            "Epoch 42/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.9812e-08 - accuracy: 1.0000 - val_loss: 0.9628 - val_accuracy: 0.9146\n",
            "Epoch 43/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.9731e-08 - accuracy: 1.0000 - val_loss: 0.9629 - val_accuracy: 0.9146\n",
            "Epoch 44/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.9660e-08 - accuracy: 1.0000 - val_loss: 0.9632 - val_accuracy: 0.9146\n",
            "Epoch 45/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.9555e-08 - accuracy: 1.0000 - val_loss: 0.9633 - val_accuracy: 0.9146\n",
            "Epoch 46/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.9488e-08 - accuracy: 1.0000 - val_loss: 0.9634 - val_accuracy: 0.9146\n",
            "Epoch 47/200\n",
            "37/37 [==============================] - 1s 15ms/step - loss: 1.9407e-08 - accuracy: 1.0000 - val_loss: 0.9636 - val_accuracy: 0.9146\n",
            "Epoch 48/200\n",
            "37/37 [==============================] - 1s 18ms/step - loss: 1.9308e-08 - accuracy: 1.0000 - val_loss: 0.9637 - val_accuracy: 0.9146\n",
            "Epoch 49/200\n",
            "37/37 [==============================] - 1s 17ms/step - loss: 1.9214e-08 - accuracy: 1.0000 - val_loss: 0.9639 - val_accuracy: 0.9146\n",
            "Epoch 50/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 1.9141e-08 - accuracy: 1.0000 - val_loss: 0.9640 - val_accuracy: 0.9146\n",
            "Epoch 51/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.9043e-08 - accuracy: 1.0000 - val_loss: 0.9641 - val_accuracy: 0.9146\n",
            "Epoch 52/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.8964e-08 - accuracy: 1.0000 - val_loss: 0.9643 - val_accuracy: 0.9146\n",
            "Epoch 53/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.8879e-08 - accuracy: 1.0000 - val_loss: 0.9644 - val_accuracy: 0.9146\n",
            "Epoch 54/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.8788e-08 - accuracy: 1.0000 - val_loss: 0.9645 - val_accuracy: 0.9146\n",
            "Epoch 55/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.8667e-08 - accuracy: 1.0000 - val_loss: 0.9648 - val_accuracy: 0.9146\n",
            "Epoch 56/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.8661e-08 - accuracy: 1.0000 - val_loss: 0.9651 - val_accuracy: 0.9146\n",
            "Epoch 57/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.8506e-08 - accuracy: 1.0000 - val_loss: 0.9652 - val_accuracy: 0.9146\n",
            "Epoch 58/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.8405e-08 - accuracy: 1.0000 - val_loss: 0.9654 - val_accuracy: 0.9146\n",
            "Epoch 59/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.8311e-08 - accuracy: 1.0000 - val_loss: 0.9655 - val_accuracy: 0.9146\n",
            "Epoch 60/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.8205e-08 - accuracy: 1.0000 - val_loss: 0.9656 - val_accuracy: 0.9146\n",
            "Epoch 61/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.8118e-08 - accuracy: 1.0000 - val_loss: 0.9659 - val_accuracy: 0.9146\n",
            "Epoch 62/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.8028e-08 - accuracy: 1.0000 - val_loss: 0.9661 - val_accuracy: 0.9146\n",
            "Epoch 63/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.7972e-08 - accuracy: 1.0000 - val_loss: 0.9663 - val_accuracy: 0.9146\n",
            "Epoch 64/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.7863e-08 - accuracy: 1.0000 - val_loss: 0.9664 - val_accuracy: 0.9146\n",
            "Epoch 65/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.7775e-08 - accuracy: 1.0000 - val_loss: 0.9666 - val_accuracy: 0.9146\n",
            "Epoch 66/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.7665e-08 - accuracy: 1.0000 - val_loss: 0.9667 - val_accuracy: 0.9146\n",
            "Epoch 67/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.7598e-08 - accuracy: 1.0000 - val_loss: 0.9670 - val_accuracy: 0.9146\n",
            "Epoch 68/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.7501e-08 - accuracy: 1.0000 - val_loss: 0.9671 - val_accuracy: 0.9146\n",
            "Epoch 69/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.7406e-08 - accuracy: 1.0000 - val_loss: 0.9673 - val_accuracy: 0.9146\n",
            "Epoch 70/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.7366e-08 - accuracy: 1.0000 - val_loss: 0.9676 - val_accuracy: 0.9146\n",
            "Epoch 71/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.7241e-08 - accuracy: 1.0000 - val_loss: 0.9676 - val_accuracy: 0.9146\n",
            "Epoch 72/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.7124e-08 - accuracy: 1.0000 - val_loss: 0.9679 - val_accuracy: 0.9146\n",
            "Epoch 73/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.7045e-08 - accuracy: 1.0000 - val_loss: 0.9681 - val_accuracy: 0.9146\n",
            "Epoch 74/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.6961e-08 - accuracy: 1.0000 - val_loss: 0.9684 - val_accuracy: 0.9146\n",
            "Epoch 75/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.6865e-08 - accuracy: 1.0000 - val_loss: 0.9686 - val_accuracy: 0.9146\n",
            "Epoch 76/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.6757e-08 - accuracy: 1.0000 - val_loss: 0.9687 - val_accuracy: 0.9146\n",
            "Epoch 77/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.6670e-08 - accuracy: 1.0000 - val_loss: 0.9690 - val_accuracy: 0.9146\n",
            "Epoch 78/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.6600e-08 - accuracy: 1.0000 - val_loss: 0.9692 - val_accuracy: 0.9146\n",
            "Epoch 79/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.6489e-08 - accuracy: 1.0000 - val_loss: 0.9694 - val_accuracy: 0.9146\n",
            "Epoch 80/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.6406e-08 - accuracy: 1.0000 - val_loss: 0.9696 - val_accuracy: 0.9146\n",
            "Epoch 81/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.6297e-08 - accuracy: 1.0000 - val_loss: 0.9699 - val_accuracy: 0.9146\n",
            "Epoch 82/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.6198e-08 - accuracy: 1.0000 - val_loss: 0.9701 - val_accuracy: 0.9146\n",
            "Epoch 83/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.6102e-08 - accuracy: 1.0000 - val_loss: 0.9702 - val_accuracy: 0.9146\n",
            "Epoch 84/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.6073e-08 - accuracy: 1.0000 - val_loss: 0.9706 - val_accuracy: 0.9146\n",
            "Epoch 85/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.5909e-08 - accuracy: 1.0000 - val_loss: 0.9707 - val_accuracy: 0.9146\n",
            "Epoch 86/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.5826e-08 - accuracy: 1.0000 - val_loss: 0.9710 - val_accuracy: 0.9146\n",
            "Epoch 87/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.5813e-08 - accuracy: 1.0000 - val_loss: 0.9713 - val_accuracy: 0.9146\n",
            "Epoch 88/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.5632e-08 - accuracy: 1.0000 - val_loss: 0.9715 - val_accuracy: 0.9146\n",
            "Epoch 89/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.5543e-08 - accuracy: 1.0000 - val_loss: 0.9717 - val_accuracy: 0.9146\n",
            "Epoch 90/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.5450e-08 - accuracy: 1.0000 - val_loss: 0.9718 - val_accuracy: 0.9146\n",
            "Epoch 91/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.5369e-08 - accuracy: 1.0000 - val_loss: 0.9723 - val_accuracy: 0.9146\n",
            "Epoch 92/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.5288e-08 - accuracy: 1.0000 - val_loss: 0.9725 - val_accuracy: 0.9146\n",
            "Epoch 93/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.5161e-08 - accuracy: 1.0000 - val_loss: 0.9728 - val_accuracy: 0.9146\n",
            "Epoch 94/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.5072e-08 - accuracy: 1.0000 - val_loss: 0.9730 - val_accuracy: 0.9146\n",
            "Epoch 95/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.4963e-08 - accuracy: 1.0000 - val_loss: 0.9733 - val_accuracy: 0.9146\n",
            "Epoch 96/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.4892e-08 - accuracy: 1.0000 - val_loss: 0.9736 - val_accuracy: 0.9146\n",
            "Epoch 97/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.4799e-08 - accuracy: 1.0000 - val_loss: 0.9739 - val_accuracy: 0.9146\n",
            "Epoch 98/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.4654e-08 - accuracy: 1.0000 - val_loss: 0.9741 - val_accuracy: 0.9146\n",
            "Epoch 99/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.4613e-08 - accuracy: 1.0000 - val_loss: 0.9744 - val_accuracy: 0.9146\n",
            "Epoch 100/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.4450e-08 - accuracy: 1.0000 - val_loss: 0.9746 - val_accuracy: 0.9146\n",
            "Epoch 101/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.4362e-08 - accuracy: 1.0000 - val_loss: 0.9749 - val_accuracy: 0.9146\n",
            "Epoch 102/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.4269e-08 - accuracy: 1.0000 - val_loss: 0.9752 - val_accuracy: 0.9146\n",
            "Epoch 103/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.4162e-08 - accuracy: 1.0000 - val_loss: 0.9755 - val_accuracy: 0.9146\n",
            "Epoch 104/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.4066e-08 - accuracy: 1.0000 - val_loss: 0.9758 - val_accuracy: 0.9146\n",
            "Epoch 105/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.4026e-08 - accuracy: 1.0000 - val_loss: 0.9761 - val_accuracy: 0.9146\n",
            "Epoch 106/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.3885e-08 - accuracy: 1.0000 - val_loss: 0.9764 - val_accuracy: 0.9146\n",
            "Epoch 107/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.3793e-08 - accuracy: 1.0000 - val_loss: 0.9767 - val_accuracy: 0.9146\n",
            "Epoch 108/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.3665e-08 - accuracy: 1.0000 - val_loss: 0.9770 - val_accuracy: 0.9146\n",
            "Epoch 109/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.3556e-08 - accuracy: 1.0000 - val_loss: 0.9773 - val_accuracy: 0.9146\n",
            "Epoch 110/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.3458e-08 - accuracy: 1.0000 - val_loss: 0.9775 - val_accuracy: 0.9146\n",
            "Epoch 111/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.3403e-08 - accuracy: 1.0000 - val_loss: 0.9783 - val_accuracy: 0.9146\n",
            "Epoch 112/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.3262e-08 - accuracy: 1.0000 - val_loss: 0.9783 - val_accuracy: 0.9146\n",
            "Epoch 113/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.3156e-08 - accuracy: 1.0000 - val_loss: 0.9789 - val_accuracy: 0.9146\n",
            "Epoch 114/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.3043e-08 - accuracy: 1.0000 - val_loss: 0.9791 - val_accuracy: 0.9146\n",
            "Epoch 115/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.2988e-08 - accuracy: 1.0000 - val_loss: 0.9795 - val_accuracy: 0.9146\n",
            "Epoch 116/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.2810e-08 - accuracy: 1.0000 - val_loss: 0.9798 - val_accuracy: 0.9146\n",
            "Epoch 117/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.2740e-08 - accuracy: 1.0000 - val_loss: 0.9802 - val_accuracy: 0.9146\n",
            "Epoch 118/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.2613e-08 - accuracy: 1.0000 - val_loss: 0.9804 - val_accuracy: 0.9146\n",
            "Epoch 119/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.2502e-08 - accuracy: 1.0000 - val_loss: 0.9807 - val_accuracy: 0.9146\n",
            "Epoch 120/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.2457e-08 - accuracy: 1.0000 - val_loss: 0.9812 - val_accuracy: 0.9146\n",
            "Epoch 121/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.2331e-08 - accuracy: 1.0000 - val_loss: 0.9815 - val_accuracy: 0.9146\n",
            "Epoch 122/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.2255e-08 - accuracy: 1.0000 - val_loss: 0.9819 - val_accuracy: 0.9146\n",
            "Epoch 123/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.2114e-08 - accuracy: 1.0000 - val_loss: 0.9822 - val_accuracy: 0.9146\n",
            "Epoch 124/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.2023e-08 - accuracy: 1.0000 - val_loss: 0.9826 - val_accuracy: 0.9146\n",
            "Epoch 125/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.1984e-08 - accuracy: 1.0000 - val_loss: 0.9829 - val_accuracy: 0.9146\n",
            "Epoch 126/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.1839e-08 - accuracy: 1.0000 - val_loss: 0.9833 - val_accuracy: 0.9146\n",
            "Epoch 127/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1731e-08 - accuracy: 1.0000 - val_loss: 0.9835 - val_accuracy: 0.9146\n",
            "Epoch 128/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.1674e-08 - accuracy: 1.0000 - val_loss: 0.9840 - val_accuracy: 0.9146\n",
            "Epoch 129/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.1622e-08 - accuracy: 1.0000 - val_loss: 0.9844 - val_accuracy: 0.9146\n",
            "Epoch 130/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1434e-08 - accuracy: 1.0000 - val_loss: 0.9847 - val_accuracy: 0.9146\n",
            "Epoch 131/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.1352e-08 - accuracy: 1.0000 - val_loss: 0.9851 - val_accuracy: 0.9146\n",
            "Epoch 132/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.1254e-08 - accuracy: 1.0000 - val_loss: 0.9855 - val_accuracy: 0.9146\n",
            "Epoch 133/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.1145e-08 - accuracy: 1.0000 - val_loss: 0.9858 - val_accuracy: 0.9146\n",
            "Epoch 134/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1043e-08 - accuracy: 1.0000 - val_loss: 0.9861 - val_accuracy: 0.9146\n",
            "Epoch 135/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0983e-08 - accuracy: 1.0000 - val_loss: 0.9867 - val_accuracy: 0.9146\n",
            "Epoch 136/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0883e-08 - accuracy: 1.0000 - val_loss: 0.9869 - val_accuracy: 0.9146\n",
            "Epoch 137/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0784e-08 - accuracy: 1.0000 - val_loss: 0.9875 - val_accuracy: 0.9146\n",
            "Epoch 138/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.0689e-08 - accuracy: 1.0000 - val_loss: 0.9880 - val_accuracy: 0.9146\n",
            "Epoch 139/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.0580e-08 - accuracy: 1.0000 - val_loss: 0.9883 - val_accuracy: 0.9146\n",
            "Epoch 140/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.0489e-08 - accuracy: 1.0000 - val_loss: 0.9888 - val_accuracy: 0.9146\n",
            "Epoch 141/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0375e-08 - accuracy: 1.0000 - val_loss: 0.9892 - val_accuracy: 0.9146\n",
            "Epoch 142/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.0324e-08 - accuracy: 1.0000 - val_loss: 0.9896 - val_accuracy: 0.9157\n",
            "Epoch 143/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0196e-08 - accuracy: 1.0000 - val_loss: 0.9899 - val_accuracy: 0.9146\n",
            "Epoch 144/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0140e-08 - accuracy: 1.0000 - val_loss: 0.9905 - val_accuracy: 0.9157\n",
            "Epoch 145/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.0061e-08 - accuracy: 1.0000 - val_loss: 0.9909 - val_accuracy: 0.9157\n",
            "Epoch 146/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 9.9295e-09 - accuracy: 1.0000 - val_loss: 0.9913 - val_accuracy: 0.9146\n",
            "Epoch 147/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.8069e-09 - accuracy: 1.0000 - val_loss: 0.9917 - val_accuracy: 0.9146\n",
            "Epoch 148/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 9.7241e-09 - accuracy: 1.0000 - val_loss: 0.9921 - val_accuracy: 0.9157\n",
            "Epoch 149/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.7210e-09 - accuracy: 1.0000 - val_loss: 0.9926 - val_accuracy: 0.9157\n",
            "Epoch 150/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.5582e-09 - accuracy: 1.0000 - val_loss: 0.9930 - val_accuracy: 0.9157\n",
            "Epoch 151/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.4385e-09 - accuracy: 1.0000 - val_loss: 0.9935 - val_accuracy: 0.9157\n",
            "Epoch 152/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.3581e-09 - accuracy: 1.0000 - val_loss: 0.9940 - val_accuracy: 0.9157\n",
            "Epoch 153/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 9.3207e-09 - accuracy: 1.0000 - val_loss: 0.9944 - val_accuracy: 0.9157\n",
            "Epoch 154/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.1951e-09 - accuracy: 1.0000 - val_loss: 0.9947 - val_accuracy: 0.9135\n",
            "Epoch 155/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.0714e-09 - accuracy: 1.0000 - val_loss: 0.9953 - val_accuracy: 0.9157\n",
            "Epoch 156/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 9.0295e-09 - accuracy: 1.0000 - val_loss: 0.9958 - val_accuracy: 0.9157\n",
            "Epoch 157/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.9220e-09 - accuracy: 1.0000 - val_loss: 0.9963 - val_accuracy: 0.9157\n",
            "Epoch 158/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.8090e-09 - accuracy: 1.0000 - val_loss: 0.9968 - val_accuracy: 0.9157\n",
            "Epoch 159/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.7547e-09 - accuracy: 1.0000 - val_loss: 0.9973 - val_accuracy: 0.9157\n",
            "Epoch 160/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.6753e-09 - accuracy: 1.0000 - val_loss: 0.9977 - val_accuracy: 0.9157\n",
            "Epoch 161/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.6499e-09 - accuracy: 1.0000 - val_loss: 0.9982 - val_accuracy: 0.9157\n",
            "Epoch 162/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.4817e-09 - accuracy: 1.0000 - val_loss: 0.9987 - val_accuracy: 0.9157\n",
            "Epoch 163/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.3822e-09 - accuracy: 1.0000 - val_loss: 0.9992 - val_accuracy: 0.9157\n",
            "Epoch 164/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.3525e-09 - accuracy: 1.0000 - val_loss: 0.9997 - val_accuracy: 0.9157\n",
            "Epoch 165/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.2054e-09 - accuracy: 1.0000 - val_loss: 1.0001 - val_accuracy: 0.9157\n",
            "Epoch 166/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.1242e-09 - accuracy: 1.0000 - val_loss: 1.0005 - val_accuracy: 0.9157\n",
            "Epoch 167/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.0738e-09 - accuracy: 1.0000 - val_loss: 1.0012 - val_accuracy: 0.9157\n",
            "Epoch 168/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.9918e-09 - accuracy: 1.0000 - val_loss: 1.0015 - val_accuracy: 0.9157\n",
            "Epoch 169/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 7.8790e-09 - accuracy: 1.0000 - val_loss: 1.0021 - val_accuracy: 0.9157\n",
            "Epoch 170/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.8328e-09 - accuracy: 1.0000 - val_loss: 1.0026 - val_accuracy: 0.9157\n",
            "Epoch 171/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 7.7759e-09 - accuracy: 1.0000 - val_loss: 1.0033 - val_accuracy: 0.9157\n",
            "Epoch 172/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.6743e-09 - accuracy: 1.0000 - val_loss: 1.0037 - val_accuracy: 0.9157\n",
            "Epoch 173/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.6028e-09 - accuracy: 1.0000 - val_loss: 1.0043 - val_accuracy: 0.9157\n",
            "Epoch 174/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.5208e-09 - accuracy: 1.0000 - val_loss: 1.0048 - val_accuracy: 0.9157\n",
            "Epoch 175/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.4508e-09 - accuracy: 1.0000 - val_loss: 1.0053 - val_accuracy: 0.9157\n",
            "Epoch 176/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 7.3314e-09 - accuracy: 1.0000 - val_loss: 1.0056 - val_accuracy: 0.9157\n",
            "Epoch 177/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.3039e-09 - accuracy: 1.0000 - val_loss: 1.0064 - val_accuracy: 0.9157\n",
            "Epoch 178/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.1816e-09 - accuracy: 1.0000 - val_loss: 1.0069 - val_accuracy: 0.9157\n",
            "Epoch 179/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.0928e-09 - accuracy: 1.0000 - val_loss: 1.0072 - val_accuracy: 0.9157\n",
            "Epoch 180/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.0291e-09 - accuracy: 1.0000 - val_loss: 1.0077 - val_accuracy: 0.9157\n",
            "Epoch 181/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 7.0172e-09 - accuracy: 1.0000 - val_loss: 1.0085 - val_accuracy: 0.9157\n",
            "Epoch 182/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.8844e-09 - accuracy: 1.0000 - val_loss: 1.0090 - val_accuracy: 0.9157\n",
            "Epoch 183/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.7994e-09 - accuracy: 1.0000 - val_loss: 1.0095 - val_accuracy: 0.9157\n",
            "Epoch 184/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.7702e-09 - accuracy: 1.0000 - val_loss: 1.0102 - val_accuracy: 0.9157\n",
            "Epoch 185/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.6639e-09 - accuracy: 1.0000 - val_loss: 1.0105 - val_accuracy: 0.9146\n",
            "Epoch 186/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.6441e-09 - accuracy: 1.0000 - val_loss: 1.0117 - val_accuracy: 0.9157\n",
            "Epoch 187/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.5137e-09 - accuracy: 1.0000 - val_loss: 1.0122 - val_accuracy: 0.9157\n",
            "Epoch 188/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.4528e-09 - accuracy: 1.0000 - val_loss: 1.0128 - val_accuracy: 0.9157\n",
            "Epoch 189/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.3832e-09 - accuracy: 1.0000 - val_loss: 1.0134 - val_accuracy: 0.9157\n",
            "Epoch 190/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.2450e-09 - accuracy: 1.0000 - val_loss: 1.0139 - val_accuracy: 0.9157\n",
            "Epoch 191/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 6.2468e-09 - accuracy: 1.0000 - val_loss: 1.0145 - val_accuracy: 0.9157\n",
            "Epoch 192/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.1541e-09 - accuracy: 1.0000 - val_loss: 1.0151 - val_accuracy: 0.9157\n",
            "Epoch 193/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 6.0381e-09 - accuracy: 1.0000 - val_loss: 1.0155 - val_accuracy: 0.9157\n",
            "Epoch 194/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.0591e-09 - accuracy: 1.0000 - val_loss: 1.0162 - val_accuracy: 0.9157\n",
            "Epoch 195/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.9398e-09 - accuracy: 1.0000 - val_loss: 1.0167 - val_accuracy: 0.9157\n",
            "Epoch 196/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.8691e-09 - accuracy: 1.0000 - val_loss: 1.0173 - val_accuracy: 0.9157\n",
            "Epoch 197/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 5.8388e-09 - accuracy: 1.0000 - val_loss: 1.0179 - val_accuracy: 0.9157\n",
            "Epoch 198/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.7256e-09 - accuracy: 1.0000 - val_loss: 1.0186 - val_accuracy: 0.9157\n",
            "Epoch 199/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 5.6661e-09 - accuracy: 1.0000 - val_loss: 1.0192 - val_accuracy: 0.9157\n",
            "Epoch 200/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 5.6532e-09 - accuracy: 1.0000 - val_loss: 1.0197 - val_accuracy: 0.9157\n",
            "36/36 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.9806e-09 - accuracy: 1.0000 - val_loss: 1.0201 - val_accuracy: 0.9157\n",
            "Epoch 2/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.9107e-09 - accuracy: 1.0000 - val_loss: 1.0204 - val_accuracy: 0.9157\n",
            "Epoch 3/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.8803e-09 - accuracy: 1.0000 - val_loss: 1.0207 - val_accuracy: 0.9157\n",
            "Epoch 4/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 2.7989e-09 - accuracy: 1.0000 - val_loss: 1.0210 - val_accuracy: 0.9146\n",
            "Epoch 5/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.8141e-09 - accuracy: 1.0000 - val_loss: 1.0212 - val_accuracy: 0.9157\n",
            "Epoch 6/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.7749e-09 - accuracy: 1.0000 - val_loss: 1.0216 - val_accuracy: 0.9146\n",
            "Epoch 7/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.7289e-09 - accuracy: 1.0000 - val_loss: 1.0218 - val_accuracy: 0.9157\n",
            "Epoch 8/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.7931e-09 - accuracy: 1.0000 - val_loss: 1.0221 - val_accuracy: 0.9157\n",
            "Epoch 9/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.7255e-09 - accuracy: 1.0000 - val_loss: 1.0225 - val_accuracy: 0.9146\n",
            "Epoch 10/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.6885e-09 - accuracy: 1.0000 - val_loss: 1.0227 - val_accuracy: 0.9146\n",
            "Epoch 11/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6266e-09 - accuracy: 1.0000 - val_loss: 1.0230 - val_accuracy: 0.9157\n",
            "Epoch 12/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6214e-09 - accuracy: 1.0000 - val_loss: 1.0233 - val_accuracy: 0.9157\n",
            "Epoch 13/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.5765e-09 - accuracy: 1.0000 - val_loss: 1.0236 - val_accuracy: 0.9157\n",
            "Epoch 14/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6080e-09 - accuracy: 1.0000 - val_loss: 1.0239 - val_accuracy: 0.9157\n",
            "Epoch 15/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.5531e-09 - accuracy: 1.0000 - val_loss: 1.0242 - val_accuracy: 0.9146\n",
            "Epoch 16/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.5462e-09 - accuracy: 1.0000 - val_loss: 1.0245 - val_accuracy: 0.9146\n",
            "Epoch 17/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.5656e-09 - accuracy: 1.0000 - val_loss: 1.0247 - val_accuracy: 0.9157\n",
            "Epoch 18/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.4891e-09 - accuracy: 1.0000 - val_loss: 1.0250 - val_accuracy: 0.9157\n",
            "Epoch 19/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.5909e-09 - accuracy: 1.0000 - val_loss: 1.0254 - val_accuracy: 0.9146\n",
            "Epoch 20/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.4058e-09 - accuracy: 1.0000 - val_loss: 1.0258 - val_accuracy: 0.9146\n",
            "Epoch 21/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.4210e-09 - accuracy: 1.0000 - val_loss: 1.0260 - val_accuracy: 0.9157\n",
            "Epoch 22/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.3956e-09 - accuracy: 1.0000 - val_loss: 1.0263 - val_accuracy: 0.9157\n",
            "Epoch 23/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.3519e-09 - accuracy: 1.0000 - val_loss: 1.0267 - val_accuracy: 0.9157\n",
            "Epoch 24/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.3265e-09 - accuracy: 1.0000 - val_loss: 1.0269 - val_accuracy: 0.9157\n",
            "Epoch 25/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.3441e-09 - accuracy: 1.0000 - val_loss: 1.0272 - val_accuracy: 0.9157\n",
            "Epoch 26/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.3314e-09 - accuracy: 1.0000 - val_loss: 1.0276 - val_accuracy: 0.9157\n",
            "Epoch 27/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.2902e-09 - accuracy: 1.0000 - val_loss: 1.0279 - val_accuracy: 0.9157\n",
            "Epoch 28/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.2408e-09 - accuracy: 1.0000 - val_loss: 1.0282 - val_accuracy: 0.9157\n",
            "Epoch 29/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.2810e-09 - accuracy: 1.0000 - val_loss: 1.0285 - val_accuracy: 0.9157\n",
            "Epoch 30/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.2557e-09 - accuracy: 1.0000 - val_loss: 1.0289 - val_accuracy: 0.9146\n",
            "Epoch 31/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.1855e-09 - accuracy: 1.0000 - val_loss: 1.0291 - val_accuracy: 0.9157\n",
            "Epoch 32/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.1774e-09 - accuracy: 1.0000 - val_loss: 1.0294 - val_accuracy: 0.9157\n",
            "Epoch 33/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.1517e-09 - accuracy: 1.0000 - val_loss: 1.0298 - val_accuracy: 0.9157\n",
            "Epoch 34/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.1538e-09 - accuracy: 1.0000 - val_loss: 1.0301 - val_accuracy: 0.9157\n",
            "Epoch 35/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.1273e-09 - accuracy: 1.0000 - val_loss: 1.0304 - val_accuracy: 0.9157\n",
            "Epoch 36/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 2.1093e-09 - accuracy: 1.0000 - val_loss: 1.0307 - val_accuracy: 0.9157\n",
            "Epoch 37/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.0694e-09 - accuracy: 1.0000 - val_loss: 1.0311 - val_accuracy: 0.9157\n",
            "Epoch 38/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1369e-09 - accuracy: 1.0000 - val_loss: 1.0314 - val_accuracy: 0.9157\n",
            "Epoch 39/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.0943e-09 - accuracy: 1.0000 - val_loss: 1.0317 - val_accuracy: 0.9157\n",
            "Epoch 40/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.0077e-09 - accuracy: 1.0000 - val_loss: 1.0321 - val_accuracy: 0.9157\n",
            "Epoch 41/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.0742e-09 - accuracy: 1.0000 - val_loss: 1.0325 - val_accuracy: 0.9157\n",
            "Epoch 42/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.0310e-09 - accuracy: 1.0000 - val_loss: 1.0327 - val_accuracy: 0.9157\n",
            "Epoch 43/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.0005e-09 - accuracy: 1.0000 - val_loss: 1.0330 - val_accuracy: 0.9157\n",
            "Epoch 44/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.9736e-09 - accuracy: 1.0000 - val_loss: 1.0334 - val_accuracy: 0.9157\n",
            "Epoch 45/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.9998e-09 - accuracy: 1.0000 - val_loss: 1.0338 - val_accuracy: 0.9157\n",
            "Epoch 46/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.9090e-09 - accuracy: 1.0000 - val_loss: 1.0341 - val_accuracy: 0.9157\n",
            "Epoch 47/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.9578e-09 - accuracy: 1.0000 - val_loss: 1.0344 - val_accuracy: 0.9157\n",
            "Epoch 48/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.9477e-09 - accuracy: 1.0000 - val_loss: 1.0348 - val_accuracy: 0.9157\n",
            "Epoch 49/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.9004e-09 - accuracy: 1.0000 - val_loss: 1.0351 - val_accuracy: 0.9157\n",
            "Epoch 50/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.8839e-09 - accuracy: 1.0000 - val_loss: 1.0355 - val_accuracy: 0.9157\n",
            "Epoch 51/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.8644e-09 - accuracy: 1.0000 - val_loss: 1.0358 - val_accuracy: 0.9157\n",
            "Epoch 52/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.8451e-09 - accuracy: 1.0000 - val_loss: 1.0361 - val_accuracy: 0.9157\n",
            "Epoch 53/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.8643e-09 - accuracy: 1.0000 - val_loss: 1.0365 - val_accuracy: 0.9157\n",
            "Epoch 54/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.8848e-09 - accuracy: 1.0000 - val_loss: 1.0368 - val_accuracy: 0.9157\n",
            "Epoch 55/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.8067e-09 - accuracy: 1.0000 - val_loss: 1.0372 - val_accuracy: 0.9157\n",
            "Epoch 56/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.8066e-09 - accuracy: 1.0000 - val_loss: 1.0375 - val_accuracy: 0.9157\n",
            "Epoch 57/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.8518e-09 - accuracy: 1.0000 - val_loss: 1.0379 - val_accuracy: 0.9157\n",
            "Epoch 58/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.7601e-09 - accuracy: 1.0000 - val_loss: 1.0383 - val_accuracy: 0.9157\n",
            "Epoch 59/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.7910e-09 - accuracy: 1.0000 - val_loss: 1.0386 - val_accuracy: 0.9157\n",
            "Epoch 60/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.8308e-09 - accuracy: 1.0000 - val_loss: 1.0390 - val_accuracy: 0.9157\n",
            "Epoch 61/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.7152e-09 - accuracy: 1.0000 - val_loss: 1.0393 - val_accuracy: 0.9157\n",
            "Epoch 62/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.7155e-09 - accuracy: 1.0000 - val_loss: 1.0397 - val_accuracy: 0.9157\n",
            "Epoch 63/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.7094e-09 - accuracy: 1.0000 - val_loss: 1.0401 - val_accuracy: 0.9157\n",
            "Epoch 64/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.7055e-09 - accuracy: 1.0000 - val_loss: 1.0404 - val_accuracy: 0.9157\n",
            "Epoch 65/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.6506e-09 - accuracy: 1.0000 - val_loss: 1.0411 - val_accuracy: 0.9146\n",
            "Epoch 66/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.6917e-09 - accuracy: 1.0000 - val_loss: 1.0411 - val_accuracy: 0.9157\n",
            "Epoch 67/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.6558e-09 - accuracy: 1.0000 - val_loss: 1.0416 - val_accuracy: 0.9157\n",
            "Epoch 68/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.6018e-09 - accuracy: 1.0000 - val_loss: 1.0419 - val_accuracy: 0.9157\n",
            "Epoch 69/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.7095e-09 - accuracy: 1.0000 - val_loss: 1.0423 - val_accuracy: 0.9157\n",
            "Epoch 70/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.6601e-09 - accuracy: 1.0000 - val_loss: 1.0427 - val_accuracy: 0.9157\n",
            "Epoch 71/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.5719e-09 - accuracy: 1.0000 - val_loss: 1.0436 - val_accuracy: 0.9146\n",
            "Epoch 72/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.6107e-09 - accuracy: 1.0000 - val_loss: 1.0433 - val_accuracy: 0.9157\n",
            "Epoch 73/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.6029e-09 - accuracy: 1.0000 - val_loss: 1.0437 - val_accuracy: 0.9157\n",
            "Epoch 74/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.5896e-09 - accuracy: 1.0000 - val_loss: 1.0441 - val_accuracy: 0.9157\n",
            "Epoch 75/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.5445e-09 - accuracy: 1.0000 - val_loss: 1.0445 - val_accuracy: 0.9157\n",
            "Epoch 76/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.5401e-09 - accuracy: 1.0000 - val_loss: 1.0449 - val_accuracy: 0.9157\n",
            "Epoch 77/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.5021e-09 - accuracy: 1.0000 - val_loss: 1.0453 - val_accuracy: 0.9157\n",
            "Epoch 78/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.5211e-09 - accuracy: 1.0000 - val_loss: 1.0458 - val_accuracy: 0.9146\n",
            "Epoch 79/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.5070e-09 - accuracy: 1.0000 - val_loss: 1.0461 - val_accuracy: 0.9157\n",
            "Epoch 80/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.5101e-09 - accuracy: 1.0000 - val_loss: 1.0465 - val_accuracy: 0.9157\n",
            "Epoch 81/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.4540e-09 - accuracy: 1.0000 - val_loss: 1.0469 - val_accuracy: 0.9157\n",
            "Epoch 82/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.4584e-09 - accuracy: 1.0000 - val_loss: 1.0474 - val_accuracy: 0.9146\n",
            "Epoch 83/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.4479e-09 - accuracy: 1.0000 - val_loss: 1.0478 - val_accuracy: 0.9146\n",
            "Epoch 84/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.4436e-09 - accuracy: 1.0000 - val_loss: 1.0481 - val_accuracy: 0.9157\n",
            "Epoch 85/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.4123e-09 - accuracy: 1.0000 - val_loss: 1.0488 - val_accuracy: 0.9146\n",
            "Epoch 86/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.4305e-09 - accuracy: 1.0000 - val_loss: 1.0488 - val_accuracy: 0.9157\n",
            "Epoch 87/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.3702e-09 - accuracy: 1.0000 - val_loss: 1.0493 - val_accuracy: 0.9146\n",
            "Epoch 88/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.4518e-09 - accuracy: 1.0000 - val_loss: 1.0496 - val_accuracy: 0.9157\n",
            "Epoch 89/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.3537e-09 - accuracy: 1.0000 - val_loss: 1.0500 - val_accuracy: 0.9157\n",
            "Epoch 90/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.3818e-09 - accuracy: 1.0000 - val_loss: 1.0509 - val_accuracy: 0.9146\n",
            "Epoch 91/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.3981e-09 - accuracy: 1.0000 - val_loss: 1.0508 - val_accuracy: 0.9157\n",
            "Epoch 92/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.3253e-09 - accuracy: 1.0000 - val_loss: 1.0513 - val_accuracy: 0.9146\n",
            "Epoch 93/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.3536e-09 - accuracy: 1.0000 - val_loss: 1.0517 - val_accuracy: 0.9146\n",
            "Epoch 94/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.3259e-09 - accuracy: 1.0000 - val_loss: 1.0522 - val_accuracy: 0.9146\n",
            "Epoch 95/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.3024e-09 - accuracy: 1.0000 - val_loss: 1.0529 - val_accuracy: 0.9146\n",
            "Epoch 96/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.3178e-09 - accuracy: 1.0000 - val_loss: 1.0532 - val_accuracy: 0.9146\n",
            "Epoch 97/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.3415e-09 - accuracy: 1.0000 - val_loss: 1.0532 - val_accuracy: 0.9157\n",
            "Epoch 98/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.2558e-09 - accuracy: 1.0000 - val_loss: 1.0537 - val_accuracy: 0.9146\n",
            "Epoch 99/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.3123e-09 - accuracy: 1.0000 - val_loss: 1.0540 - val_accuracy: 0.9157\n",
            "Epoch 100/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.3072e-09 - accuracy: 1.0000 - val_loss: 1.0544 - val_accuracy: 0.9157\n",
            "Epoch 101/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.2257e-09 - accuracy: 1.0000 - val_loss: 1.0553 - val_accuracy: 0.9146\n",
            "Epoch 102/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.2294e-09 - accuracy: 1.0000 - val_loss: 1.0560 - val_accuracy: 0.9146\n",
            "Epoch 103/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.2547e-09 - accuracy: 1.0000 - val_loss: 1.0564 - val_accuracy: 0.9146\n",
            "Epoch 104/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.3279e-09 - accuracy: 1.0000 - val_loss: 1.0561 - val_accuracy: 0.9157\n",
            "Epoch 105/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1734e-09 - accuracy: 1.0000 - val_loss: 1.0571 - val_accuracy: 0.9146\n",
            "Epoch 106/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.2403e-09 - accuracy: 1.0000 - val_loss: 1.0573 - val_accuracy: 0.9146\n",
            "Epoch 107/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.2580e-09 - accuracy: 1.0000 - val_loss: 1.0576 - val_accuracy: 0.9146\n",
            "Epoch 108/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1903e-09 - accuracy: 1.0000 - val_loss: 1.0585 - val_accuracy: 0.9146\n",
            "Epoch 109/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.2083e-09 - accuracy: 1.0000 - val_loss: 1.0590 - val_accuracy: 0.9146\n",
            "Epoch 110/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.2016e-09 - accuracy: 1.0000 - val_loss: 1.0594 - val_accuracy: 0.9146\n",
            "Epoch 111/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1882e-09 - accuracy: 1.0000 - val_loss: 1.0592 - val_accuracy: 0.9146\n",
            "Epoch 112/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.1406e-09 - accuracy: 1.0000 - val_loss: 1.0595 - val_accuracy: 0.9146\n",
            "Epoch 113/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1493e-09 - accuracy: 1.0000 - val_loss: 1.0604 - val_accuracy: 0.9146\n",
            "Epoch 114/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1330e-09 - accuracy: 1.0000 - val_loss: 1.0616 - val_accuracy: 0.9146\n",
            "Epoch 115/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.2357e-09 - accuracy: 1.0000 - val_loss: 1.0611 - val_accuracy: 0.9146\n",
            "Epoch 116/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1913e-09 - accuracy: 1.0000 - val_loss: 1.0613 - val_accuracy: 0.9146\n",
            "Epoch 117/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1755e-09 - accuracy: 1.0000 - val_loss: 1.0619 - val_accuracy: 0.9146\n",
            "Epoch 118/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0877e-09 - accuracy: 1.0000 - val_loss: 1.0619 - val_accuracy: 0.9157\n",
            "Epoch 119/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0943e-09 - accuracy: 1.0000 - val_loss: 1.0630 - val_accuracy: 0.9146\n",
            "Epoch 120/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.1362e-09 - accuracy: 1.0000 - val_loss: 1.0639 - val_accuracy: 0.9146\n",
            "Epoch 121/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.1146e-09 - accuracy: 1.0000 - val_loss: 1.0639 - val_accuracy: 0.9146\n",
            "Epoch 122/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1006e-09 - accuracy: 1.0000 - val_loss: 1.0641 - val_accuracy: 0.9146\n",
            "Epoch 123/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0492e-09 - accuracy: 1.0000 - val_loss: 1.0648 - val_accuracy: 0.9146\n",
            "Epoch 124/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.1336e-09 - accuracy: 1.0000 - val_loss: 1.0643 - val_accuracy: 0.9157\n",
            "Epoch 125/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.1070e-09 - accuracy: 1.0000 - val_loss: 1.0649 - val_accuracy: 0.9146\n",
            "Epoch 126/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0225e-09 - accuracy: 1.0000 - val_loss: 1.0675 - val_accuracy: 0.9157\n",
            "Epoch 127/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1136e-09 - accuracy: 1.0000 - val_loss: 1.0656 - val_accuracy: 0.9146\n",
            "Epoch 128/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0773e-09 - accuracy: 1.0000 - val_loss: 1.0658 - val_accuracy: 0.9146\n",
            "Epoch 129/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0625e-09 - accuracy: 1.0000 - val_loss: 1.0663 - val_accuracy: 0.9146\n",
            "Epoch 130/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0189e-09 - accuracy: 1.0000 - val_loss: 1.0692 - val_accuracy: 0.9157\n",
            "Epoch 131/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0823e-09 - accuracy: 1.0000 - val_loss: 1.0677 - val_accuracy: 0.9146\n",
            "Epoch 132/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.0210e-09 - accuracy: 1.0000 - val_loss: 1.0679 - val_accuracy: 0.9146\n",
            "Epoch 133/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0493e-09 - accuracy: 1.0000 - val_loss: 1.0682 - val_accuracy: 0.9146\n",
            "Epoch 134/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.9515e-10 - accuracy: 1.0000 - val_loss: 1.0681 - val_accuracy: 0.9157\n",
            "Epoch 135/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.0625e-09 - accuracy: 1.0000 - val_loss: 1.0689 - val_accuracy: 0.9146\n",
            "Epoch 136/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.6990e-10 - accuracy: 1.0000 - val_loss: 1.0696 - val_accuracy: 0.9146\n",
            "Epoch 137/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0085e-09 - accuracy: 1.0000 - val_loss: 1.0692 - val_accuracy: 0.9157\n",
            "Epoch 138/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.7471e-10 - accuracy: 1.0000 - val_loss: 1.0700 - val_accuracy: 0.9146\n",
            "Epoch 139/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.4668e-10 - accuracy: 1.0000 - val_loss: 1.0708 - val_accuracy: 0.9146\n",
            "Epoch 140/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.9525e-10 - accuracy: 1.0000 - val_loss: 1.0704 - val_accuracy: 0.9146\n",
            "Epoch 141/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.7467e-10 - accuracy: 1.0000 - val_loss: 1.0711 - val_accuracy: 0.9146\n",
            "Epoch 142/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0062e-09 - accuracy: 1.0000 - val_loss: 1.0720 - val_accuracy: 0.9146\n",
            "Epoch 143/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.3120e-10 - accuracy: 1.0000 - val_loss: 1.0724 - val_accuracy: 0.9146\n",
            "Epoch 144/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 9.3908e-10 - accuracy: 1.0000 - val_loss: 1.0730 - val_accuracy: 0.9146\n",
            "Epoch 145/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.2225e-10 - accuracy: 1.0000 - val_loss: 1.0732 - val_accuracy: 0.9146\n",
            "Epoch 146/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.4234e-10 - accuracy: 1.0000 - val_loss: 1.0726 - val_accuracy: 0.9146\n",
            "Epoch 147/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 9.1442e-10 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.9146\n",
            "Epoch 148/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 9.7638e-10 - accuracy: 1.0000 - val_loss: 1.0735 - val_accuracy: 0.9146\n",
            "Epoch 149/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.2257e-10 - accuracy: 1.0000 - val_loss: 1.0748 - val_accuracy: 0.9146\n",
            "Epoch 150/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.2980e-10 - accuracy: 1.0000 - val_loss: 1.0750 - val_accuracy: 0.9146\n",
            "Epoch 151/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.2716e-10 - accuracy: 1.0000 - val_loss: 1.0746 - val_accuracy: 0.9146\n",
            "Epoch 152/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.5727e-10 - accuracy: 1.0000 - val_loss: 1.0761 - val_accuracy: 0.9146\n",
            "Epoch 153/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 9.2090e-10 - accuracy: 1.0000 - val_loss: 1.0756 - val_accuracy: 0.9146\n",
            "Epoch 154/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.3022e-10 - accuracy: 1.0000 - val_loss: 1.0771 - val_accuracy: 0.9146\n",
            "Epoch 155/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 9.5872e-10 - accuracy: 1.0000 - val_loss: 1.0763 - val_accuracy: 0.9146\n",
            "Epoch 156/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.8374e-10 - accuracy: 1.0000 - val_loss: 1.0769 - val_accuracy: 0.9146\n",
            "Epoch 157/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.3101e-10 - accuracy: 1.0000 - val_loss: 1.0788 - val_accuracy: 0.9146\n",
            "Epoch 158/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.6570e-10 - accuracy: 1.0000 - val_loss: 1.0787 - val_accuracy: 0.9146\n",
            "Epoch 159/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 9.5355e-10 - accuracy: 1.0000 - val_loss: 1.0809 - val_accuracy: 0.9168\n",
            "Epoch 160/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.9309e-10 - accuracy: 1.0000 - val_loss: 1.0816 - val_accuracy: 0.9157\n",
            "Epoch 161/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.7559e-10 - accuracy: 1.0000 - val_loss: 1.0793 - val_accuracy: 0.9146\n",
            "Epoch 162/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.4979e-10 - accuracy: 1.0000 - val_loss: 1.0790 - val_accuracy: 0.9146\n",
            "Epoch 163/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.5315e-10 - accuracy: 1.0000 - val_loss: 1.0825 - val_accuracy: 0.9168\n",
            "Epoch 164/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.9542e-10 - accuracy: 1.0000 - val_loss: 1.0814 - val_accuracy: 0.9146\n",
            "Epoch 165/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.1112e-10 - accuracy: 1.0000 - val_loss: 1.0801 - val_accuracy: 0.9146\n",
            "Epoch 166/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.2421e-10 - accuracy: 1.0000 - val_loss: 1.0806 - val_accuracy: 0.9146\n",
            "Epoch 167/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.2323e-10 - accuracy: 1.0000 - val_loss: 1.0820 - val_accuracy: 0.9146\n",
            "Epoch 168/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.5507e-10 - accuracy: 1.0000 - val_loss: 1.0815 - val_accuracy: 0.9146\n",
            "Epoch 169/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.8575e-10 - accuracy: 1.0000 - val_loss: 1.0817 - val_accuracy: 0.9146\n",
            "Epoch 170/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.0560e-10 - accuracy: 1.0000 - val_loss: 1.0821 - val_accuracy: 0.9146\n",
            "Epoch 171/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.3800e-10 - accuracy: 1.0000 - val_loss: 1.0821 - val_accuracy: 0.9146\n",
            "Epoch 172/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.2677e-10 - accuracy: 1.0000 - val_loss: 1.0829 - val_accuracy: 0.9146\n",
            "Epoch 173/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 7.8590e-10 - accuracy: 1.0000 - val_loss: 1.0846 - val_accuracy: 0.9146\n",
            "Epoch 174/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.6434e-10 - accuracy: 1.0000 - val_loss: 1.0868 - val_accuracy: 0.9146\n",
            "Epoch 175/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.9310e-10 - accuracy: 1.0000 - val_loss: 1.0865 - val_accuracy: 0.9168\n",
            "Epoch 176/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.8971e-10 - accuracy: 1.0000 - val_loss: 1.0842 - val_accuracy: 0.9146\n",
            "Epoch 177/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.4813e-10 - accuracy: 1.0000 - val_loss: 1.0843 - val_accuracy: 0.9146\n",
            "Epoch 178/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.4363e-10 - accuracy: 1.0000 - val_loss: 1.0842 - val_accuracy: 0.9146\n",
            "Epoch 179/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.5743e-10 - accuracy: 1.0000 - val_loss: 1.0844 - val_accuracy: 0.9135\n",
            "Epoch 180/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.7622e-10 - accuracy: 1.0000 - val_loss: 1.0851 - val_accuracy: 0.9146\n",
            "Epoch 181/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.1036e-10 - accuracy: 1.0000 - val_loss: 1.0852 - val_accuracy: 0.9146\n",
            "Epoch 182/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.9429e-10 - accuracy: 1.0000 - val_loss: 1.0866 - val_accuracy: 0.9146\n",
            "Epoch 183/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 7.7305e-10 - accuracy: 1.0000 - val_loss: 1.0880 - val_accuracy: 0.9146\n",
            "Epoch 184/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.8517e-10 - accuracy: 1.0000 - val_loss: 1.0892 - val_accuracy: 0.9168\n",
            "Epoch 185/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.8087e-10 - accuracy: 1.0000 - val_loss: 1.0878 - val_accuracy: 0.9146\n",
            "Epoch 186/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.1157e-10 - accuracy: 1.0000 - val_loss: 1.0873 - val_accuracy: 0.9146\n",
            "Epoch 187/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.4871e-10 - accuracy: 1.0000 - val_loss: 1.0910 - val_accuracy: 0.9146\n",
            "Epoch 188/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.6337e-10 - accuracy: 1.0000 - val_loss: 1.0887 - val_accuracy: 0.9146\n",
            "Epoch 189/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.9679e-10 - accuracy: 1.0000 - val_loss: 1.0879 - val_accuracy: 0.9146\n",
            "Epoch 190/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.5317e-10 - accuracy: 1.0000 - val_loss: 1.0898 - val_accuracy: 0.9146\n",
            "Epoch 191/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.6717e-10 - accuracy: 1.0000 - val_loss: 1.0882 - val_accuracy: 0.9124\n",
            "Epoch 192/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 7.6324e-10 - accuracy: 1.0000 - val_loss: 1.0886 - val_accuracy: 0.9135\n",
            "Epoch 193/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 7.3919e-10 - accuracy: 1.0000 - val_loss: 1.0905 - val_accuracy: 0.9146\n",
            "Epoch 194/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.3114e-10 - accuracy: 1.0000 - val_loss: 1.0905 - val_accuracy: 0.9146\n",
            "Epoch 195/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.2276e-10 - accuracy: 1.0000 - val_loss: 1.0895 - val_accuracy: 0.9135\n",
            "Epoch 196/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.9769e-10 - accuracy: 1.0000 - val_loss: 1.0918 - val_accuracy: 0.9146\n",
            "Epoch 197/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.1322e-10 - accuracy: 1.0000 - val_loss: 1.0901 - val_accuracy: 0.9146\n",
            "Epoch 198/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.9395e-10 - accuracy: 1.0000 - val_loss: 1.0977 - val_accuracy: 0.9146\n",
            "Epoch 199/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.1370e-10 - accuracy: 1.0000 - val_loss: 1.0908 - val_accuracy: 0.9135\n",
            "Epoch 200/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.2686e-09 - accuracy: 1.0000 - val_loss: 1.0911 - val_accuracy: 0.9135\n",
            "36/36 [==============================] - 0s 3ms/step\n",
            "Epoch 1/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.5247e-09 - accuracy: 1.0000 - val_loss: 9.0363e-09 - val_accuracy: 1.0000\n",
            "Epoch 2/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.7295e-10 - accuracy: 1.0000 - val_loss: 8.5042e-09 - val_accuracy: 1.0000\n",
            "Epoch 3/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.5180e-10 - accuracy: 1.0000 - val_loss: 8.5018e-09 - val_accuracy: 1.0000\n",
            "Epoch 4/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.1434e-10 - accuracy: 1.0000 - val_loss: 8.4541e-09 - val_accuracy: 1.0000\n",
            "Epoch 5/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.8765e-10 - accuracy: 1.0000 - val_loss: 8.4330e-09 - val_accuracy: 1.0000\n",
            "Epoch 6/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.7977e-10 - accuracy: 1.0000 - val_loss: 8.3853e-09 - val_accuracy: 1.0000\n",
            "Epoch 7/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.3129e-10 - accuracy: 1.0000 - val_loss: 8.3507e-09 - val_accuracy: 1.0000\n",
            "Epoch 8/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.8386e-10 - accuracy: 1.0000 - val_loss: 8.3566e-09 - val_accuracy: 1.0000\n",
            "Epoch 9/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 6.1884e-10 - accuracy: 1.0000 - val_loss: 8.3814e-09 - val_accuracy: 1.0000\n",
            "Epoch 10/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 6.3287e-10 - accuracy: 1.0000 - val_loss: 8.3112e-09 - val_accuracy: 1.0000\n",
            "Epoch 11/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.7264e-10 - accuracy: 1.0000 - val_loss: 8.2637e-09 - val_accuracy: 1.0000\n",
            "Epoch 12/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 7.0837e-10 - accuracy: 1.0000 - val_loss: 8.4857e-09 - val_accuracy: 1.0000\n",
            "Epoch 13/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.4039e-10 - accuracy: 1.0000 - val_loss: 8.2627e-09 - val_accuracy: 1.0000\n",
            "Epoch 14/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.7479e-10 - accuracy: 1.0000 - val_loss: 8.4916e-09 - val_accuracy: 1.0000\n",
            "Epoch 15/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.7229e-10 - accuracy: 1.0000 - val_loss: 8.1839e-09 - val_accuracy: 1.0000\n",
            "Epoch 16/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.0437e-10 - accuracy: 1.0000 - val_loss: 8.1656e-09 - val_accuracy: 1.0000\n",
            "Epoch 17/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.8887e-10 - accuracy: 1.0000 - val_loss: 8.4367e-09 - val_accuracy: 1.0000\n",
            "Epoch 18/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.7674e-10 - accuracy: 1.0000 - val_loss: 8.1344e-09 - val_accuracy: 1.0000\n",
            "Epoch 19/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.2191e-10 - accuracy: 1.0000 - val_loss: 8.3627e-09 - val_accuracy: 1.0000\n",
            "Epoch 20/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.5400e-10 - accuracy: 1.0000 - val_loss: 8.0862e-09 - val_accuracy: 1.0000\n",
            "Epoch 21/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.1478e-10 - accuracy: 1.0000 - val_loss: 8.0763e-09 - val_accuracy: 1.0000\n",
            "Epoch 22/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.4378e-10 - accuracy: 1.0000 - val_loss: 8.0442e-09 - val_accuracy: 1.0000\n",
            "Epoch 23/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.7535e-10 - accuracy: 1.0000 - val_loss: 8.1913e-09 - val_accuracy: 1.0000\n",
            "Epoch 24/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.8530e-10 - accuracy: 1.0000 - val_loss: 8.0062e-09 - val_accuracy: 1.0000\n",
            "Epoch 25/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.1847e-10 - accuracy: 1.0000 - val_loss: 7.9702e-09 - val_accuracy: 1.0000\n",
            "Epoch 26/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 6.3562e-10 - accuracy: 1.0000 - val_loss: 8.1391e-09 - val_accuracy: 1.0000\n",
            "Epoch 27/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.1064e-10 - accuracy: 1.0000 - val_loss: 8.0280e-09 - val_accuracy: 1.0000\n",
            "Epoch 28/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.5416e-10 - accuracy: 1.0000 - val_loss: 7.9343e-09 - val_accuracy: 1.0000\n",
            "Epoch 29/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.2031e-10 - accuracy: 1.0000 - val_loss: 7.9913e-09 - val_accuracy: 1.0000\n",
            "Epoch 30/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.8023e-09 - accuracy: 1.0000 - val_loss: 8.6451e-09 - val_accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 7.3216e-10 - accuracy: 1.0000 - val_loss: 7.8460e-09 - val_accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.7344e-10 - accuracy: 1.0000 - val_loss: 7.7790e-09 - val_accuracy: 1.0000\n",
            "Epoch 33/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 7.5752e-10 - accuracy: 1.0000 - val_loss: 8.1705e-09 - val_accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.9212e-10 - accuracy: 1.0000 - val_loss: 8.1675e-09 - val_accuracy: 1.0000\n",
            "Epoch 35/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.6039e-10 - accuracy: 1.0000 - val_loss: 7.8019e-09 - val_accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.5965e-10 - accuracy: 1.0000 - val_loss: 7.7212e-09 - val_accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.8779e-10 - accuracy: 1.0000 - val_loss: 7.7931e-09 - val_accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.2949e-10 - accuracy: 1.0000 - val_loss: 7.6651e-09 - val_accuracy: 1.0000\n",
            "Epoch 39/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.9766e-10 - accuracy: 1.0000 - val_loss: 8.2148e-09 - val_accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 5.7936e-10 - accuracy: 1.0000 - val_loss: 7.6327e-09 - val_accuracy: 1.0000\n",
            "Epoch 41/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.0052e-10 - accuracy: 1.0000 - val_loss: 7.6020e-09 - val_accuracy: 1.0000\n",
            "Epoch 42/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 5.4602e-10 - accuracy: 1.0000 - val_loss: 7.6111e-09 - val_accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.4239e-10 - accuracy: 1.0000 - val_loss: 7.7606e-09 - val_accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 7.3475e-10 - accuracy: 1.0000 - val_loss: 9.2954e-09 - val_accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.2872 - accuracy: 0.9220 - val_loss: 3.0821 - val_accuracy: 0.7746\n",
            "Epoch 46/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 0.3497 - accuracy: 0.9598 - val_loss: 1.1675e-04 - val_accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 3.2565e-05 - accuracy: 1.0000 - val_loss: 7.4945e-05 - val_accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 4.1441e-05 - accuracy: 1.0000 - val_loss: 7.3009e-05 - val_accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.4891e-05 - accuracy: 1.0000 - val_loss: 6.8738e-05 - val_accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 3.0956e-05 - accuracy: 1.0000 - val_loss: 6.5546e-05 - val_accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.8297e-05 - accuracy: 1.0000 - val_loss: 6.2960e-05 - val_accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.6331e-05 - accuracy: 1.0000 - val_loss: 6.0878e-05 - val_accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.4743e-05 - accuracy: 1.0000 - val_loss: 5.9204e-05 - val_accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.3516e-05 - accuracy: 1.0000 - val_loss: 5.7625e-05 - val_accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2.2442e-05 - accuracy: 1.0000 - val_loss: 5.6349e-05 - val_accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.1496e-05 - accuracy: 1.0000 - val_loss: 5.5283e-05 - val_accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 2.0725e-05 - accuracy: 1.0000 - val_loss: 5.4239e-05 - val_accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2.0019e-05 - accuracy: 1.0000 - val_loss: 5.3399e-05 - val_accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.9400e-05 - accuracy: 1.0000 - val_loss: 5.2648e-05 - val_accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.8854e-05 - accuracy: 1.0000 - val_loss: 5.2028e-05 - val_accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.8352e-05 - accuracy: 1.0000 - val_loss: 5.1386e-05 - val_accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.7881e-05 - accuracy: 1.0000 - val_loss: 5.0788e-05 - val_accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.7465e-05 - accuracy: 1.0000 - val_loss: 5.0383e-05 - val_accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.7084e-05 - accuracy: 1.0000 - val_loss: 4.9995e-05 - val_accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.6737e-05 - accuracy: 1.0000 - val_loss: 4.9550e-05 - val_accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.6400e-05 - accuracy: 1.0000 - val_loss: 4.9285e-05 - val_accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.6103e-05 - accuracy: 1.0000 - val_loss: 4.8989e-05 - val_accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.5829e-05 - accuracy: 1.0000 - val_loss: 4.8668e-05 - val_accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.5555e-05 - accuracy: 1.0000 - val_loss: 4.8443e-05 - val_accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.5316e-05 - accuracy: 1.0000 - val_loss: 4.8215e-05 - val_accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.5084e-05 - accuracy: 1.0000 - val_loss: 4.8088e-05 - val_accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.4867e-05 - accuracy: 1.0000 - val_loss: 4.7863e-05 - val_accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.4660e-05 - accuracy: 1.0000 - val_loss: 4.7775e-05 - val_accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.4470e-05 - accuracy: 1.0000 - val_loss: 4.7656e-05 - val_accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.4291e-05 - accuracy: 1.0000 - val_loss: 4.7540e-05 - val_accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.4119e-05 - accuracy: 1.0000 - val_loss: 4.7491e-05 - val_accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.3963e-05 - accuracy: 1.0000 - val_loss: 4.7429e-05 - val_accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.3807e-05 - accuracy: 1.0000 - val_loss: 4.7392e-05 - val_accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.3662e-05 - accuracy: 1.0000 - val_loss: 4.7317e-05 - val_accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.3527e-05 - accuracy: 1.0000 - val_loss: 4.7335e-05 - val_accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.3394e-05 - accuracy: 1.0000 - val_loss: 4.7288e-05 - val_accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.3264e-05 - accuracy: 1.0000 - val_loss: 4.7312e-05 - val_accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.3143e-05 - accuracy: 1.0000 - val_loss: 4.7279e-05 - val_accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.3022e-05 - accuracy: 1.0000 - val_loss: 4.7294e-05 - val_accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.2908e-05 - accuracy: 1.0000 - val_loss: 4.7295e-05 - val_accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.2801e-05 - accuracy: 1.0000 - val_loss: 4.7379e-05 - val_accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.2696e-05 - accuracy: 1.0000 - val_loss: 4.7340e-05 - val_accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 1.2594e-05 - accuracy: 1.0000 - val_loss: 4.7413e-05 - val_accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.2497e-05 - accuracy: 1.0000 - val_loss: 4.7451e-05 - val_accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.2399e-05 - accuracy: 1.0000 - val_loss: 4.7428e-05 - val_accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.2305e-05 - accuracy: 1.0000 - val_loss: 4.7574e-05 - val_accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.2217e-05 - accuracy: 1.0000 - val_loss: 4.7601e-05 - val_accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.2129e-05 - accuracy: 1.0000 - val_loss: 4.7649e-05 - val_accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.2043e-05 - accuracy: 1.0000 - val_loss: 4.7668e-05 - val_accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1957e-05 - accuracy: 1.0000 - val_loss: 4.7724e-05 - val_accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.1875e-05 - accuracy: 1.0000 - val_loss: 4.7763e-05 - val_accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.1800e-05 - accuracy: 1.0000 - val_loss: 4.7823e-05 - val_accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1720e-05 - accuracy: 1.0000 - val_loss: 4.7835e-05 - val_accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.1640e-05 - accuracy: 1.0000 - val_loss: 4.7892e-05 - val_accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.1569e-05 - accuracy: 1.0000 - val_loss: 4.7942e-05 - val_accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.1489e-05 - accuracy: 1.0000 - val_loss: 4.7977e-05 - val_accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.1420e-05 - accuracy: 1.0000 - val_loss: 4.8039e-05 - val_accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1354e-05 - accuracy: 1.0000 - val_loss: 4.8073e-05 - val_accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.1269e-05 - accuracy: 1.0000 - val_loss: 4.8154e-05 - val_accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.1199e-05 - accuracy: 1.0000 - val_loss: 4.8180e-05 - val_accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.1138e-05 - accuracy: 1.0000 - val_loss: 4.8171e-05 - val_accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.1063e-05 - accuracy: 1.0000 - val_loss: 4.8290e-05 - val_accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0996e-05 - accuracy: 1.0000 - val_loss: 4.8338e-05 - val_accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0924e-05 - accuracy: 1.0000 - val_loss: 4.8402e-05 - val_accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0860e-05 - accuracy: 1.0000 - val_loss: 4.8417e-05 - val_accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0792e-05 - accuracy: 1.0000 - val_loss: 4.8460e-05 - val_accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0728e-05 - accuracy: 1.0000 - val_loss: 4.8497e-05 - val_accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.0658e-05 - accuracy: 1.0000 - val_loss: 4.8571e-05 - val_accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0596e-05 - accuracy: 1.0000 - val_loss: 4.8537e-05 - val_accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.0533e-05 - accuracy: 1.0000 - val_loss: 4.8719e-05 - val_accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.0467e-05 - accuracy: 1.0000 - val_loss: 4.8760e-05 - val_accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 1.0401e-05 - accuracy: 1.0000 - val_loss: 4.8794e-05 - val_accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0333e-05 - accuracy: 1.0000 - val_loss: 4.8859e-05 - val_accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 1.0268e-05 - accuracy: 1.0000 - val_loss: 4.8846e-05 - val_accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0213e-05 - accuracy: 1.0000 - val_loss: 4.8833e-05 - val_accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0146e-05 - accuracy: 1.0000 - val_loss: 4.8879e-05 - val_accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 1.0079e-05 - accuracy: 1.0000 - val_loss: 4.8871e-05 - val_accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 1.0017e-05 - accuracy: 1.0000 - val_loss: 4.8942e-05 - val_accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.9539e-06 - accuracy: 1.0000 - val_loss: 4.8987e-05 - val_accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.8896e-06 - accuracy: 1.0000 - val_loss: 4.9027e-05 - val_accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 9.8288e-06 - accuracy: 1.0000 - val_loss: 4.9044e-05 - val_accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 9.7617e-06 - accuracy: 1.0000 - val_loss: 4.9034e-05 - val_accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.7024e-06 - accuracy: 1.0000 - val_loss: 4.8994e-05 - val_accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.6331e-06 - accuracy: 1.0000 - val_loss: 4.9099e-05 - val_accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.5753e-06 - accuracy: 1.0000 - val_loss: 4.9160e-05 - val_accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.5093e-06 - accuracy: 1.0000 - val_loss: 4.9174e-05 - val_accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.4594e-06 - accuracy: 1.0000 - val_loss: 4.9064e-05 - val_accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.3899e-06 - accuracy: 1.0000 - val_loss: 4.9085e-05 - val_accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.3178e-06 - accuracy: 1.0000 - val_loss: 4.9116e-05 - val_accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.2577e-06 - accuracy: 1.0000 - val_loss: 4.9158e-05 - val_accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 9.1943e-06 - accuracy: 1.0000 - val_loss: 4.9171e-05 - val_accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.1330e-06 - accuracy: 1.0000 - val_loss: 4.9191e-05 - val_accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 9.0636e-06 - accuracy: 1.0000 - val_loss: 4.9100e-05 - val_accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 9.0032e-06 - accuracy: 1.0000 - val_loss: 4.9089e-05 - val_accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.9358e-06 - accuracy: 1.0000 - val_loss: 4.9127e-05 - val_accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 8.8730e-06 - accuracy: 1.0000 - val_loss: 4.9054e-05 - val_accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.8004e-06 - accuracy: 1.0000 - val_loss: 4.9068e-05 - val_accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.7384e-06 - accuracy: 1.0000 - val_loss: 4.9015e-05 - val_accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.6683e-06 - accuracy: 1.0000 - val_loss: 4.9101e-05 - val_accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.6065e-06 - accuracy: 1.0000 - val_loss: 4.9074e-05 - val_accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 8.5354e-06 - accuracy: 1.0000 - val_loss: 4.9012e-05 - val_accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.4749e-06 - accuracy: 1.0000 - val_loss: 4.9040e-05 - val_accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.4065e-06 - accuracy: 1.0000 - val_loss: 4.9047e-05 - val_accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 8.3466e-06 - accuracy: 1.0000 - val_loss: 4.9083e-05 - val_accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 8.2787e-06 - accuracy: 1.0000 - val_loss: 4.8939e-05 - val_accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.2131e-06 - accuracy: 1.0000 - val_loss: 4.9026e-05 - val_accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.1429e-06 - accuracy: 1.0000 - val_loss: 4.9026e-05 - val_accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.0763e-06 - accuracy: 1.0000 - val_loss: 4.8999e-05 - val_accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 8.0155e-06 - accuracy: 1.0000 - val_loss: 4.8912e-05 - val_accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.9486e-06 - accuracy: 1.0000 - val_loss: 4.8808e-05 - val_accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.8792e-06 - accuracy: 1.0000 - val_loss: 4.8879e-05 - val_accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 7.8152e-06 - accuracy: 1.0000 - val_loss: 4.8749e-05 - val_accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.7470e-06 - accuracy: 1.0000 - val_loss: 4.8805e-05 - val_accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 7.6863e-06 - accuracy: 1.0000 - val_loss: 4.8832e-05 - val_accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.6207e-06 - accuracy: 1.0000 - val_loss: 4.8702e-05 - val_accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 7.5557e-06 - accuracy: 1.0000 - val_loss: 4.8752e-05 - val_accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 7.4946e-06 - accuracy: 1.0000 - val_loss: 4.8736e-05 - val_accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 7.4249e-06 - accuracy: 1.0000 - val_loss: 4.8645e-05 - val_accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 7.3593e-06 - accuracy: 1.0000 - val_loss: 4.8657e-05 - val_accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 7.2899e-06 - accuracy: 1.0000 - val_loss: 4.8441e-05 - val_accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 7.2281e-06 - accuracy: 1.0000 - val_loss: 4.8316e-05 - val_accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "37/37 [==============================] - 1s 15ms/step - loss: 7.1621e-06 - accuracy: 1.0000 - val_loss: 4.8207e-05 - val_accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "37/37 [==============================] - 1s 15ms/step - loss: 7.0957e-06 - accuracy: 1.0000 - val_loss: 4.8168e-05 - val_accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 7.0296e-06 - accuracy: 1.0000 - val_loss: 4.8109e-05 - val_accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.9655e-06 - accuracy: 1.0000 - val_loss: 4.8029e-05 - val_accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 6.9035e-06 - accuracy: 1.0000 - val_loss: 4.7706e-05 - val_accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 6.8380e-06 - accuracy: 1.0000 - val_loss: 4.7644e-05 - val_accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 6.7721e-06 - accuracy: 1.0000 - val_loss: 4.7449e-05 - val_accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 6.7083e-06 - accuracy: 1.0000 - val_loss: 4.7431e-05 - val_accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.6461e-06 - accuracy: 1.0000 - val_loss: 4.7275e-05 - val_accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 6.5811e-06 - accuracy: 1.0000 - val_loss: 4.7118e-05 - val_accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "37/37 [==============================] - 1s 13ms/step - loss: 6.5162e-06 - accuracy: 1.0000 - val_loss: 4.7045e-05 - val_accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 6.4524e-06 - accuracy: 1.0000 - val_loss: 4.6868e-05 - val_accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.3962e-06 - accuracy: 1.0000 - val_loss: 4.6819e-05 - val_accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 6.3327e-06 - accuracy: 1.0000 - val_loss: 4.6537e-05 - val_accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.2640e-06 - accuracy: 1.0000 - val_loss: 4.6404e-05 - val_accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.2020e-06 - accuracy: 1.0000 - val_loss: 4.6325e-05 - val_accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 6.1368e-06 - accuracy: 1.0000 - val_loss: 4.6127e-05 - val_accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 6.0763e-06 - accuracy: 1.0000 - val_loss: 4.6006e-05 - val_accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 6.0122e-06 - accuracy: 1.0000 - val_loss: 4.5906e-05 - val_accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.9495e-06 - accuracy: 1.0000 - val_loss: 4.5657e-05 - val_accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.8857e-06 - accuracy: 1.0000 - val_loss: 4.5645e-05 - val_accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.8237e-06 - accuracy: 1.0000 - val_loss: 4.5417e-05 - val_accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.7587e-06 - accuracy: 1.0000 - val_loss: 4.5171e-05 - val_accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.6986e-06 - accuracy: 1.0000 - val_loss: 4.5059e-05 - val_accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 5.6375e-06 - accuracy: 1.0000 - val_loss: 4.4927e-05 - val_accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 5.5817e-06 - accuracy: 1.0000 - val_loss: 4.4739e-05 - val_accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 5.5178e-06 - accuracy: 1.0000 - val_loss: 4.4605e-05 - val_accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 5.4587e-06 - accuracy: 1.0000 - val_loss: 4.4452e-05 - val_accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 5.4003e-06 - accuracy: 1.0000 - val_loss: 4.4289e-05 - val_accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.3380e-06 - accuracy: 1.0000 - val_loss: 4.4249e-05 - val_accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 5.2817e-06 - accuracy: 1.0000 - val_loss: 4.3958e-05 - val_accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.2219e-06 - accuracy: 1.0000 - val_loss: 4.3878e-05 - val_accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 5.1650e-06 - accuracy: 1.0000 - val_loss: 4.3701e-05 - val_accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 5.1072e-06 - accuracy: 1.0000 - val_loss: 4.3515e-05 - val_accuracy: 1.0000\n",
            "36/36 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold_result_mean"
      ],
      "metadata": {
        "id": "BHDI7I_deLIG",
        "outputId": "308a4edd-8e2e-46d3-d5c9-b1ffe387118b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9869947275922671"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold_result_arr"
      ],
      "metadata": {
        "id": "FOadql0keOYu",
        "outputId": "52fb5c2c-b7c9-45db-fb2b-3ba14be568e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 1.0, 1.0, 1.0, 0.9349736379613357]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "dQyZTFSjXjir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test['id'].astype('str')\n",
        "test['tweet'].astype('str')\n",
        "\n",
        "test_bag = TF_IDF_Vectorizer.transform(test[\"tweet\"])\n",
        "test_bag = pd.DataFrame(test_bag.toarray(), index = test.index, columns = TF_IDF_Vectorizer.get_feature_names())\n",
        "\n",
        "test['largoDocumento'] = test.tweet.str.len()\n",
        "test_bag['largoDocumento'] = test['largoDocumento']\n",
        "\n",
        "test_bag_array = np.asarray(test_bag)\n",
        "test_prediction = saved_model.predict(test_bag_array)\n",
        "\n",
        "copy = test_prediction.copy()\n",
        "predNumber = [];\n",
        "predLabel = [];\n",
        "for i in range(len(copy)):\n",
        "  if copy[i][0] > 0.5:\n",
        "    predNumber.append([1])\n",
        "    predLabel.append([\"real\"])\n",
        "  else:\n",
        "    predNumber.append([0])\n",
        "    predLabel.append([\"fake\"])\n",
        "\n",
        "prediction = pd.DataFrame(predLabel, columns=['label'])\n",
        "prediction.index += 1\n",
        "prediction = prediction.to_csv('prediction.csv')"
      ],
      "metadata": {
        "id": "hZMa_5ZcWMWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3bff43-3533-4d58-aefe-58cd7aaea131"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 1/67 [..............................] - ETA: 1s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67/67 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('prediction.csv')"
      ],
      "metadata": {
        "id": "odAqKelbXH36",
        "outputId": "9f8f6856-2aca-48e4-9977-b3a1d3ddba5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b49b0da2-b01e-4639-aabc-a0ca07aba50f\", \"prediction.csv\", 20300)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}